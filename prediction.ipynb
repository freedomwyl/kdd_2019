{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from time import gmtime, strftime\n",
    "import warnings\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import catboost as cb\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_f(y_pred, train_data):\n",
    "    y_true = train_data.label\n",
    "    y_pred = y_pred.reshape((12, -1)).T\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    score = f1_score(y_true, y_pred, average='weighted')\n",
    "    return 'weighted-f1-score', score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_result(submit, result, model_name, score):\n",
    "    now_time = strftime(\"%Y%m%d%H%M\", gmtime())\n",
    "    submit['recommend_mode'] = result\n",
    "    #submit['mod_list'] = submit['mod_list'].apply(lambda x: x.strip('[]').split(','))\n",
    "    #for i, row in submit.iterrows():\n",
    "    #    if row['mod_list'][0] != '':\n",
    "    #        row['mod_list'] = map(lambda s : int(s), row['mod_list'])\n",
    "    #        if row['recommend_mode'] not in row['mod_list']: #and  len(row['mod_list']) != 0:\n",
    "                #print i,row['mod_list'], row['recommend_mode'], row['sid']\n",
    "                #submit['recommend_mode'][i] = row['mod_list'][0]\n",
    "     #           cnt = cnt +1\n",
    "                #print i,submit['recommend_mode'][i]\n",
    "    #print(\"number of correction: \", cnt)\n",
    "    #submit = submit.drop(['mod_list'], axis=1)\n",
    "    submit.to_csv('./submit/{}_result_{}_{}.csv'.format(model_name, now_time, round(score,4)), index=False)\n",
    "    print(\"done, good luck!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_result_D(sb1,sb2,sb3, lgb1,lgb2,lgb3, model_name, score):\n",
    "    now_time = strftime(\"%Y%m%d%H%M\", gmtime())\n",
    "    sb1['recommend_mode'] = lgb1\n",
    "    sb2['recommend_mode'] = lgb2\n",
    "    sb3['recommend_mode'] = lgb3\n",
    "    submit = pd.concat([sb1,sb2,sb3], axis=0)  \n",
    "    submit.to_csv('./submit/{}_result_{}_{}.csv'.format(model_name, now_time, round(score,4)), index=False)\n",
    "    print(\"done, good luck!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(train_x, train_y, val_x, val_y):\n",
    "    data = train_x.copy()\n",
    "    #data = pd.concat([train_x, train_y], axis=1)\n",
    "    data['click_mode'] = train_y\n",
    "    data0 = data[(data['click_mode'] == 0)]\n",
    "    data1 = data[(data['click_mode'] == 1)]\n",
    "    data2 = data[(data['click_mode'] == 2)]\n",
    "    data3 = data[(data['click_mode'] == 3)]\n",
    "    data4 = data[(data['click_mode'] == 4)]\n",
    "    data5 = data[(data['click_mode'] == 5)]\n",
    "    data6 = data[(data['click_mode'] == 6)]\n",
    "    data7 = data[(data['click_mode'] == 7)]\n",
    "    data8 = data[(data['click_mode'] == 8)]\n",
    "    data9 = data[(data['click_mode'] == 9)]\n",
    "    data10 = data[(data['click_mode'] == 10)]\n",
    "    data11 = data[(data['click_mode'] == 11)]\n",
    "    \n",
    "    data4 = data4.sample(frac=0.999,axis=0)\n",
    "    data6 = data6.sample(frac=0.999,axis=0)\n",
    "    data8 = data8.sample(frac=0.999,axis=0)\n",
    "    data11 = data11.sample(frac=0.999,axis=0)\n",
    "    \n",
    "    data3 = data3.sample(frac=0.999,axis=0)#98\n",
    "    data10 = data10.sample(frac=0.999,axis=0)\n",
    "    \n",
    "    data5 = data5.sample(frac=1,axis=0)#0.6\n",
    "    data9 = data9.sample(frac=1,axis=0)\n",
    "    \n",
    "    data1 = data1.sample(frac=1,axis=0)\n",
    "    data7 = data7.sample(frac=1,axis=0)\n",
    "    data0 = data0.sample(frac=1,axis=0)\n",
    "    #data0x = data0[data0['mode_length'] == 0]\n",
    "    #data0y = data0[data0['mode_length'] != 0]\n",
    "    #data0y = data0y.sample(frac=0.7,axis=0)\n",
    "    data2 = data2.sample(frac=1,axis=0)#0.5\n",
    "    \n",
    "    df = pd.concat([data0, data1, data2, data3,\n",
    "                    data4, data5, data6, data7,\n",
    "                    data8, data9, data10, data11], axis=0)\n",
    "    \n",
    "    y = df['click_mode'].copy()\n",
    "    x = df.drop(['click_mode'],  axis=1)\n",
    "    #val = pd.concat([val_x, val_y], axis=1)\n",
    "    val = val_x.copy()\n",
    "    val['click_mode'] = val_y\n",
    "    #val = val.sample(frac=0.6, axis=0)  \n",
    "    all_data = pd.concat([df, val], axis=0)\n",
    "    all_y = all_data['click_mode'].copy()\n",
    "    all_x = all_data.drop(['click_mode'],  axis=1)\n",
    "    return x,y,all_x,all_y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g[0.65, 0.5, 0.5, 0.98, 0.98, 0.5, 0.98, 0.5, 0.98, 0.5, 0.98, 0.98]\n",
    "# b [0.8, 0.45, 0.45, 0.98, 0.98, 0.6, 0.98, 0.45, 0.98, 0.45, 0.6, 0.98]\n",
    "#0.7, 0.5, 0.5, 0.98, 0.98, 0.5, 0.98, 0.5, 0.98, 0.5, 0.98, 0.98\n",
    "def down_sampling(data_all, train_y, sampling_rates=[1,1,1,1,1,1,1,1,1,1,1,1]):\n",
    "    #data_all = pd.concat([train_x, train_y], axis=1)\n",
    "    data_all['click_mode'] = train_y\n",
    "    data = {}\n",
    "    for mode, rate in enumerate(sampling_rates):\n",
    "        data[mode] = pd.DataFrame()\n",
    "        data[mode] = data_all[(data_all['click_mode'] == mode)]\n",
    "        data[mode] = data[mode].sample(frac=rate, axis=0)\n",
    "    \n",
    "    df = pd.concat([data[mode] for mode in range(12)], axis=0)\n",
    "    df = df.sample(frac=1)\n",
    "    y = np.array(df['click_mode'])\n",
    "    x = df.drop(['click_mode'],  axis=1)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels, preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "\n",
    "def train_lgb(train_x, train_y, test_x):  #,train_x1, train_y1,\n",
    "    #kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    \n",
    "    train_index = ((train_x['day_time'] <= '2018-11-23'))\n",
    "    valid_index = (train_x['day_time'] > '2018-11-23') & (train_x['day_time'] < '2018-12-01')\n",
    "    \n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    \n",
    "    tr_x1     = train_x[train_index]    #train_x1#\n",
    "    tr_y1     = train_y[train_index]    #train_y1#\n",
    "    val_x     = train_x[valid_index]\n",
    "    val_y     = train_y[valid_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    cate_cols = ['max_dist_mode', 'min_dist_mode', #'max_price_mode','pred_5']#['pred_0','pred_1','cpred_0']#\n",
    "                'min_price_mode', 'max_eta_mode', 'min_eta_mode',\n",
    "                 'first_mode','last_mode', 'week', ]\n",
    "    scores = []\n",
    "    results = []\n",
    " \n",
    "    for i in range(1):\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=41,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=2000, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.05, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "        \n",
    "        tr_x, tr_y, all_train_x, all_train_y = gen_data(tr_x1, tr_y1, val_x, val_y)\n",
    "        \n",
    "        eval_set = [(val_x, val_y)]\n",
    "        lgb_model.fit(\n",
    "            tr_x, tr_y, eval_set=eval_set,\n",
    "            eval_metric=f1_weighted,\n",
    "            categorical_feature=cate_cols,\n",
    "            verbose=50, early_stopping_rounds=70)\n",
    "        \n",
    "        \n",
    "        def get_weighted_fscore(y_pred, y_true):\n",
    "            f_score = 0\n",
    "            for i in range(12):\n",
    "                yt = y_true == i\n",
    "                yp = y_pred == i\n",
    "                f_score += dic_[i] * f1_score(y_true=yt, y_pred= yp)\n",
    "                print(i,dic_[i],f1_score(y_true=yt, y_pred= yp), precision_score(y_true=yt, y_pred= yp),recall_score(y_true=yt, y_pred= yp))\n",
    "            print(f_score)\n",
    "            return f_score\n",
    "        \n",
    "        val_pred = lgb_model.predict(val_x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = lgb_model.predict(val_x) \n",
    "        df_analysis = pd.DataFrame()\n",
    "        #df_analysis['sid']   = val_x['sid']\n",
    "        df_analysis['label'] = val_y\n",
    "        df_analysis['pred']  = pred\n",
    "        df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "        dic_ = df_analysis['label'].value_counts(normalize = True)\n",
    "        \n",
    "        val_score = get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "        \n",
    "        scores.append(val_score)\n",
    "        lgb_model.n_estimators = lgb_model.best_iteration_\n",
    "        #lgb_model.fit(all_train_x, all_train_y, categorical_feature=cate_cols)\n",
    "        pred_test = lgb_model.predict_proba(test_x)\n",
    "        results.append(pred_test)\n",
    "        \n",
    "    result = np.argmax(np.mean(results, axis=0), axis=1)  \n",
    "    print('cv f1-score: ', np.mean(scores))\n",
    "    return result, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_k_fold(train_x, train_y, test_x):\n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    lgb_paras = {\n",
    "        'objective': 'multiclass',\n",
    "        'metrics': 'multiclass',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 41,\n",
    "        'lambda_l1': 0.01,\n",
    "        'lambda_l2': 10,\n",
    "        'num_class': 12,\n",
    "        'seed': 2019,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 4,\n",
    "    }\n",
    "    cate_cols = ['pred_0','pred_1','cpred_0']#['max_dist_mode', 'min_dist_mode', #'max_price_mode','pred_5']#\n",
    "                 #'min_price_mode', 'max_eta_mode', 'min_eta_mode',\n",
    "                 #'first_mode','last_mode', 'week', ]#\n",
    "    scores = []\n",
    "    result_proba = []\n",
    "    #second_level_train_set = np.zeros((train_x.shape[0],))\n",
    "    for tr_idx, val_idx in kfold.split(train_x, train_y):\n",
    "        tr_x, tr_y, val_x, val_y = train_x.iloc[tr_idx], train_y[tr_idx], train_x.iloc[val_idx], train_y[val_idx]\n",
    "        tr_x, tr_y = down_sampling(tr_x, tr_y)\n",
    "        #val_x, val_y = down_sampling(val_x, val_y,[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "        train_set = lgb.Dataset(tr_x, tr_y, categorical_feature=cate_cols)\n",
    "        val_set = lgb.Dataset(val_x, val_y, categorical_feature=cate_cols)\n",
    "        lgb_model = lgb.train(lgb_paras, train_set,\n",
    "                              valid_sets=[val_set], early_stopping_rounds=100, num_boost_round=40000, verbose_eval=50, feval=eval_f)\n",
    "        val_proba = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n",
    "        val_pred = np.argmax(val_proba, axis=1)\n",
    "        val_score = f1_score(val_y, val_pred, average='weighted')\n",
    "        result_proba.append(lgb_model.predict(\n",
    "            test_x, num_iteration=lgb_model.best_iteration))\n",
    "        scores.append(val_score)\n",
    "    print('cv f1-score: ', np.mean(scores))\n",
    "    pred_test = np.argmax(np.mean(result_proba, axis=0), axis=1)\n",
    "    return pred_test, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_k_fold_stack(train_x, train_y, test_x):\n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    lgb_paras = {\n",
    "        'objective': 'multiclass',\n",
    "        'metrics': 'multiclass',\n",
    "        'learning_rate': 0.2,\n",
    "        'num_leaves': 41,\n",
    "        'lambda_l1': 0.01,\n",
    "        'lambda_l2': 10,\n",
    "        'num_class': 12,\n",
    "        'seed': 2019,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 4,\n",
    "    }\n",
    "    cate_cols = ['pred_0','pred_1','cpred_0']\n",
    "    scores = []\n",
    "    second_level_test_set = []\n",
    "    second_level_train_set = np.zeros((train_x.shape[0],12),dtype=np.float32)\n",
    "    for tr_idx, val_idx in kfold.split(train_x, train_y):\n",
    "        tr_x, tr_y, val_x, val_y = train_x.iloc[tr_idx], train_y[tr_idx], train_x.iloc[val_idx], train_y[val_idx]\n",
    "        tr_x, tr_y = down_sampling(tr_x, tr_y)\n",
    "        #val_x, val_y = down_sampling(val_x, val_y,[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "        train_set = lgb.Dataset(tr_x, tr_y, categorical_feature=cate_cols)\n",
    "        val_set = lgb.Dataset(val_x, val_y, categorical_feature=cate_cols)\n",
    "        lgb_model = lgb.train(lgb_paras, train_set,\n",
    "                              valid_sets=[val_set], early_stopping_rounds=70, num_boost_round=4000, verbose_eval=50, feval=eval_f)\n",
    "        val_proba = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n",
    "        val_pred = np.argmax(val_proba, axis=1)\n",
    "        second_level_train_set[val_idx] = val_proba\n",
    "        val_score = f1_score(val_y, val_pred, average='weighted')\n",
    "        second_level_test_set.append(lgb_model.predict(\n",
    "            test_x, num_iteration=lgb_model.best_iteration))\n",
    "        scores.append(val_score)\n",
    "    print('cv f1-score: ', np.mean(scores))\n",
    "    pred_test = np.argmax(np.mean(second_level_test_set, axis=0), axis=1)\n",
    "    pred_tr = np.argmax(second_level_train_set, axis=1)\n",
    "    second_level_test_set = np.mean(second_level_test_set, axis=0)\n",
    "    return second_level_train_set, pred_tr, second_level_test_set, pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_city(tr,te,sb):\n",
    "    test = pd.concat([sb, te], axis=1)\n",
    "    te1 = test[test['o1'] < 115]\n",
    "    te2 = test[(test['o1'] > 115)&(test['o1'] < 118)]\n",
    "    te3 = test[test['o1'] > 118]\n",
    "    sb1 = te1[['sid']].copy()\n",
    "    te1 = te1.drop(['sid'],axis=1)\n",
    "    sb2 = te2[['sid']].copy()\n",
    "    te2 = te2.drop(['sid'],axis=1)\n",
    "    sb3 = te3[['sid']].copy()\n",
    "    te3 = te3.drop(['sid'],axis=1)\n",
    "    \n",
    "    tr1 = tr[tr['o1'] < 115]\n",
    "    tr2 = tr[(tr['o1'] > 115)&(tr['o1'] < 118)]\n",
    "    tr3 = tr[tr['o1'] > 118]\n",
    "    return sb1,sb2,sb3,te1,te2,te3,tr1,tr2,tr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_stacking_data(train_x, train_y, test_x):\n",
    "    #tr_day = list(train_x['day_time'])\n",
    "    #te_day = list(test_x['day_time'])\n",
    "    second_tr = pd.DataFrame()\n",
    "    second_te = pd.DataFrame()\n",
    "    for j in range(1):\n",
    "        j += 2\n",
    "        s_tr, pred_tr, s_te, pred_test = train_lgb_k_fold_stack(train_x, train_y, test_x)\n",
    "        s_tr = pd.DataFrame(s_tr)\n",
    "        s_te = pd.DataFrame(s_te)\n",
    "        s_tr.columns = [ str(j)+ '_' + 'cp_{}'.format(i) for i in range(12)]\n",
    "        s_te.columns = [ str(j)+ '_' + 'cp_{}'.format(i) for i in range(12)]\n",
    "        s_tr['cpred' + '_' + str(j)] = pred_tr\n",
    "        s_te['cpred' + '_' + str(j)] = pred_test\n",
    "        second_tr = pd.concat([second_tr, s_tr], axis=1)\n",
    "        second_te = pd.concat([second_te, s_te], axis=1)\n",
    "    del train_x, test_x\n",
    "    #second_tr['day_time'] = tr_day\n",
    "    #second_te['day_time'] = te_day\n",
    "    return second_tr, second_te\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " if __name__ == '__main__':\n",
    "    train_data = pd.read_csv(\"./data/train_data.csv\")\n",
    "    #train_data = train_data[train_data['day_time'] >= '2018-10-08']\n",
    "    train_data['click_mode'] = train_data['click_mode'].apply(lambda x: int(x))\n",
    "    train_y = np.array(train_data['click_mode'])\n",
    "    #train_y = train_data['click_mode'].copy()\n",
    "    train_x = train_data.drop(['click_mode',], axis=1)\n",
    "    test_x = pd.read_csv(\"./data/test_data.csv\")#11\n",
    "    #test_x = test_x.drop([],  axis=1) #'\n",
    "    submit = pd.read_csv(\"./data/submit.csv\") \n",
    "    submit = submit.drop(['mod_list'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array(tr1['click_mode'])\n",
    "#y2 = np.array(tr2['click_mode'])\n",
    "#y3 = np.array(tr3['click_mode'])\n",
    "tr1 = tr1.drop(['click_mode',], axis=1)\n",
    "#tr2 = tr2.drop(['click_mode',], axis=1)\n",
    "#tr3 = tr3.drop(['click_mode',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tr = pd.read_csv(\"./data/second_trsh01.csv\")\n",
    "second_te = pd.read_csv(\"./data/second_tesh01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trx = pd.read_csv(\"./data/second_trsh0x.csv\")\n",
    "second_tex = pd.read_csv(\"./data/second_tesh0x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'second_trsh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3b6acc9511b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msecond_trsh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_trsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day_time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msecond_tesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_tesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day_time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'second_trsh' is not defined"
     ]
    }
   ],
   "source": [
    "second_trsh = second_trsh.drop(['day_time',], axis=1)\n",
    "second_tesh = second_tesh.drop(['day_time',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trx = pd.concat([second_tr,second_trx], axis=1)\n",
    "second_tex = pd.concat([second_te,second_tex], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_p_0</th>\n",
       "      <th>0_p_1</th>\n",
       "      <th>0_p_2</th>\n",
       "      <th>0_p_3</th>\n",
       "      <th>0_p_4</th>\n",
       "      <th>0_p_5</th>\n",
       "      <th>0_p_6</th>\n",
       "      <th>0_p_7</th>\n",
       "      <th>0_p_8</th>\n",
       "      <th>0_p_9</th>\n",
       "      <th>0_p_10</th>\n",
       "      <th>0_p_11</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>1_p_0</th>\n",
       "      <th>1_p_1</th>\n",
       "      <th>1_p_2</th>\n",
       "      <th>1_p_3</th>\n",
       "      <th>1_p_4</th>\n",
       "      <th>1_p_5</th>\n",
       "      <th>1_p_6</th>\n",
       "      <th>1_p_7</th>\n",
       "      <th>1_p_8</th>\n",
       "      <th>1_p_9</th>\n",
       "      <th>1_p_10</th>\n",
       "      <th>1_p_11</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>day_time</th>\n",
       "      <th>0_cp_0</th>\n",
       "      <th>0_cp_1</th>\n",
       "      <th>0_cp_2</th>\n",
       "      <th>0_cp_3</th>\n",
       "      <th>0_cp_4</th>\n",
       "      <th>0_cp_5</th>\n",
       "      <th>0_cp_6</th>\n",
       "      <th>0_cp_7</th>\n",
       "      <th>0_cp_8</th>\n",
       "      <th>0_cp_9</th>\n",
       "      <th>0_cp_10</th>\n",
       "      <th>0_cp_11</th>\n",
       "      <th>cpred_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054427</td>\n",
       "      <td>0.026944</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.021067</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>8.504493e-07</td>\n",
       "      <td>0.869205</td>\n",
       "      <td>1.066995e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>8.366390e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.898560</td>\n",
       "      <td>8.536940e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>0.068843</td>\n",
       "      <td>0.046271</td>\n",
       "      <td>0.029643</td>\n",
       "      <td>0.027286</td>\n",
       "      <td>0.023941</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.758206</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.014237</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.173976</td>\n",
       "      <td>0.462319</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.092876</td>\n",
       "      <td>0.046116</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>1.992413e-01</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>3.097929e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181952</td>\n",
       "      <td>0.504123</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.094853</td>\n",
       "      <td>0.045080</td>\n",
       "      <td>2.392035e-02</td>\n",
       "      <td>0.150001</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>7.334114e-07</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>0.168519</td>\n",
       "      <td>0.451878</td>\n",
       "      <td>0.037364</td>\n",
       "      <td>0.071804</td>\n",
       "      <td>0.055262</td>\n",
       "      <td>0.038681</td>\n",
       "      <td>0.117990</td>\n",
       "      <td>0.024446</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.017946</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062231</td>\n",
       "      <td>0.091938</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.032845</td>\n",
       "      <td>0.042634</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>6.027346e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>5.234158e-01</td>\n",
       "      <td>0.246883</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>8</td>\n",
       "      <td>0.056912</td>\n",
       "      <td>0.140374</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>7.020653e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>3.843707e-01</td>\n",
       "      <td>0.335178</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>0.076212</td>\n",
       "      <td>0.140832</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.034363</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.356796</td>\n",
       "      <td>0.318778</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.019052</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>6.179246e-06</td>\n",
       "      <td>0.832507</td>\n",
       "      <td>8.593302e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.100616</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7</td>\n",
       "      <td>0.038791</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.019066</td>\n",
       "      <td>0.034936</td>\n",
       "      <td>4.133176e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.782989</td>\n",
       "      <td>1.594362e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.124164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>0.062585</td>\n",
       "      <td>0.018986</td>\n",
       "      <td>0.030122</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.687562</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.115154</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.195220</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.656675e-06</td>\n",
       "      <td>0.765869</td>\n",
       "      <td>1.233404e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>1.496050e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.825415</td>\n",
       "      <td>1.283853e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>0.201605</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.027048</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.733057</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0_p_0     0_p_1     0_p_2     0_p_3     0_p_4     0_p_5         0_p_6  \\\n",
       "0  0.054427  0.026944  0.000005  0.021067  0.028333  0.000001  8.504493e-07   \n",
       "1  0.173976  0.462319  0.000017  0.092876  0.046116  0.025419  1.992413e-01   \n",
       "2  0.062231  0.091938  0.000010  0.032845  0.042634  0.000001  6.027346e-06   \n",
       "3  0.034286  0.000011  0.000005  0.013502  0.019052  0.000001  6.179246e-06   \n",
       "4  0.195220  0.000010  0.000007  0.025960  0.012906  0.000002  4.656675e-06   \n",
       "\n",
       "      0_p_7         0_p_8     0_p_9    0_p_10    0_p_11  pred_0     1_p_0  \\\n",
       "0  0.869205  1.066995e-06  0.000005  0.000008  0.000005       7  0.043756   \n",
       "1  0.000016  3.097929e-07  0.000013  0.000003  0.000004       1  0.181952   \n",
       "2  0.000013  5.234158e-01  0.246883  0.000016  0.000007       8  0.056912   \n",
       "3  0.832507  8.593302e-07  0.000005  0.100616  0.000007       7  0.038791   \n",
       "4  0.765869  1.233404e-06  0.000004  0.000014  0.000003       7  0.138900   \n",
       "\n",
       "      1_p_1     1_p_2     1_p_3     1_p_4         1_p_5     1_p_6     1_p_7  \\\n",
       "0  0.023634  0.000005  0.018145  0.015876  8.366390e-07  0.000002  0.898560   \n",
       "1  0.504123  0.000014  0.094853  0.045080  2.392035e-02  0.150001  0.000024   \n",
       "2  0.140374  0.000013  0.036218  0.046883  7.020653e-07  0.000009  0.000017   \n",
       "3  0.000017  0.000009  0.019066  0.034936  4.133176e-06  0.000009  0.782989   \n",
       "4  0.000011  0.000005  0.021333  0.014314  1.496050e-06  0.000001  0.825415   \n",
       "\n",
       "          1_p_8     1_p_9    1_p_10    1_p_11  pred_1    day_time    0_cp_0  \\\n",
       "0  8.536940e-07  0.000004  0.000009  0.000007       7  2018-10-07  0.068843   \n",
       "1  7.334114e-07  0.000022  0.000004  0.000006       1  2018-10-13  0.168519   \n",
       "2  3.843707e-01  0.335178  0.000016  0.000008       8  2018-11-26  0.076212   \n",
       "3  1.594362e-06  0.000008  0.124164  0.000006       7  2018-10-21  0.062585   \n",
       "4  1.283853e-06  0.000006  0.000008  0.000003       7  2018-10-28  0.201605   \n",
       "\n",
       "     0_cp_1    0_cp_2    0_cp_3    0_cp_4    0_cp_5    0_cp_6    0_cp_7  \\\n",
       "0  0.046271  0.029643  0.027286  0.023941  0.013094  0.005697  0.758206   \n",
       "1  0.451878  0.037364  0.071804  0.055262  0.038681  0.117990  0.024446   \n",
       "2  0.140832  0.013377  0.034363  0.036788  0.005982  0.002636  0.008826   \n",
       "3  0.018986  0.030122  0.024991  0.021429  0.013306  0.005789  0.687562   \n",
       "4  0.002631  0.004101  0.025047  0.027048  0.001850  0.000827  0.733057   \n",
       "\n",
       "     0_cp_8    0_cp_9   0_cp_10   0_cp_11  cpred_0  \n",
       "0  0.000997  0.014237  0.007260  0.004523        7  \n",
       "1  0.001257  0.017946  0.009151  0.005701        1  \n",
       "2  0.356796  0.318778  0.003330  0.002080        8  \n",
       "3  0.001013  0.014467  0.115154  0.004596        7  \n",
       "4  0.000151  0.002006  0.001030  0.000646        7  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_trx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.888509\tvalid_0's weighted-f1-score: 0.670577\n",
      "[100]\tvalid_0's multi_logloss: 0.865302\tvalid_0's weighted-f1-score: 0.670869\n",
      "[150]\tvalid_0's multi_logloss: 0.863731\tvalid_0's weighted-f1-score: 0.670786\n",
      "[200]\tvalid_0's multi_logloss: 0.863791\tvalid_0's weighted-f1-score: 0.670757\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.865302\tvalid_0's weighted-f1-score: 0.670869\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.900209\tvalid_0's weighted-f1-score: 0.671254\n",
      "[100]\tvalid_0's multi_logloss: 0.867966\tvalid_0's weighted-f1-score: 0.671694\n",
      "[150]\tvalid_0's multi_logloss: 0.865092\tvalid_0's weighted-f1-score: 0.671444\n",
      "[200]\tvalid_0's multi_logloss: 0.864981\tvalid_0's weighted-f1-score: 0.671166\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalid_0's multi_logloss: 0.867796\tvalid_0's weighted-f1-score: 0.671719\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.891478\tvalid_0's weighted-f1-score: 0.670817\n",
      "[100]\tvalid_0's multi_logloss: 0.869355\tvalid_0's weighted-f1-score: 0.670355\n",
      "[150]\tvalid_0's multi_logloss: 0.869424\tvalid_0's weighted-f1-score: 0.668993\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's multi_logloss: 0.877224\tvalid_0's weighted-f1-score: 0.671088\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.891782\tvalid_0's weighted-f1-score: 0.669137\n",
      "[100]\tvalid_0's multi_logloss: 0.868881\tvalid_0's weighted-f1-score: 0.668979\n",
      "[150]\tvalid_0's multi_logloss: 0.867343\tvalid_0's weighted-f1-score: 0.66873\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's multi_logloss: 0.884979\tvalid_0's weighted-f1-score: 0.669271\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.887609\tvalid_0's weighted-f1-score: 0.671843\n",
      "[100]\tvalid_0's multi_logloss: 0.864738\tvalid_0's weighted-f1-score: 0.672106\n",
      "[150]\tvalid_0's multi_logloss: 0.86326\tvalid_0's weighted-f1-score: 0.671959\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's multi_logloss: 0.868264\tvalid_0's weighted-f1-score: 0.672244\n",
      "('cv f1-score: ', 0.6710382110525479)\n",
      "done, good luck!!\n"
     ]
    }
   ],
   "source": [
    "lgbres, score = train_lgb_k_fold(second_trx, train_y, second_tex)\n",
    "submit_result(submit, lgbres, 'lgb', score)\n",
    "#lgb2, score2 = train_lgb_k_fold(second_trgs, y1,second_tegs)\n",
    "#lgb3, score3 = train_lgb_k_fold(second_trsh, y3,second_tesh)\n",
    "#score = (score1 + score2 + score3)/3\n",
    "#submit_result_D(sb1,sb2,sb3, lgb2, lgb1, lgb3,'lgb', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trbj = pd.read_csv(\"./data/second_trbj.csv\")\n",
    "second_tebj = pd.read_csv(\"./data/second_tebj.csv\")\n",
    "second_trsh = pd.read_csv(\"./data/second_trsh.csv\")\n",
    "second_tesh = pd.read_csv(\"./data/second_tesh.csv\")\n",
    "second_trgs = pd.read_csv(\"./data/second_trgs.csv\")\n",
    "second_tegs = pd.read_csv(\"./data/second_tegs.csv\")\n",
    "second_trbj['click_mode'] = y2\n",
    "second_trgs['click_mode'] = y1\n",
    "second_trsh['click_mode'] = y3\n",
    "second_tr_d = pd.concat([second_trbj,second_trgs,second_trsh], axis=0)\n",
    "second_te_d = pd.concat([second_tebj,second_tegs,second_tesh], axis=0)\n",
    "second_tr_d.to_csv('./data/second_trsh_d.csv', index=False)\n",
    "second_te_d.to_csv('./data/second_tesh_d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trbj = second_trbj.drop(['click_mode',], axis=1)\n",
    "second_trsh = second_trsh.drop(['click_mode',], axis=1)\n",
    "second_trgs = second_trgs.drop(['click_mode',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, good luck!!\n"
     ]
    }
   ],
   "source": [
    "submit_result_D(sb1,sb2,sb3, lgb2, lgb1, lgb3,'lgb', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_p_0</th>\n",
       "      <th>0_p_1</th>\n",
       "      <th>0_p_2</th>\n",
       "      <th>0_p_3</th>\n",
       "      <th>0_p_4</th>\n",
       "      <th>0_p_5</th>\n",
       "      <th>0_p_6</th>\n",
       "      <th>0_p_7</th>\n",
       "      <th>0_p_8</th>\n",
       "      <th>0_p_9</th>\n",
       "      <th>0_p_10</th>\n",
       "      <th>0_p_11</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>1_p_0</th>\n",
       "      <th>1_p_1</th>\n",
       "      <th>1_p_2</th>\n",
       "      <th>1_p_3</th>\n",
       "      <th>1_p_4</th>\n",
       "      <th>1_p_5</th>\n",
       "      <th>1_p_6</th>\n",
       "      <th>1_p_7</th>\n",
       "      <th>1_p_8</th>\n",
       "      <th>1_p_9</th>\n",
       "      <th>1_p_10</th>\n",
       "      <th>1_p_11</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>2_p_0</th>\n",
       "      <th>2_p_1</th>\n",
       "      <th>2_p_2</th>\n",
       "      <th>2_p_3</th>\n",
       "      <th>2_p_4</th>\n",
       "      <th>2_p_5</th>\n",
       "      <th>2_p_6</th>\n",
       "      <th>2_p_7</th>\n",
       "      <th>2_p_8</th>\n",
       "      <th>2_p_9</th>\n",
       "      <th>2_p_10</th>\n",
       "      <th>2_p_11</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>3_p_0</th>\n",
       "      <th>3_p_1</th>\n",
       "      <th>3_p_2</th>\n",
       "      <th>3_p_3</th>\n",
       "      <th>3_p_4</th>\n",
       "      <th>3_p_5</th>\n",
       "      <th>3_p_6</th>\n",
       "      <th>3_p_7</th>\n",
       "      <th>3_p_8</th>\n",
       "      <th>3_p_9</th>\n",
       "      <th>3_p_10</th>\n",
       "      <th>3_p_11</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>day_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.063521</td>\n",
       "      <td>0.092185</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.034933</td>\n",
       "      <td>0.081638</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.208854</td>\n",
       "      <td>0.518583</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>9</td>\n",
       "      <td>0.042378</td>\n",
       "      <td>0.082386</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.016214</td>\n",
       "      <td>0.030390</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.272202</td>\n",
       "      <td>0.556261</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>9</td>\n",
       "      <td>0.071603</td>\n",
       "      <td>0.079008</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.031312</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.251604</td>\n",
       "      <td>0.529209</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>9</td>\n",
       "      <td>0.051013</td>\n",
       "      <td>0.065138</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.018638</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.448510</td>\n",
       "      <td>0.386060</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-11-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100435</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.741668</td>\n",
       "      <td>0.040830</td>\n",
       "      <td>0.046701</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.034420</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.035723</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>2</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.857631</td>\n",
       "      <td>0.041134</td>\n",
       "      <td>0.018924</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.012183</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2</td>\n",
       "      <td>0.085213</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.795073</td>\n",
       "      <td>0.045320</td>\n",
       "      <td>0.028665</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.023906</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.021636</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2</td>\n",
       "      <td>0.063042</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.818190</td>\n",
       "      <td>0.037851</td>\n",
       "      <td>0.034649</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.024327</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.021735</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088905</td>\n",
       "      <td>0.347591</td>\n",
       "      <td>0.295987</td>\n",
       "      <td>0.215927</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106378</td>\n",
       "      <td>0.347318</td>\n",
       "      <td>0.213742</td>\n",
       "      <td>0.246295</td>\n",
       "      <td>0.058143</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.027928</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098038</td>\n",
       "      <td>0.411012</td>\n",
       "      <td>0.185402</td>\n",
       "      <td>0.251407</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>0.119018</td>\n",
       "      <td>0.314588</td>\n",
       "      <td>0.232325</td>\n",
       "      <td>0.258539</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.017261</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147799</td>\n",
       "      <td>0.063931</td>\n",
       "      <td>0.588528</td>\n",
       "      <td>0.073423</td>\n",
       "      <td>0.034477</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.030740</td>\n",
       "      <td>0.060919</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>2</td>\n",
       "      <td>0.120988</td>\n",
       "      <td>0.047610</td>\n",
       "      <td>0.665637</td>\n",
       "      <td>0.029811</td>\n",
       "      <td>0.036431</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.052247</td>\n",
       "      <td>0.047128</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>2</td>\n",
       "      <td>0.102034</td>\n",
       "      <td>0.073223</td>\n",
       "      <td>0.650539</td>\n",
       "      <td>0.065138</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.047849</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>2</td>\n",
       "      <td>0.119811</td>\n",
       "      <td>0.055797</td>\n",
       "      <td>0.657135</td>\n",
       "      <td>0.045720</td>\n",
       "      <td>0.028271</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.035371</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124355</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.091685</td>\n",
       "      <td>0.045553</td>\n",
       "      <td>0.445706</td>\n",
       "      <td>0.292429</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.145287</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.074586</td>\n",
       "      <td>0.034027</td>\n",
       "      <td>0.408849</td>\n",
       "      <td>0.336893</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>5</td>\n",
       "      <td>0.143082</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.062285</td>\n",
       "      <td>0.044527</td>\n",
       "      <td>0.370505</td>\n",
       "      <td>0.379225</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>6</td>\n",
       "      <td>0.116442</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.102519</td>\n",
       "      <td>0.048516</td>\n",
       "      <td>0.242149</td>\n",
       "      <td>0.490096</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-10-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0_p_0     0_p_1     0_p_2     0_p_3     0_p_4     0_p_5     0_p_6  \\\n",
       "0  0.063521  0.092185  0.000057  0.034933  0.081638  0.000007  0.000030   \n",
       "1  0.100435  0.000078  0.741668  0.040830  0.046701  0.000024  0.000030   \n",
       "2  0.088905  0.347591  0.295987  0.215927  0.035136  0.000009  0.016340   \n",
       "3  0.147799  0.063931  0.588528  0.073423  0.034477  0.000020  0.000053   \n",
       "4  0.124355  0.000111  0.000045  0.091685  0.045553  0.445706  0.292429   \n",
       "\n",
       "      0_p_7     0_p_8     0_p_9    0_p_10    0_p_11  pred_0     1_p_0  \\\n",
       "0  0.000125  0.208854  0.518583  0.000044  0.000023       9  0.042378   \n",
       "1  0.034420  0.000015  0.000046  0.035723  0.000030       2  0.044820   \n",
       "2  0.000037  0.000004  0.000037  0.000019  0.000008       1  0.106378   \n",
       "3  0.000087  0.000006  0.030740  0.060919  0.000016       2  0.120988   \n",
       "4  0.000037  0.000010  0.000035  0.000015  0.000018       5  0.145287   \n",
       "\n",
       "      1_p_1     1_p_2     1_p_3     1_p_4     1_p_5     1_p_6     1_p_7  \\\n",
       "0  0.082386  0.000061  0.016214  0.030390  0.000009  0.000016  0.000047   \n",
       "1  0.000036  0.857631  0.041134  0.018924  0.000014  0.000005  0.025233   \n",
       "2  0.347318  0.213742  0.246295  0.058143  0.000028  0.027928  0.000073   \n",
       "3  0.047610  0.665637  0.029811  0.036431  0.000027  0.000048  0.000047   \n",
       "4  0.000134  0.000048  0.074586  0.034027  0.408849  0.336893  0.000079   \n",
       "\n",
       "      1_p_8     1_p_9    1_p_10    1_p_11  pred_1     2_p_0     2_p_1  \\\n",
       "0  0.272202  0.556261  0.000027  0.000009       9  0.071603  0.079008   \n",
       "1  0.000002  0.000014  0.012183  0.000004       2  0.085213  0.000099   \n",
       "2  0.000006  0.000046  0.000031  0.000013       1  0.098038  0.411012   \n",
       "3  0.000011  0.052247  0.047128  0.000016       2  0.102034  0.073223   \n",
       "4  0.000005  0.000055  0.000012  0.000024       5  0.143082  0.000151   \n",
       "\n",
       "      2_p_2     2_p_3     2_p_4     2_p_5     2_p_6     2_p_7     2_p_8  \\\n",
       "0  0.000070  0.031312  0.036944  0.000041  0.000055  0.000069  0.251604   \n",
       "1  0.795073  0.045320  0.028665  0.000018  0.000018  0.023906  0.000006   \n",
       "2  0.185402  0.251407  0.043956  0.000011  0.010060  0.000046  0.000006   \n",
       "3  0.650539  0.065138  0.032795  0.000009  0.000041  0.000056  0.000007   \n",
       "4  0.000074  0.062285  0.044527  0.370505  0.379225  0.000034  0.000007   \n",
       "\n",
       "      2_p_9    2_p_10    2_p_11  pred_2     3_p_0     3_p_1     3_p_2  \\\n",
       "0  0.529209  0.000039  0.000047       9  0.051013  0.065138  0.000047   \n",
       "1  0.000032  0.021636  0.000014       2  0.063042  0.000083  0.818190   \n",
       "2  0.000040  0.000016  0.000006       1  0.119018  0.314588  0.232325   \n",
       "3  0.028297  0.047849  0.000012       2  0.119811  0.055797  0.657135   \n",
       "4  0.000068  0.000024  0.000020       6  0.116442  0.000095  0.000051   \n",
       "\n",
       "      3_p_3     3_p_4     3_p_5     3_p_6     3_p_7     3_p_8     3_p_9  \\\n",
       "0  0.018638  0.030511  0.000003  0.000012  0.000043  0.448510  0.386060   \n",
       "1  0.037851  0.034649  0.000021  0.000020  0.024327  0.000012  0.000045   \n",
       "2  0.258539  0.058145  0.000012  0.017261  0.000053  0.000006  0.000033   \n",
       "3  0.045720  0.028271  0.000015  0.000038  0.000053  0.000005  0.035371   \n",
       "4  0.102519  0.048516  0.242149  0.490096  0.000049  0.000005  0.000046   \n",
       "\n",
       "     3_p_10    3_p_11  pred_3    day_time  \n",
       "0  0.000015  0.000010       8  2018-11-26  \n",
       "1  0.021735  0.000026       2  2018-11-08  \n",
       "2  0.000016  0.000004       1  2018-10-18  \n",
       "3  0.057778  0.000007       2  2018-10-25  \n",
       "4  0.000010  0.000021       6  2018-10-26  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_tr_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = np.array(second_tr_d['click_mode'])\n",
    "second_tr_d = second_tr_d.drop(['click_mode',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1x=tr1.drop(['min_price', ], axis=1)\n",
    "te1x=te1.drop(['min_price', ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tr['pred_0'] = second_tr['pred_0'].apply(lambda x: int(x))\n",
    "second_tr['pred_1'] = second_tr['pred_1'].apply(lambda x: int(x))\n",
    "second_tr['pred_2'] = second_tr['pred_2'].apply(lambda x: int(x))\n",
    "second_tr['pred_3'] = second_tr['pred_3'].apply(lambda x: int(x))\n",
    "second_tr['pred_5'] = second_tr['pred_5'].apply(lambda x: int(x))\n",
    "second_te['pred_0'] = second_te['pred_0'].apply(lambda x: int(x))\n",
    "second_te['pred_1'] = second_te['pred_1'].apply(lambda x: int(x))\n",
    "second_te['pred_2'] = second_te['pred_2'].apply(lambda x: int(x))\n",
    "second_te['pred_3'] = second_te['pred_3'].apply(lambda x: int(x))\n",
    "second_te['pred_5'] = second_te['pred_5'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(second_tr['pred_0'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_p_0</th>\n",
       "      <th>0_p_1</th>\n",
       "      <th>0_p_2</th>\n",
       "      <th>0_p_3</th>\n",
       "      <th>0_p_4</th>\n",
       "      <th>0_p_5</th>\n",
       "      <th>0_p_6</th>\n",
       "      <th>0_p_7</th>\n",
       "      <th>0_p_8</th>\n",
       "      <th>0_p_9</th>\n",
       "      <th>0_p_10</th>\n",
       "      <th>0_p_11</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>1_p_0</th>\n",
       "      <th>1_p_1</th>\n",
       "      <th>1_p_2</th>\n",
       "      <th>1_p_3</th>\n",
       "      <th>1_p_4</th>\n",
       "      <th>1_p_5</th>\n",
       "      <th>1_p_6</th>\n",
       "      <th>1_p_7</th>\n",
       "      <th>1_p_8</th>\n",
       "      <th>1_p_9</th>\n",
       "      <th>1_p_10</th>\n",
       "      <th>1_p_11</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>2_p_0</th>\n",
       "      <th>2_p_1</th>\n",
       "      <th>2_p_2</th>\n",
       "      <th>2_p_3</th>\n",
       "      <th>2_p_4</th>\n",
       "      <th>2_p_5</th>\n",
       "      <th>2_p_6</th>\n",
       "      <th>2_p_7</th>\n",
       "      <th>2_p_8</th>\n",
       "      <th>2_p_9</th>\n",
       "      <th>2_p_10</th>\n",
       "      <th>2_p_11</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>3_p_0</th>\n",
       "      <th>3_p_1</th>\n",
       "      <th>3_p_2</th>\n",
       "      <th>3_p_3</th>\n",
       "      <th>3_p_4</th>\n",
       "      <th>3_p_5</th>\n",
       "      <th>3_p_6</th>\n",
       "      <th>3_p_7</th>\n",
       "      <th>3_p_8</th>\n",
       "      <th>3_p_9</th>\n",
       "      <th>3_p_10</th>\n",
       "      <th>3_p_11</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>day_time</th>\n",
       "      <th>5_p_0</th>\n",
       "      <th>5_p_1</th>\n",
       "      <th>5_p_2</th>\n",
       "      <th>5_p_3</th>\n",
       "      <th>5_p_4</th>\n",
       "      <th>5_p_5</th>\n",
       "      <th>5_p_6</th>\n",
       "      <th>5_p_7</th>\n",
       "      <th>5_p_8</th>\n",
       "      <th>5_p_9</th>\n",
       "      <th>5_p_10</th>\n",
       "      <th>5_p_11</th>\n",
       "      <th>pred_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033562</td>\n",
       "      <td>0.047144</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.022349</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.873864</td>\n",
       "      <td>1.757926e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.044344</td>\n",
       "      <td>0.050507</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.035109</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.844135</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.056194</td>\n",
       "      <td>0.053114</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.035755</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.830428</td>\n",
       "      <td>2.350842e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>7</td>\n",
       "      <td>0.047073</td>\n",
       "      <td>0.043507</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.027892</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.849509</td>\n",
       "      <td>6.131563e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.031411</td>\n",
       "      <td>0.024484</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.855020</td>\n",
       "      <td>2.849463e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130341</td>\n",
       "      <td>0.520585</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.093939</td>\n",
       "      <td>0.077766</td>\n",
       "      <td>0.035163</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>1.914481e-06</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131059</td>\n",
       "      <td>0.501161</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.092742</td>\n",
       "      <td>0.067975</td>\n",
       "      <td>0.038309</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.143977</td>\n",
       "      <td>0.497717</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.093092</td>\n",
       "      <td>0.048945</td>\n",
       "      <td>0.030602</td>\n",
       "      <td>0.185598</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>9.879611e-07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.108037</td>\n",
       "      <td>0.546799</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.083617</td>\n",
       "      <td>0.064925</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.162060</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>9.971085e-07</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>0.137766</td>\n",
       "      <td>0.517716</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.085009</td>\n",
       "      <td>0.049220</td>\n",
       "      <td>0.027040</td>\n",
       "      <td>0.183161</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1.949927e-06</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049892</td>\n",
       "      <td>0.132524</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>0.025480</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>4.238716e-01</td>\n",
       "      <td>0.332889</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>8</td>\n",
       "      <td>0.056489</td>\n",
       "      <td>0.131915</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.029554</td>\n",
       "      <td>0.031692</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.397685</td>\n",
       "      <td>0.352599</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8</td>\n",
       "      <td>0.051440</td>\n",
       "      <td>0.087051</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.192888e-01</td>\n",
       "      <td>0.380202</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>8</td>\n",
       "      <td>0.059624</td>\n",
       "      <td>0.164638</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.035616</td>\n",
       "      <td>0.037137</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>2.909146e-01</td>\n",
       "      <td>0.411998</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>0.064366</td>\n",
       "      <td>0.171058</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.059308</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>2.830513e-01</td>\n",
       "      <td>0.372248</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042847</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.021508</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.820962</td>\n",
       "      <td>2.984088e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "      <td>0.043479</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.023530</td>\n",
       "      <td>0.023543</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.781998</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.127370</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.029401</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.019535</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.827252</td>\n",
       "      <td>2.177640e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.100457</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>7</td>\n",
       "      <td>0.027291</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>0.024525</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.838425</td>\n",
       "      <td>4.833374e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.087331</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>0.039298</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.025962</td>\n",
       "      <td>0.032515</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>2.892590e-06</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.123653</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.026030</td>\n",
       "      <td>0.019273</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.831006</td>\n",
       "      <td>5.659521e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7</td>\n",
       "      <td>0.142136</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.810355</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.177270</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.035786</td>\n",
       "      <td>0.021141</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.765710</td>\n",
       "      <td>1.628460e-06</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "      <td>0.156387</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.031264</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.778485</td>\n",
       "      <td>2.199086e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>0.093893</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.015023</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.872850</td>\n",
       "      <td>8.010360e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0_p_0     0_p_1     0_p_2     0_p_3     0_p_4     0_p_5     0_p_6  \\\n",
       "0  0.033562  0.047144  0.000008  0.022349  0.023047  0.000003  0.000004   \n",
       "1  0.130341  0.520585  0.000017  0.093939  0.077766  0.035163  0.142137   \n",
       "2  0.049892  0.132524  0.000013  0.035263  0.025480  0.000004  0.000012   \n",
       "3  0.042847  0.000017  0.000007  0.022492  0.021508  0.000003  0.000006   \n",
       "4  0.123653  0.000013  0.000005  0.026030  0.019273  0.000002  0.000003   \n",
       "\n",
       "      0_p_7         0_p_8     0_p_9    0_p_10    0_p_11  pred_0     1_p_0  \\\n",
       "0  0.873864  1.757926e-06  0.000006  0.000007  0.000005       7  0.044344   \n",
       "1  0.000023  1.914481e-06  0.000014  0.000007  0.000007       1  0.131059   \n",
       "2  0.000025  4.238716e-01  0.332889  0.000015  0.000011       8  0.056489   \n",
       "3  0.820962  2.984088e-06  0.000008  0.092140  0.000006       7  0.043479   \n",
       "4  0.831006  5.659521e-07  0.000005  0.000009  0.000002       7  0.142136   \n",
       "\n",
       "      1_p_1     1_p_2     1_p_3     1_p_4     1_p_5     1_p_6     1_p_7  \\\n",
       "0  0.050507  0.000009  0.035109  0.025871  0.000004  0.000002  0.844135   \n",
       "1  0.501161  0.000025  0.092742  0.067975  0.038309  0.168663  0.000024   \n",
       "2  0.131915  0.000015  0.029554  0.031692  0.000002  0.000006  0.000022   \n",
       "3  0.000022  0.000015  0.023530  0.023543  0.000007  0.000013  0.781998   \n",
       "4  0.000022  0.000010  0.027497  0.019931  0.000006  0.000012  0.810355   \n",
       "\n",
       "      1_p_8     1_p_9    1_p_10    1_p_11  pred_1     2_p_0     2_p_1  \\\n",
       "0  0.000001  0.000006  0.000008  0.000005       7  0.056194  0.053114   \n",
       "1  0.000003  0.000025  0.000005  0.000009       1  0.143977  0.497717   \n",
       "2  0.397685  0.352599  0.000013  0.000008       8  0.051440  0.087051   \n",
       "3  0.000007  0.000011  0.127370  0.000005       7  0.029401  0.000017   \n",
       "4  0.000003  0.000011  0.000010  0.000005       7  0.177270  0.000025   \n",
       "\n",
       "      2_p_2     2_p_3     2_p_4     2_p_5     2_p_6     2_p_7         2_p_8  \\\n",
       "0  0.000014  0.035755  0.024457  0.000005  0.000008  0.830428  2.350842e-06   \n",
       "1  0.000022  0.093092  0.048945  0.030602  0.185598  0.000019  9.879611e-07   \n",
       "2  0.000015  0.029175  0.032766  0.000003  0.000012  0.000020  4.192888e-01   \n",
       "3  0.000008  0.019535  0.023310  0.000002  0.000005  0.827252  2.177640e-06   \n",
       "4  0.000009  0.035786  0.021141  0.000012  0.000012  0.765710  1.628460e-06   \n",
       "\n",
       "      2_p_9    2_p_10    2_p_11  pred_2     3_p_0     3_p_1     3_p_2  \\\n",
       "0  0.000008  0.000006  0.000010       7  0.047073  0.043507  0.000009   \n",
       "1  0.000015  0.000004  0.000007       1  0.108037  0.546799  0.000019   \n",
       "2  0.380202  0.000014  0.000013       8  0.059624  0.164638  0.000017   \n",
       "3  0.000007  0.100457  0.000004       7  0.027291  0.000022  0.000014   \n",
       "4  0.000011  0.000017  0.000006       7  0.156387  0.000017  0.000009   \n",
       "\n",
       "      3_p_3     3_p_4     3_p_5     3_p_6     3_p_7         3_p_8     3_p_9  \\\n",
       "0  0.027892  0.031986  0.000003  0.000002  0.849509  6.131563e-07  0.000005   \n",
       "1  0.083617  0.064925  0.034502  0.162060  0.000019  9.971085e-07  0.000012   \n",
       "2  0.035616  0.037137  0.000003  0.000017  0.000025  2.909146e-01  0.411998   \n",
       "3  0.022358  0.024525  0.000004  0.000008  0.838425  4.833374e-06  0.000012   \n",
       "4  0.033800  0.031264  0.000003  0.000005  0.778485  2.199086e-06  0.000006   \n",
       "\n",
       "     3_p_10    3_p_11  pred_3    day_time     5_p_0     5_p_1     5_p_2  \\\n",
       "0  0.000008  0.000005       7  2018-10-07  0.040837  0.048200  0.000011   \n",
       "1  0.000003  0.000006       1  2018-10-13  0.137766  0.517716  0.000023   \n",
       "2  0.000006  0.000006       9  2018-11-26  0.064366  0.171058  0.000016   \n",
       "3  0.087331  0.000005       7  2018-10-21  0.039298  0.000018  0.000007   \n",
       "4  0.000016  0.000005       7  2018-10-28  0.093893  0.000010  0.000005   \n",
       "\n",
       "      5_p_3     5_p_4     5_p_5     5_p_6     5_p_7         5_p_8     5_p_9  \\\n",
       "0  0.031411  0.024484  0.000006  0.000006  0.855020  2.849463e-06  0.000009   \n",
       "1  0.085009  0.049220  0.027040  0.183161  0.000032  1.949927e-06  0.000018   \n",
       "2  0.059308  0.049895  0.000003  0.000016  0.000020  2.830513e-01  0.372248   \n",
       "3  0.025962  0.032515  0.000003  0.000005  0.824782  2.892590e-06  0.000010   \n",
       "4  0.018182  0.015023  0.000002  0.000008  0.872850  8.010360e-07  0.000005   \n",
       "\n",
       "     5_p_10    5_p_11  pred_5  \n",
       "0  0.000005  0.000010       7  \n",
       "1  0.000007  0.000008       1  \n",
       "2  0.000013  0.000007       9  \n",
       "3  0.077393  0.000004       7  \n",
       "4  0.000015  0.000006       7  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 0, 1, ..., 1, 0, 9])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'second_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-28dc4913ab35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain_lgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmit_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lgb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'second_tr' is not defined"
     ]
    }
   ],
   "source": [
    "result_lgb, score =train_lgb(second_tr, train_y, second_te)\n",
    "submit_result(submit, result_lgb, 'lgb', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_tr21 = pd.DataFrame(s_tr21)\n",
    "s_te21 = pd.DataFrame(s_te21)\n",
    "s_tr21.columns = ['p_{}'.format(i) for i in range(12)]\n",
    "s_te21.columns = ['p_{}'.format(i) for i in range(12)]\n",
    "s_tr21['pred'] = pred_tr21\n",
    "s_te21['pred'] = pred_test21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_tr21.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_te2 = s_te2.drop(['day_time'], axis = 1)\n",
    "a = ['p_x{}'.format(i) for i in range(12)]\n",
    "a.append('pred_x')\n",
    "s_te2.columns = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_te22  = pd.concat([s_te21, s_te2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2193953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2296423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2212635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2189960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2281854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sid  pred_1\n",
       "0   2193953       0\n",
       "1   2296423       0\n",
       "4   2212635       0\n",
       "9   2189960       0\n",
       "12  2281854       0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb1['pred' + '_' + str(1)] = 0\n",
    "sb1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_0</th>\n",
       "      <th>p_1</th>\n",
       "      <th>p_2</th>\n",
       "      <th>p_3</th>\n",
       "      <th>p_4</th>\n",
       "      <th>p_5</th>\n",
       "      <th>p_6</th>\n",
       "      <th>p_7</th>\n",
       "      <th>p_8</th>\n",
       "      <th>p_9</th>\n",
       "      <th>p_10</th>\n",
       "      <th>p_11</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064936</td>\n",
       "      <td>0.079495</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.035471</td>\n",
       "      <td>0.068954</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.160410</td>\n",
       "      <td>0.590320</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047631</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.805921</td>\n",
       "      <td>0.051170</td>\n",
       "      <td>0.031765</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.038021</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.025320</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059350</td>\n",
       "      <td>0.312222</td>\n",
       "      <td>0.357617</td>\n",
       "      <td>0.166813</td>\n",
       "      <td>0.089756</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.014082</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.045542</td>\n",
       "      <td>0.694834</td>\n",
       "      <td>0.060587</td>\n",
       "      <td>0.025109</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.037005</td>\n",
       "      <td>0.076146</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.097660</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.084640</td>\n",
       "      <td>0.068969</td>\n",
       "      <td>0.294119</td>\n",
       "      <td>0.453977</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020353</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.194611</td>\n",
       "      <td>0.022212</td>\n",
       "      <td>0.009782</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.752706</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.080680</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.031423</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.822646</td>\n",
       "      <td>0.064766</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.030837</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.856358</td>\n",
       "      <td>0.039914</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.047226</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.024463</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.850570</td>\n",
       "      <td>0.028507</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.069904</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.073524</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.044650</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.860880</td>\n",
       "      <td>0.020614</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        p_0       p_1       p_2       p_3       p_4       p_5       p_6  \\\n",
       "0  0.064936  0.079495  0.000096  0.035471  0.068954  0.000059  0.000053   \n",
       "1  0.047631  0.000078  0.805921  0.051170  0.031765  0.000012  0.000017   \n",
       "2  0.059350  0.312222  0.357617  0.166813  0.089756  0.000018  0.014082   \n",
       "3  0.060613  0.045542  0.694834  0.060587  0.025109  0.000016  0.000053   \n",
       "4  0.097660  0.000180  0.000107  0.084640  0.068969  0.294119  0.453977   \n",
       "5  0.020353  0.000117  0.194611  0.022212  0.009782  0.000030  0.000042   \n",
       "6  0.080680  0.000105  0.000053  0.031423  0.000219  0.822646  0.064766   \n",
       "7  0.030837  0.015010  0.856358  0.039914  0.010488  0.000026  0.000028   \n",
       "8  0.024463  0.000067  0.850570  0.028507  0.026359  0.000016  0.000020   \n",
       "9  0.073524  0.000069  0.000039  0.044650  0.000141  0.860880  0.020614   \n",
       "\n",
       "        p_7       p_8       p_9      p_10      p_11  pred  \n",
       "0  0.000113  0.160410  0.590320  0.000056  0.000036     9  \n",
       "1  0.038021  0.000008  0.000038  0.025320  0.000019     2  \n",
       "2  0.000050  0.000006  0.000043  0.000024  0.000018     2  \n",
       "3  0.000060  0.000019  0.037005  0.076146  0.000016     2  \n",
       "4  0.000113  0.000033  0.000092  0.000051  0.000059     6  \n",
       "5  0.000067  0.000016  0.752706  0.000039  0.000026     9  \n",
       "6  0.000042  0.000006  0.000030  0.000015  0.000017     5  \n",
       "7  0.000056  0.000016  0.047226  0.000025  0.000018     2  \n",
       "8  0.000049  0.000006  0.069904  0.000027  0.000012     2  \n",
       "9  0.000020  0.000003  0.000033  0.000016  0.000011     5  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_tr2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resx = np.array(result_lgb).T\n",
    "for line in resx:\n",
    "        print np.argmax(np.std(line))\n",
    "        print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.613923\n",
      "[20]\tvalid_0's f1_weighted: 0.651579\n",
      "[30]\tvalid_0's f1_weighted: 0.653046\n",
      "[40]\tvalid_0's f1_weighted: 0.653523\n",
      "[50]\tvalid_0's f1_weighted: 0.654577\n",
      "[60]\tvalid_0's f1_weighted: 0.655392\n",
      "[70]\tvalid_0's f1_weighted: 0.656345\n",
      "[80]\tvalid_0's f1_weighted: 0.657102\n",
      "[90]\tvalid_0's f1_weighted: 0.657732\n",
      "[100]\tvalid_0's f1_weighted: 0.658032\n",
      "[110]\tvalid_0's f1_weighted: 0.658493\n",
      "[120]\tvalid_0's f1_weighted: 0.658885\n",
      "[130]\tvalid_0's f1_weighted: 0.659427\n",
      "[140]\tvalid_0's f1_weighted: 0.659805\n",
      "[150]\tvalid_0's f1_weighted: 0.660076\n",
      "[160]\tvalid_0's f1_weighted: 0.660442\n",
      "[170]\tvalid_0's f1_weighted: 0.660816\n",
      "[180]\tvalid_0's f1_weighted: 0.660854\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[180]\tvalid_0's f1_weighted: 0.660854\n",
      "(0, 0.20230982151379212, 0.4562672909129602, 0.6415320167564332, 0.35402906208718626)\n",
      "(1, 0.14622506442683975, 0.673741768579492, 0.6129118528027385, 0.747976501305483)\n",
      "(2, 0.22025388947217714, 0.8748896689040786, 0.8311298311298311, 0.9235136072109551)\n",
      "(3, 0.03152620024816264, 0.03702632340179346, 0.4155844155844156, 0.01937632455343627)\n",
      "(4, 0.017705450033406508, 0.0032275416890801506, 0.75, 0.0016172506738544475)\n",
      "(5, 0.09632528395533073, 0.7963398925065517, 0.7216810240721359, 0.8882282996432818)\n",
      "(6, 0.01577741719958003, 0.17287234042553193, 0.32338308457711445, 0.11796733212341198)\n",
      "(7, 0.13641309535172283, 0.7523691011773714, 0.6915361604786204, 0.8249370277078085)\n",
      "(8, 0.0033024720817027777, 0.2408759124087591, 0.32673267326732675, 0.1907514450867052)\n",
      "(9, 0.09528490980242436, 0.7672829413490965, 0.6597692862292718, 0.9166583191425424)\n",
      "(10, 0.01916579173427508, 0.5059582919563059, 0.5044554455445545, 0.5074701195219123)\n",
      "(11, 0.015710604180586046, 0.6642030056664203, 0.5586406962287609, 0.818955042527339)\n",
      "0.6608538213531733\n",
      "180\n",
      "fit over\n"
     ]
    }
   ],
   "source": [
    "result_lgb, score, imp = train_lgb_time_serie(second_trx, train_y,second_tex)#用来测试特征重要性\n",
    "submit = submit_result(submit, result_lgb, 'lgb_time', score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x121239450>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hourDF = tr1.groupby(['first_mode', 'click_mode'])['first_mode'].count().unstack('click_mode').fillna(0)\n",
    "hourDF.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels, preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "\n",
    "def lgb_sample_select(train_x, train_y, test_x):\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    \n",
    "    train_index = (train_x['day_time'] <= '2018-11-23')\n",
    "    valid_index = (train_x['day_time'] > '2018-11-23') & (train_x['day_time'] < '2018-12-01')\n",
    "    \n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    \n",
    "    tr_x1     = train_x[train_index]\n",
    "    tr_y1     = train_y[train_index]\n",
    "    val_x     = train_x[valid_index]\n",
    "    val_y     = train_y[valid_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    cate_cols = ['max_dist_mode', 'min_dist_mode', #'max_price_mode',\n",
    "                 'min_price_mode', 'max_eta_mode', 'min_eta_mode',\n",
    "                 'first_mode','last_mode', 'week', ]\n",
    "    scores = []\n",
    "    result = []\n",
    "    \n",
    "    select = pd.DataFrame()\n",
    "    for i in range(5):\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=41,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=2000, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.1, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "        \n",
    "        tr_x, tr_y, all_train_x, all_train_y = gen_data(tr_x1, tr_y1, val_x, val_y)\n",
    "        \n",
    "        eval_set = [(val_x, val_y)]\n",
    "        lgb_model.fit(\n",
    "            tr_x, tr_y, eval_set=eval_set,\n",
    "            eval_metric=f1_weighted,\n",
    "            categorical_feature=cate_cols,\n",
    "            verbose=50, early_stopping_rounds=50)\n",
    "        \n",
    "        \n",
    "        def get_weighted_fscore(y_pred, y_true):\n",
    "            f_score = 0\n",
    "            for i in range(12):\n",
    "                yt = y_true == i\n",
    "                yp = y_pred == i\n",
    "                f_score += dic_[i] * f1_score(y_true=yt, y_pred= yp)\n",
    "                print(i,dic_[i],f1_score(y_true=yt, y_pred= yp), precision_score(y_true=yt, y_pred= yp),recall_score(y_true=yt, y_pred= yp))\n",
    "            print(f_score)\n",
    "            return f_score\n",
    "        \n",
    "        val_pred = lgb_model.predict(val_x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = lgb_model.predict(val_x) \n",
    "        df_analysis = pd.DataFrame()\n",
    "        #df_analysis['sid']   = val_x['sid']\n",
    "        df_analysis['label'] = val_y\n",
    "        df_analysis['pred']  = pred\n",
    "        df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "        dic_ = df_analysis['label'].value_counts(normalize = True)\n",
    "        \n",
    "        val_score = get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "        \n",
    "        if(val_score > 0.66395):\n",
    "            df1 = pd.concat([tr_x, tr_y], axis=1)\n",
    "            \n",
    "            select = pd.concat([select, df1], axis=0)\n",
    "            select.drop_duplicates(keep='first',inplace=True)\n",
    "    select.to_csv('./data/selected_samples.csv', index=False)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12051ce90>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hourDF = tr1.groupby(['rank2_t', 'click_mode'])['rank2_t'].count().unstack('click_mode').fillna(0)\n",
    "hourDF.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x121154fd0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hourDF = tr1.groupby(['last_mode', 'click_mode'])['last_mode'].count().unstack('click_mode').fillna(0)\n",
    "hourDF.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353719"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.660105\n",
      "[100]\tvalid_0's f1_weighted: 0.662656\n",
      "[150]\tvalid_0's f1_weighted: 0.663351\n",
      "[200]\tvalid_0's f1_weighted: 0.663552\n",
      "[250]\tvalid_0's f1_weighted: 0.663936\n",
      "[300]\tvalid_0's f1_weighted: 0.664107\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid_0's f1_weighted: 0.664377\n",
      "(0, 0.20230982151379212, 0.4559170495467528, 0.6665758896151053, 0.3464332892998679)\n",
      "(1, 0.14622506442683975, 0.6705946337926032, 0.6242686768676867, 0.7243472584856396)\n",
      "(2, 0.22025388947217714, 0.8751364375888133, 0.8338369765707783, 0.920740162939851)\n",
      "(3, 0.03152620024816264, 0.08078660643103906, 0.33043478260869563, 0.04601877081441114)\n",
      "(4, 0.017705450033406508, 0.04523107177974434, 0.2569832402234637, 0.024797843665768194)\n",
      "(5, 0.09632528395533073, 0.7963071380319748, 0.7298769916618509, 0.8760404280618311)\n",
      "(6, 0.01577741719958003, 0.2669245647969052, 0.2857142857142857, 0.25045372050816694)\n",
      "(7, 0.13641309535172283, 0.7507414571244359, 0.696078431372549, 0.8147215225300868)\n",
      "(8, 0.0033024720817027777, 0.3124231242312423, 0.27194860813704497, 0.3670520231213873)\n",
      "(9, 0.09528490980242436, 0.7673317551572539, 0.6637899509983178, 0.909145547430632)\n",
      "(10, 0.01916579173427508, 0.5289993626513704, 0.46128195628010377, 0.6200199203187251)\n",
      "(11, 0.015710604180586046, 0.6601142857142857, 0.5291315500183217, 0.8772782503037667)\n",
      "0.6643774248348499\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.659572\n",
      "[100]\tvalid_0's f1_weighted: 0.662153\n",
      "[150]\tvalid_0's f1_weighted: 0.662858\n",
      "[200]\tvalid_0's f1_weighted: 0.663303\n",
      "[250]\tvalid_0's f1_weighted: 0.664002\n",
      "[300]\tvalid_0's f1_weighted: 0.664228\n",
      "Early stopping, best iteration is:\n",
      "[296]\tvalid_0's f1_weighted: 0.664417\n",
      "(0, 0.20230982151379212, 0.4552643048378059, 0.6692293611568735, 0.3449707491979619)\n",
      "(1, 0.14622506442683975, 0.6712146824438541, 0.6243261455525606, 0.7257180156657963)\n",
      "(2, 0.22025388947217714, 0.875193048205424, 0.8337976223172597, 0.920913503206795)\n",
      "(3, 0.03152620024816264, 0.07999999999999999, 0.3199152542372881, 0.0457160157432637)\n",
      "(4, 0.017705450033406508, 0.044987775061124696, 0.24210526315789474, 0.024797843665768194)\n",
      "(5, 0.09632528395533073, 0.793781353098025, 0.72970502700457, 0.8701942132382084)\n",
      "(6, 0.01577741719958003, 0.2788794460182562, 0.29068241469816275, 0.26799758015728975)\n",
      "(7, 0.13641309535172283, 0.7518292879476517, 0.6970294662602355, 0.8159809683739154)\n",
      "(8, 0.0033024720817027777, 0.2935064935064935, 0.2665094339622642, 0.3265895953757225)\n",
      "(9, 0.09528490980242436, 0.7675310984608896, 0.662758520244684, 0.9116498046679354)\n",
      "(10, 0.01916579173427508, 0.5320906587587376, 0.462956137117582, 0.6254980079681275)\n",
      "(11, 0.015710604180586046, 0.6593507087334248, 0.5285923753665689, 0.8760631834750912)\n",
      "0.6644168853549784\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.660188\n",
      "[100]\tvalid_0's f1_weighted: 0.662798\n",
      "[150]\tvalid_0's f1_weighted: 0.663214\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's f1_weighted: 0.663542\n",
      "(0, 0.20230982151379212, 0.45240410249976476, 0.6748385889398334, 0.34025287790149084)\n",
      "(1, 0.14622506442683975, 0.6713021491782554, 0.6228912970617808, 0.7278720626631854)\n",
      "(2, 0.22025388947217714, 0.8753681695536654, 0.8341511285574092, 0.920870168140059)\n",
      "(3, 0.03152620024816264, 0.07229565686538979, 0.3316831683168317, 0.04056917953375719)\n",
      "(4, 0.017705450033406508, 0.0380952380952381, 0.2714285714285714, 0.020485175202156335)\n",
      "(5, 0.09632528395533073, 0.7961068805479206, 0.7300223122056029, 0.8753468093539437)\n",
      "(6, 0.01577741719958003, 0.2718995290423862, 0.28263707571801566, 0.2619479733817302)\n",
      "(7, 0.13641309535172283, 0.7506693764314977, 0.6964146764829113, 0.8140917996081724)\n",
      "(8, 0.0033024720817027777, 0.32911392405063294, 0.2734225621414914, 0.41329479768786126)\n",
      "(9, 0.09528490980242436, 0.7675534601784813, 0.6619472881725115, 0.9132525292998097)\n",
      "(10, 0.01916579173427508, 0.5293870696893368, 0.45754716981132076, 0.6279880478087649)\n",
      "(11, 0.015710604180586046, 0.6589673913043478, 0.5252707581227437, 0.8839611178614823)\n",
      "0.6635422147997652\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.65972\n",
      "[100]\tvalid_0's f1_weighted: 0.662118\n",
      "[150]\tvalid_0's f1_weighted: 0.663002\n",
      "[200]\tvalid_0's f1_weighted: 0.662943\n",
      "Early stopping, best iteration is:\n",
      "[175]\tvalid_0's f1_weighted: 0.663331\n",
      "(0, 0.20230982151379212, 0.45297385009049496, 0.6689400921658987, 0.3424230986978675)\n",
      "(1, 0.14622506442683975, 0.6715315260983626, 0.6240542509667657, 0.7268276762402088)\n",
      "(2, 0.22025388947217714, 0.8754863012288755, 0.8337972946481083, 0.921563529207835)\n",
      "(3, 0.03152620024816264, 0.07221182134260497, 0.30963302752293576, 0.04087193460490463)\n",
      "(4, 0.017705450033406508, 0.04375932371954251, 0.28205128205128205, 0.02371967654986523)\n",
      "(5, 0.09632528395533073, 0.7920747188973521, 0.7301069876295553, 0.8655370590566785)\n",
      "(6, 0.01577741719958003, 0.27627818355033346, 0.2907754010695187, 0.2631578947368421)\n",
      "(7, 0.13641309535172283, 0.7510461597888367, 0.6954811017050196, 0.8162608452280996)\n",
      "(8, 0.0033024720817027777, 0.32886723507917176, 0.28421052631578947, 0.3901734104046243)\n",
      "(9, 0.09528490980242436, 0.7658464773922188, 0.660092807424594, 0.9119503155364119)\n",
      "(10, 0.01916579173427508, 0.5290131300296486, 0.46020633750921147, 0.6220119521912351)\n",
      "(11, 0.015710604180586046, 0.6560870550895488, 0.5233273056057867, 0.8791008505467801)\n",
      "0.6633308819895068\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.660897\n",
      "[100]\tvalid_0's f1_weighted: 0.66299\n",
      "[150]\tvalid_0's f1_weighted: 0.663453\n",
      "[200]\tvalid_0's f1_weighted: 0.66359\n",
      "[250]\tvalid_0's f1_weighted: 0.663995\n",
      "[300]\tvalid_0's f1_weighted: 0.663544\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's f1_weighted: 0.664075\n",
      "(0, 0.20230982151379212, 0.45389013174248083, 0.6647251547142337, 0.3445933194942442)\n",
      "(1, 0.14622506442683975, 0.6717330749666949, 0.6264400271063926, 0.7240861618798956)\n",
      "(2, 0.22025388947217714, 0.8740557785324687, 0.8323728880003136, 0.9201334720055468)\n",
      "(3, 0.03152620024816264, 0.0805687203791469, 0.3090909090909091, 0.04632152588555858)\n",
      "(4, 0.017705450033406508, 0.0466038671294001, 0.29012345679012347, 0.025336927223719677)\n",
      "(5, 0.09632528395533073, 0.7965250441156508, 0.7329502872845366, 0.8721759809750297)\n",
      "(6, 0.01577741719958003, 0.28046850269072493, 0.29415670650730413, 0.26799758015728975)\n",
      "(7, 0.13641309535172283, 0.7510711639444605, 0.6959818496626664, 0.8156311223061853)\n",
      "(8, 0.0033024720817027777, 0.3128342245989305, 0.291044776119403, 0.33815028901734107)\n",
      "(9, 0.09528490980242436, 0.766676499094012, 0.6616962467267966, 0.9112491235099669)\n",
      "(10, 0.01916579173427508, 0.5278481012658228, 0.45790629575402636, 0.62300796812749)\n",
      "(11, 0.015710604180586046, 0.6578947368421053, 0.5249818971759594, 0.8809234507897934)\n",
      "0.6640748864642911\n"
     ]
    }
   ],
   "source": [
    "lgb_sample_select(tr1, y1,te1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels, preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "\n",
    "def select(train_x, train_y):\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=41,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=2000, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.05, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "    \n",
    "    train_index = (train_x['day_time'] <= '2018-11-23')\n",
    "    valid_index = (train_x['day_time'] > '2018-11-23') & (train_x['day_time'] < '2018-12-01')\n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    \n",
    "    tr_x     = train_x[train_index]\n",
    "    tr_y     = train_y[train_index]\n",
    "    val_x     = train_x[valid_index]\n",
    "    val_y     = train_y[valid_index]\n",
    "\n",
    "    cate_cols = ['max_dist_mode', 'min_dist_mode', 'max_price_mode',\n",
    "                 'min_price_mode', 'max_eta_mode', 'min_eta_mode',\n",
    "                 'first_mode', 'last_mode','week']\n",
    "        \n",
    "        \n",
    "    print('Fiting...')\n",
    "    eval_set = [(val_x, val_y)]\n",
    "    lgb_model.fit(\n",
    "        tr_x, tr_y, eval_set=eval_set,\n",
    "        eval_metric=f1_weighted,\n",
    "        categorical_feature=cate_cols,\n",
    "        verbose=10, early_stopping_rounds=50)\n",
    "    \n",
    "    pred = lgb_model.predict(val_x) \n",
    "    df_analysis = pd.DataFrame()\n",
    "    #df_analysis['sid']   = val_x['sid']\n",
    "    df_analysis['label'] = val_y\n",
    "    df_analysis['pred']  = pred\n",
    "    df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "    dic_ = df_analysis['label'].value_counts(normalize = True)\n",
    "    \n",
    "    def get_weighted_fscore(y_pred, y_true):\n",
    "        f_score = 0\n",
    "        for i in range(12):\n",
    "            yt = y_true == i\n",
    "            yp = y_pred == i\n",
    "            f_score += dic_[i] * f1_score(y_true=yt, y_pred= yp)\n",
    "            print(i,dic_[i],f1_score(y_true=yt, y_pred= yp), precision_score(y_true=yt, y_pred= yp),recall_score(y_true=yt, y_pred= yp))\n",
    "        print(f_score)\n",
    "        return f_score\n",
    "    baseloss = get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "    \n",
    "    imp = pd.DataFrame()\n",
    "    imp['fea'] = list(val_x.columns)\n",
    "    imp['imp'] = lgb_model.feature_importances_\n",
    "    #imp = imp[imp['imp']>0]\n",
    "    imp = imp.sort_values('imp',ascending = False)\n",
    "    print('feature number',imp.shape[0])\n",
    "    n = lgb_model.best_iteration_\n",
    "    print('base f1 score:',baseloss)\n",
    "    col =list(imp['fea'])\n",
    "    col = [i for i in col if i not in cate_cols] \n",
    "    \n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=61,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=500, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.05, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "    \n",
    "\n",
    "    def evalsLoss(cols):\n",
    "        print('Runing...')\n",
    "        s = time.time()\n",
    "        colsx = list(set(cols).union(set(cate_cols)))\n",
    "        val_x1 = val_x[colsx].copy()\n",
    "        tr_x1 = tr_x[colsx].copy()\n",
    "        eval_set = [(val_x1, val_y)]\n",
    "        lgb_model.fit(tr_x1, tr_y, eval_set=eval_set,\n",
    "                      eval_metric=f1_weighted,\n",
    "                      categorical_feature=cate_cols,\n",
    "                      verbose=10, early_stopping_rounds=50)\n",
    "        pred = lgb_model.predict(val_x1)\n",
    "        df_analysis = pd.DataFrame()\n",
    "        df_analysis['label'] = val_y\n",
    "        df_analysis['pred']  = pred\n",
    "        df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "        print(time.time()-s,\"s\")\n",
    "        return get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "    \n",
    "    print('start to choose...')\n",
    "    all_num = int(len(col)/10)*10\n",
    "    print('total number of',all_num,'to computer')\n",
    "    loss = []\n",
    "    best_num = all_num\n",
    "    break_num = 0\n",
    "    for i in range(110,all_num,10):\n",
    "        loss.append(evalsLoss(col[:i]))\n",
    "        if loss[-1]>baseloss:\n",
    "            best_num = i\n",
    "            baseloss = loss[-1]\n",
    "        print('the beginning',i,'feature',loss[-1],'scored as',baseloss)\n",
    "    print('筛选出来最佳特征个数为',best_num,'这下子训练速度终于可以大大提升了')\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     34055\n",
      "7     22518\n",
      "1     17860\n",
      "5     10880\n",
      "10     3419\n",
      "0      2158\n",
      "9      1832\n",
      "3       558\n",
      "6       389\n",
      "11      351\n",
      "8       255\n",
      "4        83\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "res = pd.value_counts(result_lgb)\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels, preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "   \n",
    "def train_lgb_time_serie(train_x, train_y, test_x): \n",
    "    \n",
    "    all_train_x = train_x\n",
    "    all_train_y = train_y\n",
    "\n",
    "    train_index = (train_x['day_time'] <= '2018-11-23')\n",
    "    valid_index = (train_x['day_time'] > '2018-11-23') & (train_x['day_time'] < '2018-12-01')\n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    all_train_x = all_train_x.drop(['day_time'], axis = 1)\n",
    "    \n",
    "    \n",
    "    tr_x     = train_x[train_index]\n",
    "    tr_y     = train_y[train_index]\n",
    "    val_x     = train_x[valid_index]\n",
    "    val_y     = train_y[valid_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    cate_cols = ['pred_0','pred_1','cpred_0']\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=41,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=100, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.2, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    eval_set = [(val_x, val_y)]\n",
    "    lgb_model.fit(\n",
    "        tr_x, tr_y, eval_set=eval_set,\n",
    "        eval_metric=f1_weighted,\n",
    "        categorical_feature=cate_cols,\n",
    "        verbose=10, early_stopping_rounds=50)\n",
    "    \n",
    "    \n",
    "    pred = lgb_model.predict(val_x) \n",
    "    df_analysis = pd.DataFrame()\n",
    "    #df_analysis['sid']   = val_x['sid']\n",
    "    df_analysis['label'] = val_y\n",
    "    df_analysis['pred']  = pred\n",
    "    df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "    dic_ = df_analysis['label'].value_counts(normalize = True)\n",
    "    \n",
    "    def get_weighted_fscore(y_pred, y_true):\n",
    "        f_score = 0\n",
    "        for i in range(12):\n",
    "            yt = y_true == i\n",
    "            yp = y_pred == i\n",
    "            f_score += dic_[i] * f1_score(y_true=yt, y_pred= yp)\n",
    "            print(i,dic_[i],f1_score(y_true=yt, y_pred= yp), precision_score(y_true=yt, y_pred= yp),recall_score(y_true=yt, y_pred= yp))\n",
    "        print(f_score)\n",
    "        return f_score\n",
    "    score = get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "    \n",
    "    imp = pd.DataFrame()\n",
    "    imp['fea'] = list(val_x.columns)\n",
    "    imp['imp'] = lgb_model.feature_importances_ \n",
    "    imp = imp.sort_values('imp',ascending = False)\n",
    "    \n",
    "    print(lgb_model.best_iteration_)\n",
    "    lgb_model.n_estimators = lgb_model.best_iteration_\n",
    "    #lgb_model.fit(all_train_x, all_train_y, categorical_feature=cate_cols)\n",
    "    print('fit over')\n",
    "    pred_test = lgb_model.predict(test_x)\n",
    "    return pred_test, score, imp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                fea       val\n",
      "1        click_mode  1.000000\n",
      "172            fm_O  0.640902\n",
      "95       first_mode  0.640898\n",
      "52       mode_7_dis  0.407287\n",
      "53       mode_7_eta  0.406561\n",
      "54           rank_7  0.406467\n",
      "51        mode_7_pr  0.406464\n",
      "89    max_dist_mode  0.312223\n",
      "62       mode_9_dis  0.278280\n",
      "63       mode_9_eta  0.277675\n",
      "61        mode_9_pr  0.277606\n",
      "64           rank_9  0.277594\n",
      "26        mode_2_pr  0.243237\n",
      "29           rank_2  0.243235\n",
      "28       mode_2_eta  0.243084\n",
      "27       mode_2_dis  0.242443\n",
      "65            t*d_9  0.228915\n",
      "55            t*d_7  0.228136\n",
      "67      mode_10_dis  0.219395\n",
      "68      mode_10_eta  0.219091\n",
      "66       mode_10_pr  0.219054\n",
      "69          rank_10  0.219049\n",
      "86         mean_eta  0.212682\n",
      "85          min_eta  0.199715\n",
      "84          max_eta  0.197451\n",
      "70           t*d_10  0.188711\n",
      "76         max_dist  0.185831\n",
      "78        mean_dist  0.181788\n",
      "77         min_dist  0.175519\n",
      "80        max_price  0.173600\n",
      "4         time_diff  0.172313\n",
      "88      mode_length  0.161333\n",
      "45            t*d_5  0.153418\n",
      "82       mean_price  0.151261\n",
      "83        std_price  0.147644\n",
      "174      min_etam_O  0.138065\n",
      "94     min_eta_mode  0.138015\n",
      "176       min_dis_D  0.136378\n",
      "173      min_dism_O  0.136349\n",
      "90    min_dist_mode  0.136298\n",
      "177         dis/eta  0.135375\n",
      "40            t*d_4  0.130897\n",
      "47       mode_6_dis  0.129880\n",
      "48       mode_6_eta  0.129839\n",
      "49           rank_6  0.129821\n",
      "46        mode_6_pr  0.129820\n",
      "35            t*d_3  0.126768\n",
      "72      mode_11_dis  0.123520\n",
      "73      mode_11_eta  0.122862\n",
      "42       mode_5_dis  0.122805\n",
      "71       mode_11_pr  0.122784\n",
      "43       mode_5_eta  0.122784\n",
      "74          rank_11  0.122776\n",
      "44           rank_5  0.122599\n",
      "41        mode_5_pr  0.122595\n",
      "32       mode_3_dis  0.120937\n",
      "91   max_price_mode  0.117320\n",
      "87          std_eta  0.115315\n",
      "33       mode_3_eta  0.111186\n",
      "31        mode_3_pr  0.110102\n",
      "34           rank_3  0.110101\n",
      "79         std_dist  0.106629\n",
      "75           t*d_11  0.102851\n",
      "50            t*d_6  0.101235\n",
      "30            t*d_2  0.098586\n",
      "92   min_price_mode  0.093862\n",
      "93     max_eta_mode  0.086022\n",
      "98           max_OP  0.084568\n",
      "101          max_DP  0.082348\n",
      "100       second_OP  0.077288\n",
      "15         line_dis  0.072854\n",
      "24           rank_1  0.068709\n",
      "21        mode_1_pr  0.068666\n",
      "23       mode_1_eta  0.068111\n",
      "22       mode_1_dis  0.066277\n",
      "103       second_DP  0.057371\n",
      "104              p0  0.045467\n",
      "25            t*d_1  0.043799\n",
      "57       mode_8_dis  0.041015\n",
      "58       mode_8_eta  0.040927\n",
      "56        mode_8_pr  0.040927\n",
      "59           rank_8  0.040911\n",
      "37       mode_4_dis  0.040578\n",
      "96        last_mode  0.035709\n",
      "36        mode_4_pr  0.035572\n",
      "137             p33  0.034918\n",
      "38       mode_4_eta  0.034048\n",
      "102        max_DP_m  0.033762\n",
      "114             p10  0.033666\n",
      "60            t*d_8  0.033446\n",
      "39           rank_4  0.033327\n",
      "135             p31  0.032095\n",
      "113              p9  0.031881\n",
      "134             p30  0.031724\n",
      "106              p2  0.028059\n",
      "175      max_OP_m*O  0.027348\n",
      "99         max_OP_m  0.027310\n",
      "136             p32  0.026288\n",
      "126             p22  0.024475\n",
      "167             p63  0.020718\n",
      "115             p11  0.020654\n",
      "123             p19  0.018993\n",
      "151             p47  0.018371\n",
      "107              p3  0.018219\n",
      "147             p43  0.015765\n",
      "81        min_price  0.015181\n",
      "121             p17  0.014809\n",
      "155             p51  0.014339\n",
      "158             p54  0.014087\n",
      "143             p39  0.013674\n",
      "152             p48  0.013352\n",
      "132             p28  0.013287\n",
      "168             p64  0.011922\n",
      "110              p6  0.011453\n",
      "165             p61  0.011147\n",
      "108              p4  0.011104\n",
      "122             p18  0.010736\n",
      "160             p56  0.010661\n",
      "118             p14  0.010566\n",
      "7                o2  0.010156\n",
      "138             p34  0.009955\n",
      "163             p59  0.009947\n",
      "156             p52  0.009364\n",
      "119             p15  0.009327\n",
      "117             p13  0.009232\n",
      "140             p36  0.009117\n",
      "130             p26  0.008889\n",
      "139             p35  0.008733\n",
      "8          o2_trans  0.008398\n",
      "133             p29  0.008096\n",
      "18            o1*o2  0.008084\n",
      "124             p20  0.008080\n",
      "129             p25  0.007741\n",
      "109              p5  0.007567\n",
      "150             p46  0.007022\n",
      "166             p62  0.006870\n",
      "9                d1  0.006417\n",
      "17            d1&d2  0.006416\n",
      "10         d1_trans  0.006391\n",
      "12         d2_trans  0.006240\n",
      "148             p44  0.006081\n",
      "120             p16  0.006070\n",
      "164             p60  0.006021\n",
      "111              p7  0.005688\n",
      "20      d2*o2*d1*o1  0.005597\n",
      "105              p1  0.005547\n",
      "112              p8  0.005421\n",
      "154             p50  0.004837\n",
      "170             OTH  0.004796\n",
      "171             OTM  0.004747\n",
      "2              hour  0.004726\n",
      "3              mins  0.004679\n",
      "14            d2-o2  0.004673\n",
      "11               d2  0.004456\n",
      "142             p38  0.004425\n",
      "157             p53  0.003941\n",
      "131             p27  0.003582\n",
      "5                o1  0.003451\n",
      "16            o1&o2  0.003450\n",
      "125             p21  0.003398\n",
      "145             p41  0.003129\n",
      "97      second_mode  0.003081\n",
      "146             p42  0.003081\n",
      "0               pid  0.003059\n",
      "169             p65  0.002986\n",
      "13            d1-o1  0.002955\n",
      "128             p24  0.002727\n",
      "153             p49  0.002713\n",
      "162             p58  0.002628\n",
      "149             p45  0.002224\n",
      "161             p57  0.002001\n",
      "19            d1*d2  0.001724\n",
      "141             p37  0.001678\n",
      "144             p40  0.001398\n",
      "6          o1_trans  0.001051\n",
      "127             p23  0.000959\n",
      "159             p55  0.000829\n",
      "116             p12  0.000743\n"
     ]
    }
   ],
   "source": [
    "train_x = train_data.drop(['day_time','week'],axis =1)\n",
    "\n",
    "train_x.head()\n",
    "train_x = train_x.loc[0:30000]\n",
    "a = list(train_x.corrwith(train_x.click_mode).index)\n",
    "cor_f = pd.DataFrame()\n",
    "cor_f['fea'] =  a\n",
    "cor_f['val'] =  abs(train_x.corrwith(train_x.click_mode).values)\n",
    "print cor_f.sort_values('val',ascending = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>mode_1_pr</th>\n",
       "      <th>mode_1_dis</th>\n",
       "      <th>mode_1_eta</th>\n",
       "      <th>mode_2_pr</th>\n",
       "      <th>mode_2_dis</th>\n",
       "      <th>mode_2_eta</th>\n",
       "      <th>mode_3_pr</th>\n",
       "      <th>mode_3_dis</th>\n",
       "      <th>mode_3_eta</th>\n",
       "      <th>mode_4_pr</th>\n",
       "      <th>mode_4_dis</th>\n",
       "      <th>mode_4_eta</th>\n",
       "      <th>mode_5_pr</th>\n",
       "      <th>mode_5_dis</th>\n",
       "      <th>mode_5_eta</th>\n",
       "      <th>mode_6_pr</th>\n",
       "      <th>mode_6_dis</th>\n",
       "      <th>mode_6_eta</th>\n",
       "      <th>mode_7_pr</th>\n",
       "      <th>mode_7_dis</th>\n",
       "      <th>mode_7_eta</th>\n",
       "      <th>mode_8_pr</th>\n",
       "      <th>mode_8_dis</th>\n",
       "      <th>mode_8_eta</th>\n",
       "      <th>mode_9_pr</th>\n",
       "      <th>mode_9_dis</th>\n",
       "      <th>mode_9_eta</th>\n",
       "      <th>mode_10_pr</th>\n",
       "      <th>mode_10_dis</th>\n",
       "      <th>mode_10_eta</th>\n",
       "      <th>mode_11_pr</th>\n",
       "      <th>mode_11_dis</th>\n",
       "      <th>mode_11_eta</th>\n",
       "      <th>max_dist</th>\n",
       "      <th>min_dist</th>\n",
       "      <th>std_dist</th>\n",
       "      <th>std_price</th>\n",
       "      <th>max_eta</th>\n",
       "      <th>min_eta</th>\n",
       "      <th>std_eta</th>\n",
       "      <th>max_dist_mode</th>\n",
       "      <th>min_dist_mode</th>\n",
       "      <th>max_price_mode</th>\n",
       "      <th>min_price_mode</th>\n",
       "      <th>max_eta_mode</th>\n",
       "      <th>min_eta_mode</th>\n",
       "      <th>first_mode</th>\n",
       "      <th>last_mode</th>\n",
       "      <th>second_mode</th>\n",
       "      <th>day-time</th>\n",
       "      <th>month</th>\n",
       "      <th>vac</th>\n",
       "      <th>week</th>\n",
       "      <th>hour</th>\n",
       "      <th>subway_zone</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>o1</th>\n",
       "      <th>o2</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d1-o1</th>\n",
       "      <th>d2-o2</th>\n",
       "      <th>svd_fea_0</th>\n",
       "      <th>svd_fea_1</th>\n",
       "      <th>svd_fea_2</th>\n",
       "      <th>svd_fea_3</th>\n",
       "      <th>svd_fea_4</th>\n",
       "      <th>svd_fea_5</th>\n",
       "      <th>svd_fea_6</th>\n",
       "      <th>svd_fea_7</th>\n",
       "      <th>svd_fea_8</th>\n",
       "      <th>svd_fea_9</th>\n",
       "      <th>svd_fea_10</th>\n",
       "      <th>svd_fea_11</th>\n",
       "      <th>svd_fea_12</th>\n",
       "      <th>svd_fea_13</th>\n",
       "      <th>svd_fea_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>30</td>\n",
       "      <td>521</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>453</td>\n",
       "      <td>103</td>\n",
       "      <td>210</td>\n",
       "      <td>453</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>404</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>124</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>30</td>\n",
       "      <td>521</td>\n",
       "      <td>136</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>5219.0</td>\n",
       "      <td>4046.0</td>\n",
       "      <td>467.713825</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>3672.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>914.260433</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-11-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>116.29</td>\n",
       "      <td>39.97</td>\n",
       "      <td>116.32</td>\n",
       "      <td>39.96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210736.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1124</td>\n",
       "      <td>357</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1110</td>\n",
       "      <td>112</td>\n",
       "      <td>290</td>\n",
       "      <td>1110</td>\n",
       "      <td>136</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>60</td>\n",
       "      <td>1386</td>\n",
       "      <td>322</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>40</td>\n",
       "      <td>1348</td>\n",
       "      <td>231</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>13864.0</td>\n",
       "      <td>11106.0</td>\n",
       "      <td>1243.375342</td>\n",
       "      <td>1048.045801</td>\n",
       "      <td>3578.0</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>974.408251</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>116.39</td>\n",
       "      <td>39.84</td>\n",
       "      <td>116.33</td>\n",
       "      <td>39.79</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1.659321</td>\n",
       "      <td>0.658922</td>\n",
       "      <td>-0.722464</td>\n",
       "      <td>0.950416</td>\n",
       "      <td>-0.391069</td>\n",
       "      <td>0.522419</td>\n",
       "      <td>0.059124</td>\n",
       "      <td>-0.103123</td>\n",
       "      <td>0.576893</td>\n",
       "      <td>0.834474</td>\n",
       "      <td>0.564825</td>\n",
       "      <td>-0.659383</td>\n",
       "      <td>-0.606791</td>\n",
       "      <td>0.635751</td>\n",
       "      <td>0.465786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>40</td>\n",
       "      <td>1229</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>1130</td>\n",
       "      <td>140</td>\n",
       "      <td>370</td>\n",
       "      <td>1130</td>\n",
       "      <td>164</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>902</td>\n",
       "      <td>272</td>\n",
       "      <td>60</td>\n",
       "      <td>1301</td>\n",
       "      <td>365</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>40</td>\n",
       "      <td>1229</td>\n",
       "      <td>247</td>\n",
       "      <td>170</td>\n",
       "      <td>1299</td>\n",
       "      <td>234</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>13018.0</td>\n",
       "      <td>9023.0</td>\n",
       "      <td>1286.001936</td>\n",
       "      <td>1233.710183</td>\n",
       "      <td>3657.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>693.587715</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>116.31</td>\n",
       "      <td>39.93</td>\n",
       "      <td>116.27</td>\n",
       "      <td>40.00</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202427.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1364</td>\n",
       "      <td>398</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1473</td>\n",
       "      <td>194</td>\n",
       "      <td>310</td>\n",
       "      <td>1473</td>\n",
       "      <td>206</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>60</td>\n",
       "      <td>1512</td>\n",
       "      <td>374</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>170</td>\n",
       "      <td>1485</td>\n",
       "      <td>291</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>15124.0</td>\n",
       "      <td>13640.0</td>\n",
       "      <td>508.944241</td>\n",
       "      <td>1135.957746</td>\n",
       "      <td>3982.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>836.608152</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>116.27</td>\n",
       "      <td>39.88</td>\n",
       "      <td>116.39</td>\n",
       "      <td>39.90</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.299006</td>\n",
       "      <td>-0.720142</td>\n",
       "      <td>-1.369530</td>\n",
       "      <td>-0.654634</td>\n",
       "      <td>-0.417466</td>\n",
       "      <td>-0.535664</td>\n",
       "      <td>0.228437</td>\n",
       "      <td>0.477593</td>\n",
       "      <td>-0.157794</td>\n",
       "      <td>0.066474</td>\n",
       "      <td>0.079345</td>\n",
       "      <td>0.053064</td>\n",
       "      <td>-0.111287</td>\n",
       "      <td>-0.053937</td>\n",
       "      <td>0.283331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172251.0</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>60</td>\n",
       "      <td>1320</td>\n",
       "      <td>336</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>13203.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6521.737551</td>\n",
       "      <td>1577.973384</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1586.413782</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>116.34</td>\n",
       "      <td>39.96</td>\n",
       "      <td>116.37</td>\n",
       "      <td>39.86</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.662153</td>\n",
       "      <td>0.925719</td>\n",
       "      <td>1.405886</td>\n",
       "      <td>0.261671</td>\n",
       "      <td>-0.694085</td>\n",
       "      <td>-0.038230</td>\n",
       "      <td>-0.167124</td>\n",
       "      <td>0.418496</td>\n",
       "      <td>-0.072835</td>\n",
       "      <td>1.218224</td>\n",
       "      <td>0.029496</td>\n",
       "      <td>0.405468</td>\n",
       "      <td>0.124493</td>\n",
       "      <td>-0.252527</td>\n",
       "      <td>0.092441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pid  mode_1_pr  mode_1_dis  mode_1_eta  mode_2_pr  mode_2_dis  \\\n",
       "0      -1.0   -1000000    -1000000    -1000000         30         521   \n",
       "1  210736.0         30        1124         357   -1000000    -1000000   \n",
       "2      -1.0   -1000000    -1000000    -1000000         40        1229   \n",
       "3  202427.0         30        1364         398   -1000000    -1000000   \n",
       "4  172251.0   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "\n",
       "   mode_2_eta  mode_3_pr  mode_3_dis  mode_3_eta  mode_4_pr  mode_4_dis  \\\n",
       "0         167          0         453         103        210         453   \n",
       "1    -1000000          0        1110         112        290        1110   \n",
       "2         271          0        1130         140        370        1130   \n",
       "3    -1000000          0        1473         194        310        1473   \n",
       "4    -1000000          0           0           0        400           0   \n",
       "\n",
       "   mode_4_eta  mode_5_pr  mode_5_dis  mode_5_eta  mode_6_pr  mode_6_dis  \\\n",
       "0         109          0         404         367          0         411   \n",
       "1         136   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "2         164   -1000000    -1000000    -1000000          0         902   \n",
       "3         206   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "4          24   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "\n",
       "   mode_6_eta  mode_7_pr  mode_7_dis  mode_7_eta  mode_8_pr  mode_8_dis  \\\n",
       "0         124   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "1    -1000000         60        1386         322   -1000000    -1000000   \n",
       "2         272         60        1301         365   -1000000    -1000000   \n",
       "3    -1000000         60        1512         374   -1000000    -1000000   \n",
       "4    -1000000         60        1320         336   -1000000    -1000000   \n",
       "\n",
       "   mode_8_eta  mode_9_pr  mode_9_dis  mode_9_eta  mode_10_pr  mode_10_dis  \\\n",
       "0    -1000000         30         521         136    -1000000     -1000000   \n",
       "1    -1000000         40        1348         231    -1000000     -1000000   \n",
       "2    -1000000         40        1229         247         170         1299   \n",
       "3    -1000000   -1000000    -1000000    -1000000         170         1485   \n",
       "4    -1000000   -1000000    -1000000    -1000000    -1000000     -1000000   \n",
       "\n",
       "   mode_10_eta  mode_11_pr  mode_11_dis  mode_11_eta  max_dist  min_dist  \\\n",
       "0     -1000000    -1000000     -1000000     -1000000    5219.0    4046.0   \n",
       "1     -1000000    -1000000     -1000000     -1000000   13864.0   11106.0   \n",
       "2          234    -1000000     -1000000     -1000000   13018.0    9023.0   \n",
       "3          291    -1000000     -1000000     -1000000   15124.0   13640.0   \n",
       "4     -1000000    -1000000     -1000000     -1000000   13203.0       1.0   \n",
       "\n",
       "      std_dist    std_price  max_eta  min_eta      std_eta  max_dist_mode  \\\n",
       "0   467.713825   750.000000   3672.0   1035.0   914.260433              2   \n",
       "1  1243.375342  1048.045801   3578.0   1122.0   974.408251              7   \n",
       "2  1286.001936  1233.710183   3657.0   1400.0   693.587715              7   \n",
       "3   508.944241  1135.957746   3982.0   1941.0   836.608152              7   \n",
       "4  6521.737551  1577.973384   3363.0      1.0  1586.413782              7   \n",
       "\n",
       "   min_dist_mode  max_price_mode  min_price_mode  max_eta_mode  min_eta_mode  \\\n",
       "0              5               4               3           5.0           3.0   \n",
       "1              3               4               3           1.0           3.0   \n",
       "2              6               4               3           7.0           3.0   \n",
       "3              1               4               3           1.0           3.0   \n",
       "4              3               4               3           7.0           3.0   \n",
       "\n",
       "   first_mode  last_mode  second_mode    day-time  month  vac  week  hour  \\\n",
       "0           9          5            5  2018-11-02      1    0     5    17   \n",
       "1           7          1            1  2018-11-16      0    0     5    10   \n",
       "2           9          7            7  2018-10-06      2    1     6    10   \n",
       "3          10          1            1  2018-11-23      0    0     5    14   \n",
       "4           7          7            7  2018-10-30      2    0     2    11   \n",
       "\n",
       "   subway_zone  time_zone      o1     o2      d1     d2  d1-o1  d2-o2  \\\n",
       "0            1          2  116.29  39.97  116.32  39.96    3.0   -1.0   \n",
       "1            1          1  116.39  39.84  116.33  39.79   -6.0   -5.0   \n",
       "2            1          1  116.31  39.93  116.27  40.00   -4.0    7.0   \n",
       "3            1          2  116.27  39.88  116.39  39.90   12.0    2.0   \n",
       "4            1          2  116.34  39.96  116.37  39.86    3.0  -10.0   \n",
       "\n",
       "   svd_fea_0  svd_fea_1  svd_fea_2  svd_fea_3  svd_fea_4  svd_fea_5  \\\n",
       "0   0.000000  -0.000000   0.000000  -0.000000   0.000000  -0.000000   \n",
       "1   1.659321   0.658922  -0.722464   0.950416  -0.391069   0.522419   \n",
       "2   0.000000  -0.000000   0.000000  -0.000000   0.000000  -0.000000   \n",
       "3   1.299006  -0.720142  -1.369530  -0.654634  -0.417466  -0.535664   \n",
       "4   1.662153   0.925719   1.405886   0.261671  -0.694085  -0.038230   \n",
       "\n",
       "   svd_fea_6  svd_fea_7  svd_fea_8  svd_fea_9  svd_fea_10  svd_fea_11  \\\n",
       "0  -0.000000   0.000000   0.000000   0.000000   -0.000000    0.000000   \n",
       "1   0.059124  -0.103123   0.576893   0.834474    0.564825   -0.659383   \n",
       "2  -0.000000   0.000000   0.000000   0.000000   -0.000000    0.000000   \n",
       "3   0.228437   0.477593  -0.157794   0.066474    0.079345    0.053064   \n",
       "4  -0.167124   0.418496  -0.072835   1.218224    0.029496    0.405468   \n",
       "\n",
       "   svd_fea_12  svd_fea_13  svd_fea_14  \n",
       "0    0.000000    0.000000    0.000000  \n",
       "1   -0.606791    0.635751    0.465786  \n",
       "2    0.000000    0.000000    0.000000  \n",
       "3   -0.111287   -0.053937    0.283331  \n",
       "4    0.124493   -0.252527    0.092441  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = pd.read_csv(path + 'address_info.csv', names=['lng_lat', 'address_info'], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print map(lambda s: int(s), submit['mod_list'][0].strip('[]').split(','))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_na = np.zeros(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2']"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[1,2]'.strip('[]').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
