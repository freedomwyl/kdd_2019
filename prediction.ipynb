{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from time import gmtime, strftime\n",
    "import warnings\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import catboost as cb\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_f(y_pred, train_data):\n",
    "    y_true = train_data.label\n",
    "    y_pred = y_pred.reshape((12, -1)).T\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    score = f1_score(y_true, y_pred, average='weighted')\n",
    "    return 'weighted-f1-score', score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_result(submit, result, model_name, score):\n",
    "    now_time = strftime(\"%Y%m%d%H%M\", gmtime())\n",
    "    submit['recommend_mode'] = result\n",
    "    #submit['mod_list'] = submit['mod_list'].apply(lambda x: x.strip('[]').split(','))\n",
    "    #for i, row in submit.iterrows():\n",
    "    #    if row['mod_list'][0] != '':\n",
    "    #        row['mod_list'] = map(lambda s : int(s), row['mod_list'])\n",
    "    #        if row['recommend_mode'] not in row['mod_list']: #and  len(row['mod_list']) != 0:\n",
    "                #print i,row['mod_list'], row['recommend_mode'], row['sid']\n",
    "                #submit['recommend_mode'][i] = row['mod_list'][0]\n",
    "     #           cnt = cnt +1\n",
    "                #print i,submit['recommend_mode'][i]\n",
    "    #print(\"number of correction: \", cnt)\n",
    "    #submit = submit.drop(['mod_list'], axis=1)\n",
    "    submit.to_csv('./submit/{}_result_{}_{}.csv'.format(model_name, now_time, round(score,4)), index=False)\n",
    "    print(\"done, good luck!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_result_D(sb1,sb2,sb3, lgb1,lgb2,lgb3, model_name, score):\n",
    "    now_time = strftime(\"%Y%m%d%H%M\", gmtime())\n",
    "    sb1['recommend_mode'] = lgb1\n",
    "    sb2['recommend_mode'] = lgb2\n",
    "    sb3['recommend_mode'] = lgb3\n",
    "    submit = pd.concat([sb1,sb2,sb3], axis=0)  \n",
    "    submit.to_csv('./submit/{}_result_{}_{}.csv'.format(model_name, now_time, round(score,4)), index=False)\n",
    "    print(\"done, good luck!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(train_x, train_y, val_x, val_y):\n",
    "    data = train_x.copy()\n",
    "    #data = pd.concat([train_x, train_y], axis=1)\n",
    "    data['click_mode'] = train_y\n",
    "    data0 = data[(data['click_mode'] == 0)]\n",
    "    data1 = data[(data['click_mode'] == 1)]\n",
    "    data2 = data[(data['click_mode'] == 2)]\n",
    "    data3 = data[(data['click_mode'] == 3)]\n",
    "    data4 = data[(data['click_mode'] == 4)]\n",
    "    data5 = data[(data['click_mode'] == 5)]\n",
    "    data6 = data[(data['click_mode'] == 6)]\n",
    "    data7 = data[(data['click_mode'] == 7)]\n",
    "    data8 = data[(data['click_mode'] == 8)]\n",
    "    data9 = data[(data['click_mode'] == 9)]\n",
    "    data10 = data[(data['click_mode'] == 10)]\n",
    "    data11 = data[(data['click_mode'] == 11)]\n",
    "    \n",
    "    data4 = data4.sample(frac=0.999,axis=0)\n",
    "    data6 = data6.sample(frac=0.999,axis=0)\n",
    "    data8 = data8.sample(frac=0.999,axis=0)\n",
    "    data11 = data11.sample(frac=0.999,axis=0)\n",
    "    \n",
    "    data3 = data3.sample(frac=0.999,axis=0)#98\n",
    "    data10 = data10.sample(frac=0.999,axis=0)\n",
    "    \n",
    "    data5 = data5.sample(frac=1,axis=0)#0.6\n",
    "    data9 = data9.sample(frac=1,axis=0)\n",
    "    \n",
    "    data1 = data1.sample(frac=1,axis=0)\n",
    "    data7 = data7.sample(frac=1,axis=0)\n",
    "    data0 = data0.sample(frac=1,axis=0)\n",
    "    #data0x = data0[data0['mode_length'] == 0]\n",
    "    #data0y = data0[data0['mode_length'] != 0]\n",
    "    #data0y = data0y.sample(frac=0.7,axis=0)\n",
    "    data2 = data2.sample(frac=1,axis=0)#0.5\n",
    "    \n",
    "    df = pd.concat([data0, data1, data2, data3,\n",
    "                    data4, data5, data6, data7,\n",
    "                    data8, data9, data10, data11], axis=0)\n",
    "    \n",
    "    y = df['click_mode'].copy()\n",
    "    x = df.drop(['click_mode'],  axis=1)\n",
    "    #val = pd.concat([val_x, val_y], axis=1)\n",
    "    val = val_x.copy()\n",
    "    val['click_mode'] = val_y\n",
    "    #val = val.sample(frac=0.6, axis=0)  \n",
    "    all_data = pd.concat([df, val], axis=0)\n",
    "    all_y = all_data['click_mode'].copy()\n",
    "    all_x = all_data.drop(['click_mode'],  axis=1)\n",
    "    return x,y,all_x,all_y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g[0.65, 0.5, 0.5, 0.98, 0.98, 0.5, 0.98, 0.5, 0.98, 0.5, 0.98, 0.98]\n",
    "# b [0.8, 0.45, 0.45, 0.98, 0.98, 0.6, 0.98, 0.45, 0.98, 0.45, 0.6, 0.98]\n",
    "#0.7, 0.5, 0.5, 0.98, 0.98, 0.5, 0.98, 0.5, 0.98, 0.5, 0.98, 0.98\n",
    "def down_sampling(data_all, train_y, sampling_rates=[1,1,1,1,1,1,1,1,1,1,1,1]):\n",
    "    #data_all = pd.concat([train_x, train_y], axis=1)\n",
    "    data_all['click_mode'] = train_y\n",
    "    data = {}\n",
    "    for mode, rate in enumerate(sampling_rates):\n",
    "        data[mode] = pd.DataFrame()\n",
    "        data[mode] = data_all[(data_all['click_mode'] == mode)]\n",
    "        data[mode] = data[mode].sample(frac=rate, axis=0)\n",
    "    \n",
    "    df = pd.concat([data[mode] for mode in range(12)], axis=0)\n",
    "    df = df.sample(frac=1)\n",
    "    y = np.array(df['click_mode'])\n",
    "    x = df.drop(['click_mode'],  axis=1)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels, preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "\n",
    "def train_lgb(train_x, train_y, test_x):  #,train_x1, train_y1,\n",
    "    #kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    \n",
    "    train_index = ((train_x['day_time'] <= '2018-11-23'))\n",
    "    valid_index = (train_x['day_time'] > '2018-11-23') & (train_x['day_time'] < '2018-12-01')\n",
    "    \n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    \n",
    "    tr_x1     = train_x[train_index]    #train_x1#\n",
    "    tr_y1     = train_y[train_index]    #train_y1#\n",
    "    val_x     = train_x[valid_index]\n",
    "    val_y     = train_y[valid_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    cate_cols = ['max_dist_mode', 'min_dist_mode', #'max_price_mode','pred_5']#['pred_0','pred_1','cpred_0']#\n",
    "                'min_price_mode', 'max_eta_mode', 'min_eta_mode',\n",
    "                 'first_mode','last_mode', 'week', ]\n",
    "    scores = []\n",
    "    results = []\n",
    " \n",
    "    for i in range(1):\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=41,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=2000, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.05, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "        \n",
    "        tr_x, tr_y, all_train_x, all_train_y = gen_data(tr_x1, tr_y1, val_x, val_y)\n",
    "        \n",
    "        eval_set = [(val_x, val_y)]\n",
    "        lgb_model.fit(\n",
    "            tr_x, tr_y, eval_set=eval_set,\n",
    "            eval_metric=f1_weighted,\n",
    "            categorical_feature=cate_cols,\n",
    "            verbose=50, early_stopping_rounds=70)\n",
    "        \n",
    "        \n",
    "        def get_weighted_fscore(y_pred, y_true):\n",
    "            f_score = 0\n",
    "            for i in range(12):\n",
    "                yt = y_true == i\n",
    "                yp = y_pred == i\n",
    "                f_score += dic_[i] * f1_score(y_true=yt, y_pred= yp)\n",
    "                print(i,dic_[i],f1_score(y_true=yt, y_pred= yp), precision_score(y_true=yt, y_pred= yp),recall_score(y_true=yt, y_pred= yp))\n",
    "            print(f_score)\n",
    "            return f_score\n",
    "        \n",
    "        val_pred = lgb_model.predict(val_x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = lgb_model.predict(val_x) \n",
    "        df_analysis = pd.DataFrame()\n",
    "        #df_analysis['sid']   = val_x['sid']\n",
    "        df_analysis['label'] = val_y\n",
    "        df_analysis['pred']  = pred\n",
    "        df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "        dic_ = df_analysis['label'].value_counts(normalize = True)\n",
    "        \n",
    "        val_score = get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "        \n",
    "        scores.append(val_score)\n",
    "        lgb_model.n_estimators = lgb_model.best_iteration_\n",
    "        #lgb_model.fit(all_train_x, all_train_y, categorical_feature=cate_cols)\n",
    "        pred_test = lgb_model.predict_proba(test_x)\n",
    "        results.append(pred_test)\n",
    "        \n",
    "    result = np.argmax(np.mean(results, axis=0), axis=1)  \n",
    "    print('cv f1-score: ', np.mean(scores))\n",
    "    return result, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_k_fold(train_x, train_y, test_x):\n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    lgb_paras = {\n",
    "        'objective': 'multiclass',\n",
    "        'metrics': 'multiclass',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 41,\n",
    "        'lambda_l1': 0.01,\n",
    "        'lambda_l2': 10,\n",
    "        'num_class': 12,\n",
    "        'seed': 2019,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 4,\n",
    "    }\n",
    "    cate_cols = ['pred_0','pred_1','cpred_0']#['max_dist_mode', 'min_dist_mode', #'max_price_mode','pred_5']#\n",
    "                 #'min_price_mode', 'max_eta_mode', 'min_eta_mode',\n",
    "                 #'first_mode','last_mode', 'week', ]#\n",
    "    scores = []\n",
    "    result_proba = []\n",
    "    #second_level_train_set = np.zeros((train_x.shape[0],))\n",
    "    for tr_idx, val_idx in kfold.split(train_x, train_y):\n",
    "        tr_x, tr_y, val_x, val_y = train_x.iloc[tr_idx], train_y[tr_idx], train_x.iloc[val_idx], train_y[val_idx]\n",
    "        tr_x, tr_y = down_sampling(tr_x, tr_y)\n",
    "        #val_x, val_y = down_sampling(val_x, val_y,[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "        train_set = lgb.Dataset(tr_x, tr_y, categorical_feature=cate_cols)\n",
    "        val_set = lgb.Dataset(val_x, val_y, categorical_feature=cate_cols)\n",
    "        lgb_model = lgb.train(lgb_paras, train_set,\n",
    "                              valid_sets=[val_set], early_stopping_rounds=100, num_boost_round=40000, verbose_eval=50, feval=eval_f)\n",
    "        val_proba = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n",
    "        val_pred = np.argmax(val_proba, axis=1)\n",
    "        val_score = f1_score(val_y, val_pred, average='weighted')\n",
    "        result_proba.append(lgb_model.predict(\n",
    "            test_x, num_iteration=lgb_model.best_iteration))\n",
    "        scores.append(val_score)\n",
    "    print('cv f1-score: ', np.mean(scores))\n",
    "    pred_test = np.argmax(np.mean(result_proba, axis=0), axis=1)\n",
    "    return pred_test, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_k_fold_stack(train_x, train_y, test_x):\n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    lgb_paras = {\n",
    "        'objective': 'multiclass',\n",
    "        'metrics': 'multiclass',\n",
    "        'learning_rate': 0.2,\n",
    "        'num_leaves': 41,\n",
    "        'lambda_l1': 0.01,\n",
    "        'lambda_l2': 10,\n",
    "        'num_class': 12,\n",
    "        'seed': 2019,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 4,\n",
    "    }\n",
    "    cate_cols = ['pred_0','pred_1','cpred_0']\n",
    "    scores = []\n",
    "    second_level_test_set = []\n",
    "    second_level_train_set = np.zeros((train_x.shape[0],12),dtype=np.float32)\n",
    "    for tr_idx, val_idx in kfold.split(train_x, train_y):\n",
    "        tr_x, tr_y, val_x, val_y = train_x.iloc[tr_idx], train_y[tr_idx], train_x.iloc[val_idx], train_y[val_idx]\n",
    "        tr_x, tr_y = down_sampling(tr_x, tr_y)\n",
    "        #val_x, val_y = down_sampling(val_x, val_y,[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "        train_set = lgb.Dataset(tr_x, tr_y, categorical_feature=cate_cols)\n",
    "        val_set = lgb.Dataset(val_x, val_y, categorical_feature=cate_cols)\n",
    "        lgb_model = lgb.train(lgb_paras, train_set,\n",
    "                              valid_sets=[val_set], early_stopping_rounds=70, num_boost_round=4000, verbose_eval=50, feval=eval_f)\n",
    "        val_proba = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n",
    "        val_pred = np.argmax(val_proba, axis=1)\n",
    "        second_level_train_set[val_idx] = val_proba\n",
    "        val_score = f1_score(val_y, val_pred, average='weighted')\n",
    "        second_level_test_set.append(lgb_model.predict(\n",
    "            test_x, num_iteration=lgb_model.best_iteration))\n",
    "        scores.append(val_score)\n",
    "    print('cv f1-score: ', np.mean(scores))\n",
    "    pred_test = np.argmax(np.mean(second_level_test_set, axis=0), axis=1)\n",
    "    pred_tr = np.argmax(second_level_train_set, axis=1)\n",
    "    second_level_test_set = np.mean(second_level_test_set, axis=0)\n",
    "    return second_level_train_set, pred_tr, second_level_test_set, pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_city(tr,te,sb):\n",
    "    test = pd.concat([sb, te], axis=1)\n",
    "    te1 = test[test['o1'] < 115]\n",
    "    te2 = test[(test['o1'] > 115)&(test['o1'] < 118)]\n",
    "    te3 = test[test['o1'] > 118]\n",
    "    sb1 = te1[['sid']].copy()\n",
    "    te1 = te1.drop(['sid'],axis=1)\n",
    "    sb2 = te2[['sid']].copy()\n",
    "    te2 = te2.drop(['sid'],axis=1)\n",
    "    sb3 = te3[['sid']].copy()\n",
    "    te3 = te3.drop(['sid'],axis=1)\n",
    "    \n",
    "    tr1 = tr[tr['o1'] < 115]\n",
    "    tr2 = tr[(tr['o1'] > 115)&(tr['o1'] < 118)]\n",
    "    tr3 = tr[tr['o1'] > 118]\n",
    "    return sb1,sb2,sb3,te1,te2,te3,tr1,tr2,tr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_stacking_data(train_x, train_y, test_x):\n",
    "    #tr_day = list(train_x['day_time'])\n",
    "    #te_day = list(test_x['day_time'])\n",
    "    second_tr = pd.DataFrame()\n",
    "    second_te = pd.DataFrame()\n",
    "    for j in range(1):\n",
    "        j += 2\n",
    "        s_tr, pred_tr, s_te, pred_test = train_lgb_k_fold_stack(train_x, train_y, test_x)\n",
    "        s_tr = pd.DataFrame(s_tr)\n",
    "        s_te = pd.DataFrame(s_te)\n",
    "        s_tr.columns = [ str(j)+ '_' + 'cp_{}'.format(i) for i in range(12)]\n",
    "        s_te.columns = [ str(j)+ '_' + 'cp_{}'.format(i) for i in range(12)]\n",
    "        s_tr['cpred' + '_' + str(j)] = pred_tr\n",
    "        s_te['cpred' + '_' + str(j)] = pred_test\n",
    "        second_tr = pd.concat([second_tr, s_tr], axis=1)\n",
    "        second_te = pd.concat([second_te, s_te], axis=1)\n",
    "    del train_x, test_x\n",
    "    #second_tr['day_time'] = tr_day\n",
    "    #second_te['day_time'] = te_day\n",
    "    return second_tr, second_te\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " if __name__ == '__main__':\n",
    "    train_data = pd.read_csv(\"./data/train_data.csv\")\n",
    "    #train_data = train_data[train_data['day_time'] >= '2018-10-08']\n",
    "    train_data['click_mode'] = train_data['click_mode'].apply(lambda x: int(x))\n",
    "    train_y = np.array(train_data['click_mode'])\n",
    "    #train_y = train_data['click_mode'].copy()\n",
    "    train_x = train_data.drop(['click_mode',], axis=1)\n",
    "    test_x = pd.read_csv(\"./data/test_data.csv\")#11\n",
    "    #test_x = test_x.drop([],  axis=1) #'\n",
    "    submit = pd.read_csv(\"./data/submit.csv\") \n",
    "    submit = submit.drop(['mod_list'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array(tr1['click_mode'])\n",
    "#y2 = np.array(tr2['click_mode'])\n",
    "#y3 = np.array(tr3['click_mode'])\n",
    "tr1 = tr1.drop(['click_mode',], axis=1)\n",
    "#tr2 = tr2.drop(['click_mode',], axis=1)\n",
    "#tr3 = tr3.drop(['click_mode',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tr = pd.read_csv(\"./data/second_trsh01.csv\")\n",
    "second_te = pd.read_csv(\"./data/second_tesh01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trx = pd.read_csv(\"./data/second_trsh0x.csv\")\n",
    "second_tex = pd.read_csv(\"./data/second_tesh0x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'second_trsh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3b6acc9511b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msecond_trsh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_trsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day_time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msecond_tesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_tesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day_time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'second_trsh' is not defined"
     ]
    }
   ],
   "source": [
    "second_trsh = second_trsh.drop(['day_time',], axis=1)\n",
    "second_tesh = second_tesh.drop(['day_time',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trx = pd.concat([second_tr,second_trx], axis=1)\n",
    "second_tex = pd.concat([second_te,second_tex], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_p_0</th>\n",
       "      <th>0_p_1</th>\n",
       "      <th>0_p_2</th>\n",
       "      <th>0_p_3</th>\n",
       "      <th>0_p_4</th>\n",
       "      <th>0_p_5</th>\n",
       "      <th>0_p_6</th>\n",
       "      <th>0_p_7</th>\n",
       "      <th>0_p_8</th>\n",
       "      <th>0_p_9</th>\n",
       "      <th>0_p_10</th>\n",
       "      <th>0_p_11</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>1_p_0</th>\n",
       "      <th>1_p_1</th>\n",
       "      <th>1_p_2</th>\n",
       "      <th>1_p_3</th>\n",
       "      <th>1_p_4</th>\n",
       "      <th>1_p_5</th>\n",
       "      <th>1_p_6</th>\n",
       "      <th>1_p_7</th>\n",
       "      <th>1_p_8</th>\n",
       "      <th>1_p_9</th>\n",
       "      <th>1_p_10</th>\n",
       "      <th>1_p_11</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>day_time</th>\n",
       "      <th>0_cp_0</th>\n",
       "      <th>0_cp_1</th>\n",
       "      <th>0_cp_2</th>\n",
       "      <th>0_cp_3</th>\n",
       "      <th>0_cp_4</th>\n",
       "      <th>0_cp_5</th>\n",
       "      <th>0_cp_6</th>\n",
       "      <th>0_cp_7</th>\n",
       "      <th>0_cp_8</th>\n",
       "      <th>0_cp_9</th>\n",
       "      <th>0_cp_10</th>\n",
       "      <th>0_cp_11</th>\n",
       "      <th>cpred_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054427</td>\n",
       "      <td>0.026944</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.021067</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>8.504493e-07</td>\n",
       "      <td>0.869205</td>\n",
       "      <td>1.066995e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>8.366390e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.898560</td>\n",
       "      <td>8.536940e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>0.068843</td>\n",
       "      <td>0.046271</td>\n",
       "      <td>0.029643</td>\n",
       "      <td>0.027286</td>\n",
       "      <td>0.023941</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.758206</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.014237</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.173976</td>\n",
       "      <td>0.462319</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.092876</td>\n",
       "      <td>0.046116</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>1.992413e-01</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>3.097929e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181952</td>\n",
       "      <td>0.504123</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.094853</td>\n",
       "      <td>0.045080</td>\n",
       "      <td>2.392035e-02</td>\n",
       "      <td>0.150001</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>7.334114e-07</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>0.168519</td>\n",
       "      <td>0.451878</td>\n",
       "      <td>0.037364</td>\n",
       "      <td>0.071804</td>\n",
       "      <td>0.055262</td>\n",
       "      <td>0.038681</td>\n",
       "      <td>0.117990</td>\n",
       "      <td>0.024446</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.017946</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062231</td>\n",
       "      <td>0.091938</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.032845</td>\n",
       "      <td>0.042634</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>6.027346e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>5.234158e-01</td>\n",
       "      <td>0.246883</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>8</td>\n",
       "      <td>0.056912</td>\n",
       "      <td>0.140374</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>7.020653e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>3.843707e-01</td>\n",
       "      <td>0.335178</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>0.076212</td>\n",
       "      <td>0.140832</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.034363</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.356796</td>\n",
       "      <td>0.318778</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.019052</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>6.179246e-06</td>\n",
       "      <td>0.832507</td>\n",
       "      <td>8.593302e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.100616</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7</td>\n",
       "      <td>0.038791</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.019066</td>\n",
       "      <td>0.034936</td>\n",
       "      <td>4.133176e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.782989</td>\n",
       "      <td>1.594362e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.124164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>0.062585</td>\n",
       "      <td>0.018986</td>\n",
       "      <td>0.030122</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.687562</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.115154</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.195220</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.656675e-06</td>\n",
       "      <td>0.765869</td>\n",
       "      <td>1.233404e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>1.496050e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.825415</td>\n",
       "      <td>1.283853e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>0.201605</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.027048</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.733057</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0_p_0     0_p_1     0_p_2     0_p_3     0_p_4     0_p_5         0_p_6  \\\n",
       "0  0.054427  0.026944  0.000005  0.021067  0.028333  0.000001  8.504493e-07   \n",
       "1  0.173976  0.462319  0.000017  0.092876  0.046116  0.025419  1.992413e-01   \n",
       "2  0.062231  0.091938  0.000010  0.032845  0.042634  0.000001  6.027346e-06   \n",
       "3  0.034286  0.000011  0.000005  0.013502  0.019052  0.000001  6.179246e-06   \n",
       "4  0.195220  0.000010  0.000007  0.025960  0.012906  0.000002  4.656675e-06   \n",
       "\n",
       "      0_p_7         0_p_8     0_p_9    0_p_10    0_p_11  pred_0     1_p_0  \\\n",
       "0  0.869205  1.066995e-06  0.000005  0.000008  0.000005       7  0.043756   \n",
       "1  0.000016  3.097929e-07  0.000013  0.000003  0.000004       1  0.181952   \n",
       "2  0.000013  5.234158e-01  0.246883  0.000016  0.000007       8  0.056912   \n",
       "3  0.832507  8.593302e-07  0.000005  0.100616  0.000007       7  0.038791   \n",
       "4  0.765869  1.233404e-06  0.000004  0.000014  0.000003       7  0.138900   \n",
       "\n",
       "      1_p_1     1_p_2     1_p_3     1_p_4         1_p_5     1_p_6     1_p_7  \\\n",
       "0  0.023634  0.000005  0.018145  0.015876  8.366390e-07  0.000002  0.898560   \n",
       "1  0.504123  0.000014  0.094853  0.045080  2.392035e-02  0.150001  0.000024   \n",
       "2  0.140374  0.000013  0.036218  0.046883  7.020653e-07  0.000009  0.000017   \n",
       "3  0.000017  0.000009  0.019066  0.034936  4.133176e-06  0.000009  0.782989   \n",
       "4  0.000011  0.000005  0.021333  0.014314  1.496050e-06  0.000001  0.825415   \n",
       "\n",
       "          1_p_8     1_p_9    1_p_10    1_p_11  pred_1    day_time    0_cp_0  \\\n",
       "0  8.536940e-07  0.000004  0.000009  0.000007       7  2018-10-07  0.068843   \n",
       "1  7.334114e-07  0.000022  0.000004  0.000006       1  2018-10-13  0.168519   \n",
       "2  3.843707e-01  0.335178  0.000016  0.000008       8  2018-11-26  0.076212   \n",
       "3  1.594362e-06  0.000008  0.124164  0.000006       7  2018-10-21  0.062585   \n",
       "4  1.283853e-06  0.000006  0.000008  0.000003       7  2018-10-28  0.201605   \n",
       "\n",
       "     0_cp_1    0_cp_2    0_cp_3    0_cp_4    0_cp_5    0_cp_6    0_cp_7  \\\n",
       "0  0.046271  0.029643  0.027286  0.023941  0.013094  0.005697  0.758206   \n",
       "1  0.451878  0.037364  0.071804  0.055262  0.038681  0.117990  0.024446   \n",
       "2  0.140832  0.013377  0.034363  0.036788  0.005982  0.002636  0.008826   \n",
       "3  0.018986  0.030122  0.024991  0.021429  0.013306  0.005789  0.687562   \n",
       "4  0.002631  0.004101  0.025047  0.027048  0.001850  0.000827  0.733057   \n",
       "\n",
       "     0_cp_8    0_cp_9   0_cp_10   0_cp_11  cpred_0  \n",
       "0  0.000997  0.014237  0.007260  0.004523        7  \n",
       "1  0.001257  0.017946  0.009151  0.005701        1  \n",
       "2  0.356796  0.318778  0.003330  0.002080        8  \n",
       "3  0.001013  0.014467  0.115154  0.004596        7  \n",
       "4  0.000151  0.002006  0.001030  0.000646        7  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_trx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.888509\tvalid_0's weighted-f1-score: 0.670577\n",
      "[100]\tvalid_0's multi_logloss: 0.865302\tvalid_0's weighted-f1-score: 0.670869\n",
      "[150]\tvalid_0's multi_logloss: 0.863731\tvalid_0's weighted-f1-score: 0.670786\n",
      "[200]\tvalid_0's multi_logloss: 0.863791\tvalid_0's weighted-f1-score: 0.670757\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.865302\tvalid_0's weighted-f1-score: 0.670869\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.900209\tvalid_0's weighted-f1-score: 0.671254\n",
      "[100]\tvalid_0's multi_logloss: 0.867966\tvalid_0's weighted-f1-score: 0.671694\n",
      "[150]\tvalid_0's multi_logloss: 0.865092\tvalid_0's weighted-f1-score: 0.671444\n",
      "[200]\tvalid_0's multi_logloss: 0.864981\tvalid_0's weighted-f1-score: 0.671166\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalid_0's multi_logloss: 0.867796\tvalid_0's weighted-f1-score: 0.671719\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.891478\tvalid_0's weighted-f1-score: 0.670817\n",
      "[100]\tvalid_0's multi_logloss: 0.869355\tvalid_0's weighted-f1-score: 0.670355\n",
      "[150]\tvalid_0's multi_logloss: 0.869424\tvalid_0's weighted-f1-score: 0.668993\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's multi_logloss: 0.877224\tvalid_0's weighted-f1-score: 0.671088\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.891782\tvalid_0's weighted-f1-score: 0.669137\n",
      "[100]\tvalid_0's multi_logloss: 0.868881\tvalid_0's weighted-f1-score: 0.668979\n",
      "[150]\tvalid_0's multi_logloss: 0.867343\tvalid_0's weighted-f1-score: 0.66873\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's multi_logloss: 0.884979\tvalid_0's weighted-f1-score: 0.669271\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.887609\tvalid_0's weighted-f1-score: 0.671843\n",
      "[100]\tvalid_0's multi_logloss: 0.864738\tvalid_0's weighted-f1-score: 0.672106\n",
      "[150]\tvalid_0's multi_logloss: 0.86326\tvalid_0's weighted-f1-score: 0.671959\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's multi_logloss: 0.868264\tvalid_0's weighted-f1-score: 0.672244\n",
      "('cv f1-score: ', 0.6710382110525479)\n",
      "done, good luck!!\n"
     ]
    }
   ],
   "source": [
    "lgbres, score = train_lgb_k_fold(second_trx, train_y, second_tex)\n",
    "submit_result(submit, lgbres, 'lgb', score)\n",
    "#lgb2, score2 = train_lgb_k_fold(second_trgs, y1,second_tegs)\n",
    "#lgb3, score3 = train_lgb_k_fold(second_trsh, y3,second_tesh)\n",
    "#score = (score1 + score2 + score3)/3\n",
    "#submit_result_D(sb1,sb2,sb3, lgb2, lgb1, lgb3,'lgb', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_p_0</th>\n",
       "      <th>0_p_1</th>\n",
       "      <th>0_p_2</th>\n",
       "      <th>0_p_3</th>\n",
       "      <th>0_p_4</th>\n",
       "      <th>0_p_5</th>\n",
       "      <th>0_p_6</th>\n",
       "      <th>0_p_7</th>\n",
       "      <th>0_p_8</th>\n",
       "      <th>0_p_9</th>\n",
       "      <th>0_p_10</th>\n",
       "      <th>0_p_11</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>1_p_0</th>\n",
       "      <th>1_p_1</th>\n",
       "      <th>1_p_2</th>\n",
       "      <th>1_p_3</th>\n",
       "      <th>1_p_4</th>\n",
       "      <th>1_p_5</th>\n",
       "      <th>1_p_6</th>\n",
       "      <th>1_p_7</th>\n",
       "      <th>1_p_8</th>\n",
       "      <th>1_p_9</th>\n",
       "      <th>1_p_10</th>\n",
       "      <th>1_p_11</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>day_time</th>\n",
       "      <th>0_cp_0</th>\n",
       "      <th>0_cp_1</th>\n",
       "      <th>0_cp_2</th>\n",
       "      <th>0_cp_3</th>\n",
       "      <th>0_cp_4</th>\n",
       "      <th>0_cp_5</th>\n",
       "      <th>0_cp_6</th>\n",
       "      <th>0_cp_7</th>\n",
       "      <th>0_cp_8</th>\n",
       "      <th>0_cp_9</th>\n",
       "      <th>0_cp_10</th>\n",
       "      <th>0_cp_11</th>\n",
       "      <th>cpred_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054427</td>\n",
       "      <td>0.026944</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.021067</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>8.504493e-07</td>\n",
       "      <td>0.869205</td>\n",
       "      <td>1.066995e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>8.366390e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.898560</td>\n",
       "      <td>8.536940e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>0.068843</td>\n",
       "      <td>0.046271</td>\n",
       "      <td>0.029643</td>\n",
       "      <td>0.027286</td>\n",
       "      <td>0.023941</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.758206</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.014237</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.173976</td>\n",
       "      <td>0.462319</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.092876</td>\n",
       "      <td>0.046116</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>1.992413e-01</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>3.097929e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181952</td>\n",
       "      <td>0.504123</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.094853</td>\n",
       "      <td>0.045080</td>\n",
       "      <td>2.392035e-02</td>\n",
       "      <td>0.150001</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>7.334114e-07</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>0.168519</td>\n",
       "      <td>0.451878</td>\n",
       "      <td>0.037364</td>\n",
       "      <td>0.071804</td>\n",
       "      <td>0.055262</td>\n",
       "      <td>0.038681</td>\n",
       "      <td>0.117990</td>\n",
       "      <td>0.024446</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.017946</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062231</td>\n",
       "      <td>0.091938</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.032845</td>\n",
       "      <td>0.042634</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>6.027346e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>5.234158e-01</td>\n",
       "      <td>0.246883</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>8</td>\n",
       "      <td>0.056912</td>\n",
       "      <td>0.140374</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>7.020653e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>3.843707e-01</td>\n",
       "      <td>0.335178</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>0.076212</td>\n",
       "      <td>0.140832</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.034363</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.356796</td>\n",
       "      <td>0.318778</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.019052</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>6.179246e-06</td>\n",
       "      <td>0.832507</td>\n",
       "      <td>8.593302e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.100616</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7</td>\n",
       "      <td>0.038791</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.019066</td>\n",
       "      <td>0.034936</td>\n",
       "      <td>4.133176e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.782989</td>\n",
       "      <td>1.594362e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.124164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>0.062585</td>\n",
       "      <td>0.018986</td>\n",
       "      <td>0.030122</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.687562</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.115154</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.195220</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.656675e-06</td>\n",
       "      <td>0.765869</td>\n",
       "      <td>1.233404e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>1.496050e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.825415</td>\n",
       "      <td>1.283853e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>0.201605</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.027048</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.733057</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0_p_0     0_p_1     0_p_2     0_p_3     0_p_4     0_p_5         0_p_6  \\\n",
       "0  0.054427  0.026944  0.000005  0.021067  0.028333  0.000001  8.504493e-07   \n",
       "1  0.173976  0.462319  0.000017  0.092876  0.046116  0.025419  1.992413e-01   \n",
       "2  0.062231  0.091938  0.000010  0.032845  0.042634  0.000001  6.027346e-06   \n",
       "3  0.034286  0.000011  0.000005  0.013502  0.019052  0.000001  6.179246e-06   \n",
       "4  0.195220  0.000010  0.000007  0.025960  0.012906  0.000002  4.656675e-06   \n",
       "\n",
       "      0_p_7         0_p_8     0_p_9    0_p_10    0_p_11  pred_0     1_p_0  \\\n",
       "0  0.869205  1.066995e-06  0.000005  0.000008  0.000005       7  0.043756   \n",
       "1  0.000016  3.097929e-07  0.000013  0.000003  0.000004       1  0.181952   \n",
       "2  0.000013  5.234158e-01  0.246883  0.000016  0.000007       8  0.056912   \n",
       "3  0.832507  8.593302e-07  0.000005  0.100616  0.000007       7  0.038791   \n",
       "4  0.765869  1.233404e-06  0.000004  0.000014  0.000003       7  0.138900   \n",
       "\n",
       "      1_p_1     1_p_2     1_p_3     1_p_4         1_p_5     1_p_6     1_p_7  \\\n",
       "0  0.023634  0.000005  0.018145  0.015876  8.366390e-07  0.000002  0.898560   \n",
       "1  0.504123  0.000014  0.094853  0.045080  2.392035e-02  0.150001  0.000024   \n",
       "2  0.140374  0.000013  0.036218  0.046883  7.020653e-07  0.000009  0.000017   \n",
       "3  0.000017  0.000009  0.019066  0.034936  4.133176e-06  0.000009  0.782989   \n",
       "4  0.000011  0.000005  0.021333  0.014314  1.496050e-06  0.000001  0.825415   \n",
       "\n",
       "          1_p_8     1_p_9    1_p_10    1_p_11  pred_1    day_time    0_cp_0  \\\n",
       "0  8.536940e-07  0.000004  0.000009  0.000007       7  2018-10-07  0.068843   \n",
       "1  7.334114e-07  0.000022  0.000004  0.000006       1  2018-10-13  0.168519   \n",
       "2  3.843707e-01  0.335178  0.000016  0.000008       8  2018-11-26  0.076212   \n",
       "3  1.594362e-06  0.000008  0.124164  0.000006       7  2018-10-21  0.062585   \n",
       "4  1.283853e-06  0.000006  0.000008  0.000003       7  2018-10-28  0.201605   \n",
       "\n",
       "     0_cp_1    0_cp_2    0_cp_3    0_cp_4    0_cp_5    0_cp_6    0_cp_7  \\\n",
       "0  0.046271  0.029643  0.027286  0.023941  0.013094  0.005697  0.758206   \n",
       "1  0.451878  0.037364  0.071804  0.055262  0.038681  0.117990  0.024446   \n",
       "2  0.140832  0.013377  0.034363  0.036788  0.005982  0.002636  0.008826   \n",
       "3  0.018986  0.030122  0.024991  0.021429  0.013306  0.005789  0.687562   \n",
       "4  0.002631  0.004101  0.025047  0.027048  0.001850  0.000827  0.733057   \n",
       "\n",
       "     0_cp_8    0_cp_9   0_cp_10   0_cp_11  cpred_0  \n",
       "0  0.000997  0.014237  0.007260  0.004523        7  \n",
       "1  0.001257  0.017946  0.009151  0.005701        1  \n",
       "2  0.356796  0.318778  0.003330  0.002080        8  \n",
       "3  0.001013  0.014467  0.115154  0.004596        7  \n",
       "4  0.000151  0.002006  0.001030  0.000646        7  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_trx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trbj = pd.read_csv(\"./data/second_trbj.csv\")\n",
    "second_tebj = pd.read_csv(\"./data/second_tebj.csv\")\n",
    "second_trsh = pd.read_csv(\"./data/second_trsh.csv\")\n",
    "second_tesh = pd.read_csv(\"./data/second_tesh.csv\")\n",
    "second_trgs = pd.read_csv(\"./data/second_trgs.csv\")\n",
    "second_tegs = pd.read_csv(\"./data/second_tegs.csv\")\n",
    "second_trbj['click_mode'] = y2\n",
    "second_trgs['click_mode'] = y1\n",
    "second_trsh['click_mode'] = y3\n",
    "second_tr_d = pd.concat([second_trbj,second_trgs,second_trsh], axis=0)\n",
    "second_te_d = pd.concat([second_tebj,second_tegs,second_tesh], axis=0)\n",
    "second_tr_d.to_csv('./data/second_trsh_d.csv', index=False)\n",
    "second_te_d.to_csv('./data/second_tesh_d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trbj = second_trbj.drop(['click_mode',], axis=1)\n",
    "second_trsh = second_trsh.drop(['click_mode',], axis=1)\n",
    "second_trgs = second_trgs.drop(['click_mode',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, good luck!!\n"
     ]
    }
   ],
   "source": [
    "submit_result_D(sb1,sb2,sb3, lgb2, lgb1, lgb3,'lgb', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5242730\ttest: 0.5239069\tbest: 0.5239069 (0)\ttotal: 4.14s\tremaining: 2h 17m 58s\n",
      "50:\tlearn: 0.6951490\ttest: 0.6959526\tbest: 0.6963076 (43)\ttotal: 3m 57s\tremaining: 2h 31m 21s\n",
      "100:\tlearn: 0.7021680\ttest: 0.7026674\tbest: 0.7026674 (100)\ttotal: 8m 21s\tremaining: 2h 37m 10s\n",
      "150:\tlearn: 0.7060355\ttest: 0.7064923\tbest: 0.7065923 (148)\ttotal: 12m\tremaining: 2h 26m 59s\n",
      "200:\tlearn: 0.7125645\ttest: 0.7123222\tbest: 0.7123222 (200)\ttotal: 15m 51s\tremaining: 2h 21m 56s\n",
      "250:\tlearn: 0.7186835\ttest: 0.7182120\tbest: 0.7182120 (250)\ttotal: 19m 53s\tremaining: 2h 18m 36s\n",
      "300:\tlearn: 0.7217845\ttest: 0.7215970\tbest: 0.7216420 (299)\ttotal: 23m\tremaining: 2h 9m 53s\n",
      "350:\tlearn: 0.7271545\ttest: 0.7272018\tbest: 0.7272468 (349)\ttotal: 26m 56s\tremaining: 2h 6m 33s\n",
      "400:\tlearn: 0.7336410\ttest: 0.7332517\tbest: 0.7332517 (400)\ttotal: 31m 16s\tremaining: 2h 4m 42s\n",
      "450:\tlearn: 0.7379085\ttest: 0.7366866\tbest: 0.7366866 (450)\ttotal: 35m 20s\tremaining: 2h 1m 21s\n",
      "500:\tlearn: 0.7408790\ttest: 0.7395865\tbest: 0.7395865 (494)\ttotal: 39m 13s\tremaining: 1h 57m 20s\n",
      "550:\tlearn: 0.7460515\ttest: 0.7441364\tbest: 0.7441364 (550)\ttotal: 43m 23s\tremaining: 1h 54m 5s\n",
      "600:\tlearn: 0.7514280\ttest: 0.7494513\tbest: 0.7495163 (598)\ttotal: 48m 27s\tremaining: 1h 52m 47s\n",
      "650:\tlearn: 0.7565800\ttest: 0.7542361\tbest: 0.7542361 (650)\ttotal: 52m 34s\tremaining: 1h 48m 56s\n",
      "700:\tlearn: 0.7627690\ttest: 0.7600160\tbest: 0.7600260 (699)\ttotal: 56m 57s\tremaining: 1h 45m 32s\n",
      "750:\tlearn: 0.7673550\ttest: 0.7646459\tbest: 0.7646459 (750)\ttotal: 1h 1m 28s\tremaining: 1h 42m 13s\n",
      "800:\tlearn: 0.7709265\ttest: 0.7678058\tbest: 0.7678308 (797)\ttotal: 1h 5m 10s\tremaining: 1h 37m 32s\n",
      "850:\tlearn: 0.7744180\ttest: 0.7715007\tbest: 0.7715007 (850)\ttotal: 1h 8m 37s\tremaining: 1h 32m 38s\n",
      "900:\tlearn: 0.7782635\ttest: 0.7757406\tbest: 0.7757406 (900)\ttotal: 1h 12m 59s\tremaining: 1h 29m 2s\n",
      "950:\tlearn: 0.7827280\ttest: 0.7797605\tbest: 0.7797605 (950)\ttotal: 1h 17m 7s\tremaining: 1h 25m 4s\n",
      "1000:\tlearn: 0.7870440\ttest: 0.7836704\tbest: 0.7836954 (997)\ttotal: 1h 21m 5s\tremaining: 1h 20m 55s\n",
      "1050:\tlearn: 0.7916085\ttest: 0.7877003\tbest: 0.7878153 (1049)\ttotal: 1h 25m 29s\tremaining: 1h 17m 11s\n",
      "1100:\tlearn: 0.7957995\ttest: 0.7911102\tbest: 0.7911152 (1092)\ttotal: 1h 29m 29s\tremaining: 1h 13m 4s\n",
      "1150:\tlearn: 0.7983945\ttest: 0.7938502\tbest: 0.7938602 (1149)\ttotal: 1h 32m 30s\tremaining: 1h 8m 14s\n",
      "1200:\tlearn: 0.8024695\ttest: 0.7988800\tbest: 0.7989050 (1197)\ttotal: 1h 36m 35s\tremaining: 1h 4m 15s\n",
      "1250:\tlearn: 0.8074170\ttest: 0.8037399\tbest: 0.8038049 (1247)\ttotal: 1h 40m 47s\tremaining: 1h 20s\n",
      "1300:\tlearn: 0.8111045\ttest: 0.8069848\tbest: 0.8070148 (1299)\ttotal: 1h 44m 58s\tremaining: 56m 24s\n",
      "1350:\tlearn: 0.8150480\ttest: 0.8108647\tbest: 0.8108647 (1350)\ttotal: 1h 49m 14s\tremaining: 52m 28s\n",
      "1400:\tlearn: 0.8200370\ttest: 0.8156496\tbest: 0.8156646 (1398)\ttotal: 1h 53m 31s\tremaining: 48m 32s\n",
      "1450:\tlearn: 0.8241700\ttest: 0.8197145\tbest: 0.8197145 (1450)\ttotal: 1h 57m 24s\tremaining: 44m 25s\n",
      "1500:\tlearn: 0.8267855\ttest: 0.8221594\tbest: 0.8221794 (1499)\ttotal: 2h 1m 1s\tremaining: 40m 14s\n",
      "1550:\tlearn: 0.8312825\ttest: 0.8270643\tbest: 0.8270643 (1550)\ttotal: 2h 5m 26s\tremaining: 36m 18s\n",
      "1600:\tlearn: 0.8345785\ttest: 0.8301342\tbest: 0.8301392 (1599)\ttotal: 2h 9m 33s\tremaining: 32m 17s\n",
      "1650:\tlearn: 0.8398620\ttest: 0.8354741\tbest: 0.8355741 (1647)\ttotal: 2h 14m 30s\tremaining: 28m 26s\n",
      "1700:\tlearn: 0.8428595\ttest: 0.8381390\tbest: 0.8381590 (1699)\ttotal: 2h 18m 31s\tremaining: 24m 20s\n",
      "1750:\tlearn: 0.8460245\ttest: 0.8412590\tbest: 0.8412840 (1749)\ttotal: 2h 22m 50s\tremaining: 20m 18s\n",
      "1800:\tlearn: 0.8492350\ttest: 0.8439989\tbest: 0.8440089 (1799)\ttotal: 2h 26m 43s\tremaining: 16m 12s\n",
      "1850:\tlearn: 0.8514995\ttest: 0.8462838\tbest: 0.8463538 (1848)\ttotal: 2h 30m 50s\tremaining: 12m 8s\n",
      "1900:\tlearn: 0.8536775\ttest: 0.8479038\tbest: 0.8479138 (1899)\ttotal: 2h 34m 35s\tremaining: 8m 3s\n",
      "1950:\tlearn: 0.8579695\ttest: 0.8520387\tbest: 0.8520387 (1950)\ttotal: 2h 39m 18s\tremaining: 4m\n",
      "1999:\tlearn: 0.8612675\ttest: 0.8552286\tbest: 0.8552436 (1998)\ttotal: 2h 43m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8552436189\n",
      "bestIteration = 1998\n",
      "\n",
      "Shrink model to first 1999 iterations.\n",
      "0.8485415918150102\n",
      "0:\tlearn: 0.5242730\ttest: 0.5239569\tbest: 0.5239569 (0)\ttotal: 4.46s\tremaining: 2h 28m 32s\n",
      "50:\tlearn: 0.6951490\ttest: 0.6953076\tbest: 0.6958726 (43)\ttotal: 4m 15s\tremaining: 2h 42m 47s\n",
      "100:\tlearn: 0.7021680\ttest: 0.7021424\tbest: 0.7021424 (100)\ttotal: 9m 2s\tremaining: 2h 49m 56s\n",
      "150:\tlearn: 0.7060355\ttest: 0.7053324\tbest: 0.7053974 (149)\ttotal: 13m 7s\tremaining: 2h 40m 39s\n",
      "200:\tlearn: 0.7125645\ttest: 0.7110772\tbest: 0.7110772 (200)\ttotal: 17m 30s\tremaining: 2h 36m 42s\n",
      "250:\tlearn: 0.7186835\ttest: 0.7163071\tbest: 0.7163471 (249)\ttotal: 21m 58s\tremaining: 2h 33m 8s\n",
      "300:\tlearn: 0.7217845\ttest: 0.7192170\tbest: 0.7192170 (296)\ttotal: 25m 24s\tremaining: 2h 23m 25s\n",
      "350:\tlearn: 0.7271545\ttest: 0.7238069\tbest: 0.7238569 (349)\ttotal: 29m 29s\tremaining: 2h 18m 34s\n",
      "400:\tlearn: 0.7336410\ttest: 0.7301417\tbest: 0.7301417 (400)\ttotal: 33m 46s\tremaining: 2h 14m 40s\n",
      "450:\tlearn: 0.7379085\ttest: 0.7336067\tbest: 0.7336467 (445)\ttotal: 37m 46s\tremaining: 2h 9m 44s\n",
      "500:\tlearn: 0.7408790\ttest: 0.7357766\tbest: 0.7357766 (500)\ttotal: 41m 35s\tremaining: 2h 4m 26s\n",
      "550:\tlearn: 0.7460515\ttest: 0.7401915\tbest: 0.7401915 (550)\ttotal: 45m 43s\tremaining: 2h 14s\n",
      "600:\tlearn: 0.7514280\ttest: 0.7448064\tbest: 0.7448564 (597)\ttotal: 50m 35s\tremaining: 1h 57m 46s\n",
      "650:\tlearn: 0.7565800\ttest: 0.7496513\tbest: 0.7496513 (650)\ttotal: 54m 38s\tremaining: 1h 53m 14s\n",
      "700:\tlearn: 0.7627690\ttest: 0.7549861\tbest: 0.7549861 (699)\ttotal: 59m 1s\tremaining: 1h 49m 23s\n",
      "750:\tlearn: 0.7673550\ttest: 0.7589410\tbest: 0.7589410 (750)\ttotal: 1h 3m 36s\tremaining: 1h 45m 47s\n",
      "800:\tlearn: 0.7709265\ttest: 0.7621109\tbest: 0.7621209 (797)\ttotal: 1h 7m 18s\tremaining: 1h 40m 44s\n",
      "850:\tlearn: 0.7744180\ttest: 0.7653509\tbest: 0.7653509 (848)\ttotal: 1h 10m 43s\tremaining: 1h 35m 28s\n",
      "900:\tlearn: 0.7782635\ttest: 0.7693308\tbest: 0.7693708 (897)\ttotal: 1h 15m 3s\tremaining: 1h 31m 33s\n",
      "950:\tlearn: 0.7827280\ttest: 0.7734957\tbest: 0.7735157 (949)\ttotal: 1h 19m 6s\tremaining: 1h 27m 16s\n",
      "1000:\tlearn: 0.7870440\ttest: 0.7772256\tbest: 0.7772806 (997)\ttotal: 1h 23m 1s\tremaining: 1h 22m 51s\n",
      "1050:\tlearn: 0.7916085\ttest: 0.7811905\tbest: 0.7812855 (1049)\ttotal: 1h 27m 22s\tremaining: 1h 18m 53s\n",
      "1100:\tlearn: 0.7957995\ttest: 0.7852604\tbest: 0.7852604 (1100)\ttotal: 1h 31m 25s\tremaining: 1h 14m 38s\n",
      "1150:\tlearn: 0.7983945\ttest: 0.7873603\tbest: 0.7873753 (1146)\ttotal: 1h 34m 23s\tremaining: 1h 9m 37s\n",
      "1200:\tlearn: 0.8024695\ttest: 0.7913302\tbest: 0.7914052 (1197)\ttotal: 1h 38m 24s\tremaining: 1h 5m 28s\n",
      "1250:\tlearn: 0.8074170\ttest: 0.7958501\tbest: 0.7958551 (1249)\ttotal: 1h 42m 35s\tremaining: 1h 1m 25s\n",
      "1300:\tlearn: 0.8111045\ttest: 0.7991350\tbest: 0.7991350 (1300)\ttotal: 1h 46m 47s\tremaining: 57m 22s\n",
      "1350:\tlearn: 0.8150480\ttest: 0.8032249\tbest: 0.8032249 (1350)\ttotal: 1h 51m 3s\tremaining: 53m 21s\n",
      "1400:\tlearn: 0.8200370\ttest: 0.8076348\tbest: 0.8076348 (1400)\ttotal: 1h 55m 21s\tremaining: 49m 19s\n",
      "1450:\tlearn: 0.8241700\ttest: 0.8114197\tbest: 0.8114447 (1447)\ttotal: 1h 59m 13s\tremaining: 45m 6s\n",
      "1500:\tlearn: 0.8267855\ttest: 0.8139447\tbest: 0.8139497 (1499)\ttotal: 2h 2m 51s\tremaining: 40m 50s\n",
      "1550:\tlearn: 0.8312825\ttest: 0.8186795\tbest: 0.8186795 (1550)\ttotal: 2h 7m 16s\tremaining: 36m 50s\n",
      "1600:\tlearn: 0.8345785\ttest: 0.8216245\tbest: 0.8216245 (1600)\ttotal: 2h 11m 14s\tremaining: 32m 42s\n",
      "1650:\tlearn: 0.8398620\ttest: 0.8263293\tbest: 0.8263943 (1647)\ttotal: 2h 16m 8s\tremaining: 28m 46s\n",
      "1700:\tlearn: 0.8428595\ttest: 0.8292093\tbest: 0.8292543 (1698)\ttotal: 2h 20m 9s\tremaining: 24m 38s\n",
      "1750:\tlearn: 0.8460245\ttest: 0.8322642\tbest: 0.8322692 (1749)\ttotal: 2h 24m 28s\tremaining: 20m 32s\n",
      "1800:\tlearn: 0.8492350\ttest: 0.8346241\tbest: 0.8346591 (1799)\ttotal: 2h 28m 25s\tremaining: 16m 23s\n",
      "1850:\tlearn: 0.8514995\ttest: 0.8370641\tbest: 0.8370791 (1849)\ttotal: 2h 32m 30s\tremaining: 12m 16s\n",
      "1900:\tlearn: 0.8536775\ttest: 0.8384040\tbest: 0.8384590 (1898)\ttotal: 2h 36m 15s\tremaining: 8m 8s\n",
      "1950:\tlearn: 0.8579695\ttest: 0.8426139\tbest: 0.8426139 (1950)\ttotal: 2h 40m 58s\tremaining: 4m 2s\n",
      "1999:\tlearn: 0.8612675\ttest: 0.8454039\tbest: 0.8454689 (1997)\ttotal: 2h 45m\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8454688633\n",
      "bestIteration = 1997\n",
      "\n",
      "Shrink model to first 1998 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8377240364821147\n",
      "0:\tlearn: 0.5242730\ttest: 0.5241521\tbest: 0.5241521 (0)\ttotal: 4.04s\tremaining: 2h 14m 37s\n",
      "50:\tlearn: 0.6951490\ttest: 0.6940846\tbest: 0.6944596 (38)\ttotal: 4m 16s\tremaining: 2h 43m 29s\n",
      "100:\tlearn: 0.7021680\ttest: 0.7011295\tbest: 0.7011295 (100)\ttotal: 9m 3s\tremaining: 2h 50m 18s\n",
      "150:\tlearn: 0.7060355\ttest: 0.7050944\tbest: 0.7051444 (147)\ttotal: 13m 9s\tremaining: 2h 41m 11s\n",
      "200:\tlearn: 0.7125645\ttest: 0.7108043\tbest: 0.7108143 (199)\ttotal: 17m 30s\tremaining: 2h 36m 41s\n",
      "250:\tlearn: 0.7186835\ttest: 0.7162143\tbest: 0.7162593 (248)\ttotal: 22m 1s\tremaining: 2h 33m 25s\n",
      "300:\tlearn: 0.7217845\ttest: 0.7191892\tbest: 0.7192092 (299)\ttotal: 25m 28s\tremaining: 2h 23m 47s\n",
      "350:\tlearn: 0.7271545\ttest: 0.7241391\tbest: 0.7241391 (350)\ttotal: 29m 37s\tremaining: 2h 19m 10s\n",
      "400:\tlearn: 0.7336410\ttest: 0.7301890\tbest: 0.7302340 (399)\ttotal: 33m 54s\tremaining: 2h 15m 11s\n",
      "450:\tlearn: 0.7379085\ttest: 0.7339240\tbest: 0.7339240 (450)\ttotal: 37m 51s\tremaining: 2h 10m 2s\n",
      "500:\tlearn: 0.7408790\ttest: 0.7367989\tbest: 0.7367989 (500)\ttotal: 41m 39s\tremaining: 2h 4m 38s\n",
      "550:\tlearn: 0.7460515\ttest: 0.7414439\tbest: 0.7414439 (550)\ttotal: 45m 47s\tremaining: 2h 25s\n",
      "600:\tlearn: 0.7514280\ttest: 0.7462188\tbest: 0.7462188 (600)\ttotal: 50m 43s\tremaining: 1h 58m 4s\n",
      "650:\tlearn: 0.7565800\ttest: 0.7504387\tbest: 0.7504387 (650)\ttotal: 54m 48s\tremaining: 1h 53m 34s\n",
      "700:\tlearn: 0.7627690\ttest: 0.7561887\tbest: 0.7561887 (699)\ttotal: 59m 8s\tremaining: 1h 49m 36s\n",
      "750:\tlearn: 0.7673550\ttest: 0.7603986\tbest: 0.7603986 (750)\ttotal: 1h 3m 36s\tremaining: 1h 45m 47s\n",
      "800:\tlearn: 0.7709265\ttest: 0.7638235\tbest: 0.7639285 (797)\ttotal: 1h 7m 21s\tremaining: 1h 40m 49s\n",
      "850:\tlearn: 0.7744180\ttest: 0.7673985\tbest: 0.7674385 (848)\ttotal: 1h 10m 48s\tremaining: 1h 35m 36s\n",
      "900:\tlearn: 0.7782635\ttest: 0.7709634\tbest: 0.7709634 (898)\ttotal: 1h 15m 13s\tremaining: 1h 31m 44s\n",
      "950:\tlearn: 0.7827280\ttest: 0.7749234\tbest: 0.7749984 (949)\ttotal: 1h 19m 22s\tremaining: 1h 27m 32s\n",
      "1000:\tlearn: 0.7870440\ttest: 0.7791533\tbest: 0.7791533 (1000)\ttotal: 1h 23m 20s\tremaining: 1h 23m 10s\n",
      "1050:\tlearn: 0.7916085\ttest: 0.7834232\tbest: 0.7834482 (1049)\ttotal: 1h 27m 46s\tremaining: 1h 19m 15s\n",
      "1100:\tlearn: 0.7957995\ttest: 0.7875432\tbest: 0.7875432 (1100)\ttotal: 1h 31m 49s\tremaining: 1h 14m 58s\n",
      "1150:\tlearn: 0.7983945\ttest: 0.7899282\tbest: 0.7899332 (1147)\ttotal: 1h 34m 50s\tremaining: 1h 9m 57s\n",
      "1200:\tlearn: 0.8024695\ttest: 0.7938631\tbest: 0.7939631 (1197)\ttotal: 1h 38m 26s\tremaining: 1h 5m 29s\n",
      "1250:\tlearn: 0.8074170\ttest: 0.7984680\tbest: 0.7984680 (1247)\ttotal: 1h 42m 21s\tremaining: 1h 1m 16s\n",
      "1300:\tlearn: 0.8111045\ttest: 0.8024430\tbest: 0.8024680 (1299)\ttotal: 1h 46m 15s\tremaining: 57m 5s\n",
      "1350:\tlearn: 0.8150480\ttest: 0.8063229\tbest: 0.8063229 (1350)\ttotal: 1h 50m 2s\tremaining: 52m 51s\n",
      "1400:\tlearn: 0.8200370\ttest: 0.8110528\tbest: 0.8111228 (1396)\ttotal: 1h 53m 51s\tremaining: 48m 41s\n",
      "1450:\tlearn: 0.8241700\ttest: 0.8152428\tbest: 0.8152428 (1450)\ttotal: 1h 57m 19s\tremaining: 44m 23s\n",
      "1500:\tlearn: 0.8267855\ttest: 0.8176727\tbest: 0.8176877 (1499)\ttotal: 2h 45s\tremaining: 40m 8s\n",
      "1550:\tlearn: 0.8312825\ttest: 0.8213777\tbest: 0.8214277 (1548)\ttotal: 2h 4m 44s\tremaining: 36m 6s\n",
      "1600:\tlearn: 0.8345785\ttest: 0.8245076\tbest: 0.8245126 (1599)\ttotal: 2h 8m 29s\tremaining: 32m 1s\n",
      "1650:\tlearn: 0.8398620\ttest: 0.8297576\tbest: 0.8298526 (1647)\ttotal: 2h 12m 55s\tremaining: 28m 5s\n",
      "1700:\tlearn: 0.8428595\ttest: 0.8325375\tbest: 0.8325575 (1699)\ttotal: 2h 16m 29s\tremaining: 23m 59s\n",
      "1750:\tlearn: 0.8460245\ttest: 0.8356975\tbest: 0.8357025 (1745)\ttotal: 2h 20m 19s\tremaining: 19m 57s\n",
      "1800:\tlearn: 0.8492350\ttest: 0.8390524\tbest: 0.8390874 (1799)\ttotal: 2h 23m 54s\tremaining: 15m 54s\n",
      "1850:\tlearn: 0.8514995\ttest: 0.8411574\tbest: 0.8411624 (1849)\ttotal: 2h 27m 37s\tremaining: 11m 53s\n",
      "1900:\tlearn: 0.8536775\ttest: 0.8434373\tbest: 0.8434573 (1898)\ttotal: 2h 31m 13s\tremaining: 7m 52s\n",
      "1950:\tlearn: 0.8579695\ttest: 0.8476923\tbest: 0.8476923 (1950)\ttotal: 2h 35m 46s\tremaining: 3m 54s\n",
      "1999:\tlearn: 0.8612675\ttest: 0.8509772\tbest: 0.8510522 (1997)\ttotal: 2h 39m 22s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8510522342\n",
      "bestIteration = 1997\n",
      "\n",
      "Shrink model to first 1998 iterations.\n",
      "0.8438295418583703\n",
      "0:\tlearn: 0.5242730\ttest: 0.5241674\tbest: 0.5241674 (0)\ttotal: 4.32s\tremaining: 2h 24m 3s\n",
      "50:\tlearn: 0.6951490\ttest: 0.6946915\tbest: 0.6950015 (43)\ttotal: 4m 47s\tremaining: 3h 2m 50s\n",
      "100:\tlearn: 0.7021680\ttest: 0.7013115\tbest: 0.7013115 (100)\ttotal: 9m 31s\tremaining: 2h 59m 11s\n",
      "150:\tlearn: 0.7060355\ttest: 0.7048965\tbest: 0.7049065 (147)\ttotal: 13m 32s\tremaining: 2h 45m 47s\n",
      "200:\tlearn: 0.7125645\ttest: 0.7114514\tbest: 0.7114614 (198)\ttotal: 17m 20s\tremaining: 2h 35m 14s\n",
      "250:\tlearn: 0.7186835\ttest: 0.7171214\tbest: 0.7171214 (250)\ttotal: 21m 20s\tremaining: 2h 28m 43s\n",
      "300:\tlearn: 0.7217845\ttest: 0.7198464\tbest: 0.7198814 (299)\ttotal: 24m 31s\tremaining: 2h 18m 25s\n",
      "350:\tlearn: 0.7271545\ttest: 0.7249314\tbest: 0.7249714 (349)\ttotal: 28m 26s\tremaining: 2h 13m 37s\n",
      "400:\tlearn: 0.7336410\ttest: 0.7317463\tbest: 0.7317463 (400)\ttotal: 32m 45s\tremaining: 2h 10m 35s\n",
      "450:\tlearn: 0.7379085\ttest: 0.7354563\tbest: 0.7354563 (445)\ttotal: 36m 36s\tremaining: 2h 5m 44s\n",
      "500:\tlearn: 0.7408790\ttest: 0.7383063\tbest: 0.7383513 (493)\ttotal: 40m 3s\tremaining: 1h 59m 51s\n",
      "550:\tlearn: 0.7460515\ttest: 0.7440513\tbest: 0.7440513 (550)\ttotal: 43m 46s\tremaining: 1h 55m 6s\n",
      "600:\tlearn: 0.7514280\ttest: 0.7482613\tbest: 0.7482613 (599)\ttotal: 48m 18s\tremaining: 1h 52m 26s\n",
      "650:\tlearn: 0.7565800\ttest: 0.7529962\tbest: 0.7529962 (650)\ttotal: 51m 59s\tremaining: 1h 47m 44s\n",
      "700:\tlearn: 0.7627690\ttest: 0.7586312\tbest: 0.7586312 (699)\ttotal: 56m 1s\tremaining: 1h 43m 48s\n",
      "750:\tlearn: 0.7673550\ttest: 0.7631662\tbest: 0.7631662 (750)\ttotal: 1h 36s\tremaining: 1h 40m 48s\n",
      "800:\tlearn: 0.7709265\ttest: 0.7661462\tbest: 0.7662262 (797)\ttotal: 1h 3m 53s\tremaining: 1h 35m 38s\n",
      "850:\tlearn: 0.7744180\ttest: 0.7699162\tbest: 0.7699262 (845)\ttotal: 1h 6m 59s\tremaining: 1h 30m 27s\n",
      "900:\tlearn: 0.7782635\ttest: 0.7735611\tbest: 0.7735611 (897)\ttotal: 1h 10m 51s\tremaining: 1h 26m 25s\n",
      "950:\tlearn: 0.7827280\ttest: 0.7777011\tbest: 0.7777011 (950)\ttotal: 1h 14m 31s\tremaining: 1h 22m 12s\n",
      "1000:\tlearn: 0.7870440\ttest: 0.7818961\tbest: 0.7819611 (996)\ttotal: 1h 18m 4s\tremaining: 1h 17m 55s\n",
      "1050:\tlearn: 0.7916085\ttest: 0.7861661\tbest: 0.7861761 (1044)\ttotal: 1h 21m 54s\tremaining: 1h 13m 57s\n",
      "1100:\tlearn: 0.7957995\ttest: 0.7906310\tbest: 0.7906310 (1100)\ttotal: 1h 25m 31s\tremaining: 1h 9m 50s\n",
      "1150:\tlearn: 0.7983945\ttest: 0.7931410\tbest: 0.7931810 (1147)\ttotal: 1h 28m 12s\tremaining: 1h 5m 4s\n",
      "1200:\tlearn: 0.8024695\ttest: 0.7965860\tbest: 0.7966160 (1197)\ttotal: 1h 31m 55s\tremaining: 1h 1m 9s\n",
      "1250:\tlearn: 0.8074170\ttest: 0.8009260\tbest: 0.8010260 (1245)\ttotal: 1h 35m 41s\tremaining: 57m 17s\n",
      "1300:\tlearn: 0.8111045\ttest: 0.8051010\tbest: 0.8051060 (1299)\ttotal: 1h 39m 26s\tremaining: 53m 25s\n",
      "1350:\tlearn: 0.8150480\ttest: 0.8085460\tbest: 0.8085460 (1350)\ttotal: 1h 43m 16s\tremaining: 49m 36s\n",
      "1400:\tlearn: 0.8200370\ttest: 0.8133659\tbest: 0.8133659 (1400)\ttotal: 1h 47m 6s\tremaining: 45m 47s\n",
      "1450:\tlearn: 0.8241700\ttest: 0.8174159\tbest: 0.8174159 (1450)\ttotal: 1h 50m 46s\tremaining: 41m 54s\n",
      "1500:\tlearn: 0.8267855\ttest: 0.8197909\tbest: 0.8197909 (1499)\ttotal: 1h 54m 1s\tremaining: 37m 54s\n",
      "1550:\tlearn: 0.8312825\ttest: 0.8236559\tbest: 0.8236709 (1549)\ttotal: 1h 57m 59s\tremaining: 34m 9s\n",
      "1600:\tlearn: 0.8345785\ttest: 0.8268009\tbest: 0.8268009 (1600)\ttotal: 2h 1m 33s\tremaining: 30m 17s\n",
      "1650:\tlearn: 0.8398620\ttest: 0.8319408\tbest: 0.8319708 (1647)\ttotal: 2h 5m 52s\tremaining: 26m 36s\n",
      "1700:\tlearn: 0.8428595\ttest: 0.8350208\tbest: 0.8350208 (1699)\ttotal: 2h 9m 25s\tremaining: 22m 45s\n",
      "1750:\tlearn: 0.8460245\ttest: 0.8380408\tbest: 0.8380658 (1749)\ttotal: 2h 13m 15s\tremaining: 18m 56s\n",
      "1800:\tlearn: 0.8492350\ttest: 0.8414608\tbest: 0.8414608 (1800)\ttotal: 2h 16m 48s\tremaining: 15m 7s\n",
      "1850:\tlearn: 0.8514995\ttest: 0.8438508\tbest: 0.8438508 (1849)\ttotal: 2h 20m 53s\tremaining: 11m 20s\n",
      "1900:\tlearn: 0.8536775\ttest: 0.8459308\tbest: 0.8459408 (1899)\ttotal: 2h 24m 38s\tremaining: 7m 31s\n",
      "1950:\tlearn: 0.8579695\ttest: 0.8501707\tbest: 0.8501907 (1949)\ttotal: 2h 29m 23s\tremaining: 3m 45s\n",
      "1999:\tlearn: 0.8612675\ttest: 0.8532357\tbest: 0.8532907 (1997)\ttotal: 2h 33m 32s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8532907335\n",
      "bestIteration = 1997\n",
      "\n",
      "Shrink model to first 1998 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8463172395106957\n",
      "0:\tlearn: 0.5242730\ttest: 0.5244250\tbest: 0.5244250 (0)\ttotal: 4.52s\tremaining: 2h 30m 44s\n",
      "50:\tlearn: 0.6951490\ttest: 0.6952950\tbest: 0.6957200 (43)\ttotal: 4m 18s\tremaining: 2h 44m 36s\n",
      "100:\tlearn: 0.7021680\ttest: 0.7017950\tbest: 0.7017950 (100)\ttotal: 9m 2s\tremaining: 2h 49m 53s\n",
      "150:\tlearn: 0.7060355\ttest: 0.7054400\tbest: 0.7054400 (150)\ttotal: 13m 7s\tremaining: 2h 40m 42s\n",
      "200:\tlearn: 0.7125645\ttest: 0.7122550\tbest: 0.7122600 (198)\ttotal: 17m 9s\tremaining: 2h 33m 34s\n",
      "250:\tlearn: 0.7186835\ttest: 0.7181150\tbest: 0.7181250 (248)\ttotal: 21m 12s\tremaining: 2h 27m 47s\n",
      "300:\tlearn: 0.7217845\ttest: 0.7215400\tbest: 0.7215600 (299)\ttotal: 24m 20s\tremaining: 2h 17m 25s\n",
      "350:\tlearn: 0.7271545\ttest: 0.7269200\tbest: 0.7269500 (349)\ttotal: 28m 1s\tremaining: 2h 11m 40s\n",
      "400:\tlearn: 0.7336410\ttest: 0.7338350\tbest: 0.7338350 (400)\ttotal: 31m 50s\tremaining: 2h 6m 58s\n",
      "450:\tlearn: 0.7379085\ttest: 0.7392100\tbest: 0.7392100 (450)\ttotal: 35m 22s\tremaining: 2h 1m 29s\n",
      "500:\tlearn: 0.7408790\ttest: 0.7427150\tbest: 0.7427250 (498)\ttotal: 38m 50s\tremaining: 1h 56m 12s\n",
      "550:\tlearn: 0.7460515\ttest: 0.7486550\tbest: 0.7486550 (550)\ttotal: 42m 28s\tremaining: 1h 51m 40s\n",
      "600:\tlearn: 0.7514280\ttest: 0.7541100\tbest: 0.7541100 (600)\ttotal: 47m 11s\tremaining: 1h 49m 51s\n",
      "650:\tlearn: 0.7565800\ttest: 0.7589050\tbest: 0.7589050 (650)\ttotal: 50m 55s\tremaining: 1h 45m 31s\n",
      "700:\tlearn: 0.7627690\ttest: 0.7649250\tbest: 0.7649650 (698)\ttotal: 54m 50s\tremaining: 1h 41m 38s\n",
      "750:\tlearn: 0.7673550\ttest: 0.7696550\tbest: 0.7696550 (750)\ttotal: 58m 47s\tremaining: 1h 37m 45s\n",
      "800:\tlearn: 0.7709265\ttest: 0.7731900\tbest: 0.7732650 (797)\ttotal: 1h 2m 2s\tremaining: 1h 32m 52s\n",
      "850:\tlearn: 0.7744180\ttest: 0.7765500\tbest: 0.7766050 (841)\ttotal: 1h 5m 8s\tremaining: 1h 27m 57s\n",
      "900:\tlearn: 0.7782635\ttest: 0.7799250\tbest: 0.7799950 (897)\ttotal: 1h 9m 35s\tremaining: 1h 24m 53s\n",
      "950:\tlearn: 0.7827280\ttest: 0.7841750\tbest: 0.7841750 (950)\ttotal: 1h 13m 41s\tremaining: 1h 21m 17s\n",
      "1000:\tlearn: 0.7870440\ttest: 0.7891950\tbest: 0.7892300 (997)\ttotal: 1h 17m 18s\tremaining: 1h 17m 9s\n",
      "1050:\tlearn: 0.7916085\ttest: 0.7941300\tbest: 0.7941750 (1049)\ttotal: 1h 21m 13s\tremaining: 1h 13m 20s\n",
      "1100:\tlearn: 0.7957995\ttest: 0.7986550\tbest: 0.7986550 (1100)\ttotal: 1h 24m 52s\tremaining: 1h 9m 18s\n",
      "1150:\tlearn: 0.7983945\ttest: 0.8013650\tbest: 0.8013900 (1149)\ttotal: 1h 27m 34s\tremaining: 1h 4m 35s\n",
      "1200:\tlearn: 0.8024695\ttest: 0.8057100\tbest: 0.8057150 (1197)\ttotal: 1h 31m 10s\tremaining: 1h 39s\n",
      "1250:\tlearn: 0.8074170\ttest: 0.8102500\tbest: 0.8103400 (1247)\ttotal: 1h 34m 52s\tremaining: 56m 48s\n",
      "1300:\tlearn: 0.8111045\ttest: 0.8142000\tbest: 0.8142150 (1299)\ttotal: 1h 38m 38s\tremaining: 52m 59s\n",
      "1350:\tlearn: 0.8150480\ttest: 0.8186250\tbest: 0.8186250 (1350)\ttotal: 1h 42m 26s\tremaining: 49m 12s\n",
      "1400:\tlearn: 0.8200370\ttest: 0.8231750\tbest: 0.8232000 (1396)\ttotal: 1h 46m 14s\tremaining: 45m 25s\n",
      "1450:\tlearn: 0.8241700\ttest: 0.8269950\tbest: 0.8271300 (1446)\ttotal: 1h 49m 59s\tremaining: 41m 36s\n",
      "1500:\tlearn: 0.8267855\ttest: 0.8301600\tbest: 0.8301850 (1499)\ttotal: 1h 53m 31s\tremaining: 37m 44s\n",
      "1550:\tlearn: 0.8312825\ttest: 0.8344400\tbest: 0.8344400 (1550)\ttotal: 1h 57m 24s\tremaining: 33m 59s\n",
      "1600:\tlearn: 0.8345785\ttest: 0.8376550\tbest: 0.8376900 (1599)\ttotal: 2h 53s\tremaining: 30m 7s\n",
      "1650:\tlearn: 0.8398620\ttest: 0.8430400\tbest: 0.8431100 (1647)\ttotal: 2h 5m 7s\tremaining: 26m 27s\n",
      "1700:\tlearn: 0.8428595\ttest: 0.8464650\tbest: 0.8465300 (1698)\ttotal: 2h 8m 42s\tremaining: 22m 37s\n",
      "1750:\tlearn: 0.8460245\ttest: 0.8495350\tbest: 0.8495450 (1749)\ttotal: 2h 12m 33s\tremaining: 18m 50s\n",
      "1800:\tlearn: 0.8492350\ttest: 0.8523650\tbest: 0.8524000 (1799)\ttotal: 2h 16m 5s\tremaining: 15m 2s\n",
      "1850:\tlearn: 0.8514995\ttest: 0.8544750\tbest: 0.8544800 (1848)\ttotal: 2h 19m 49s\tremaining: 11m 15s\n",
      "1900:\tlearn: 0.8536775\ttest: 0.8571650\tbest: 0.8571800 (1899)\ttotal: 2h 23m 10s\tremaining: 7m 27s\n",
      "1950:\tlearn: 0.8579695\ttest: 0.8613450\tbest: 0.8613850 (1949)\ttotal: 2h 27m 20s\tremaining: 3m 42s\n",
      "1999:\tlearn: 0.8612675\ttest: 0.8646000\tbest: 0.8646100 (1998)\ttotal: 2h 30m 59s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.86461\n",
      "bestIteration = 1998\n",
      "\n",
      "Shrink model to first 1999 iterations.\n",
      "0.8589575226104825\n",
      "('cv f1-score: ', 0.8470739864553346)\n",
      "done, good luck!!\n"
     ]
    }
   ],
   "source": [
    "result_lgb, score = train_lgb_k_fold(second_tr, train_y, second_te)\n",
    "submit_result(submit, result_lgb, 'lgb_cb', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_p_0</th>\n",
       "      <th>0_p_1</th>\n",
       "      <th>0_p_2</th>\n",
       "      <th>0_p_3</th>\n",
       "      <th>0_p_4</th>\n",
       "      <th>0_p_5</th>\n",
       "      <th>0_p_6</th>\n",
       "      <th>0_p_7</th>\n",
       "      <th>0_p_8</th>\n",
       "      <th>0_p_9</th>\n",
       "      <th>0_p_10</th>\n",
       "      <th>0_p_11</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>1_p_0</th>\n",
       "      <th>1_p_1</th>\n",
       "      <th>1_p_2</th>\n",
       "      <th>1_p_3</th>\n",
       "      <th>1_p_4</th>\n",
       "      <th>1_p_5</th>\n",
       "      <th>1_p_6</th>\n",
       "      <th>1_p_7</th>\n",
       "      <th>1_p_8</th>\n",
       "      <th>1_p_9</th>\n",
       "      <th>1_p_10</th>\n",
       "      <th>1_p_11</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>2_p_0</th>\n",
       "      <th>2_p_1</th>\n",
       "      <th>2_p_2</th>\n",
       "      <th>2_p_3</th>\n",
       "      <th>2_p_4</th>\n",
       "      <th>2_p_5</th>\n",
       "      <th>2_p_6</th>\n",
       "      <th>2_p_7</th>\n",
       "      <th>2_p_8</th>\n",
       "      <th>2_p_9</th>\n",
       "      <th>2_p_10</th>\n",
       "      <th>2_p_11</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>3_p_0</th>\n",
       "      <th>3_p_1</th>\n",
       "      <th>3_p_2</th>\n",
       "      <th>3_p_3</th>\n",
       "      <th>3_p_4</th>\n",
       "      <th>3_p_5</th>\n",
       "      <th>3_p_6</th>\n",
       "      <th>3_p_7</th>\n",
       "      <th>3_p_8</th>\n",
       "      <th>3_p_9</th>\n",
       "      <th>3_p_10</th>\n",
       "      <th>3_p_11</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>day_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.063521</td>\n",
       "      <td>0.092185</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.034933</td>\n",
       "      <td>0.081638</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.208854</td>\n",
       "      <td>0.518583</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>9</td>\n",
       "      <td>0.042378</td>\n",
       "      <td>0.082386</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.016214</td>\n",
       "      <td>0.030390</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.272202</td>\n",
       "      <td>0.556261</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>9</td>\n",
       "      <td>0.071603</td>\n",
       "      <td>0.079008</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.031312</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.251604</td>\n",
       "      <td>0.529209</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>9</td>\n",
       "      <td>0.051013</td>\n",
       "      <td>0.065138</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.018638</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.448510</td>\n",
       "      <td>0.386060</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-11-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100435</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.741668</td>\n",
       "      <td>0.040830</td>\n",
       "      <td>0.046701</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.034420</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.035723</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>2</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.857631</td>\n",
       "      <td>0.041134</td>\n",
       "      <td>0.018924</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.012183</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2</td>\n",
       "      <td>0.085213</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.795073</td>\n",
       "      <td>0.045320</td>\n",
       "      <td>0.028665</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.023906</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.021636</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2</td>\n",
       "      <td>0.063042</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.818190</td>\n",
       "      <td>0.037851</td>\n",
       "      <td>0.034649</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.024327</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.021735</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088905</td>\n",
       "      <td>0.347591</td>\n",
       "      <td>0.295987</td>\n",
       "      <td>0.215927</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106378</td>\n",
       "      <td>0.347318</td>\n",
       "      <td>0.213742</td>\n",
       "      <td>0.246295</td>\n",
       "      <td>0.058143</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.027928</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098038</td>\n",
       "      <td>0.411012</td>\n",
       "      <td>0.185402</td>\n",
       "      <td>0.251407</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>0.119018</td>\n",
       "      <td>0.314588</td>\n",
       "      <td>0.232325</td>\n",
       "      <td>0.258539</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.017261</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147799</td>\n",
       "      <td>0.063931</td>\n",
       "      <td>0.588528</td>\n",
       "      <td>0.073423</td>\n",
       "      <td>0.034477</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.030740</td>\n",
       "      <td>0.060919</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>2</td>\n",
       "      <td>0.120988</td>\n",
       "      <td>0.047610</td>\n",
       "      <td>0.665637</td>\n",
       "      <td>0.029811</td>\n",
       "      <td>0.036431</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.052247</td>\n",
       "      <td>0.047128</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>2</td>\n",
       "      <td>0.102034</td>\n",
       "      <td>0.073223</td>\n",
       "      <td>0.650539</td>\n",
       "      <td>0.065138</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.047849</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>2</td>\n",
       "      <td>0.119811</td>\n",
       "      <td>0.055797</td>\n",
       "      <td>0.657135</td>\n",
       "      <td>0.045720</td>\n",
       "      <td>0.028271</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.035371</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124355</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.091685</td>\n",
       "      <td>0.045553</td>\n",
       "      <td>0.445706</td>\n",
       "      <td>0.292429</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.145287</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.074586</td>\n",
       "      <td>0.034027</td>\n",
       "      <td>0.408849</td>\n",
       "      <td>0.336893</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>5</td>\n",
       "      <td>0.143082</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.062285</td>\n",
       "      <td>0.044527</td>\n",
       "      <td>0.370505</td>\n",
       "      <td>0.379225</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>6</td>\n",
       "      <td>0.116442</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.102519</td>\n",
       "      <td>0.048516</td>\n",
       "      <td>0.242149</td>\n",
       "      <td>0.490096</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-10-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0_p_0     0_p_1     0_p_2     0_p_3     0_p_4     0_p_5     0_p_6  \\\n",
       "0  0.063521  0.092185  0.000057  0.034933  0.081638  0.000007  0.000030   \n",
       "1  0.100435  0.000078  0.741668  0.040830  0.046701  0.000024  0.000030   \n",
       "2  0.088905  0.347591  0.295987  0.215927  0.035136  0.000009  0.016340   \n",
       "3  0.147799  0.063931  0.588528  0.073423  0.034477  0.000020  0.000053   \n",
       "4  0.124355  0.000111  0.000045  0.091685  0.045553  0.445706  0.292429   \n",
       "\n",
       "      0_p_7     0_p_8     0_p_9    0_p_10    0_p_11  pred_0     1_p_0  \\\n",
       "0  0.000125  0.208854  0.518583  0.000044  0.000023       9  0.042378   \n",
       "1  0.034420  0.000015  0.000046  0.035723  0.000030       2  0.044820   \n",
       "2  0.000037  0.000004  0.000037  0.000019  0.000008       1  0.106378   \n",
       "3  0.000087  0.000006  0.030740  0.060919  0.000016       2  0.120988   \n",
       "4  0.000037  0.000010  0.000035  0.000015  0.000018       5  0.145287   \n",
       "\n",
       "      1_p_1     1_p_2     1_p_3     1_p_4     1_p_5     1_p_6     1_p_7  \\\n",
       "0  0.082386  0.000061  0.016214  0.030390  0.000009  0.000016  0.000047   \n",
       "1  0.000036  0.857631  0.041134  0.018924  0.000014  0.000005  0.025233   \n",
       "2  0.347318  0.213742  0.246295  0.058143  0.000028  0.027928  0.000073   \n",
       "3  0.047610  0.665637  0.029811  0.036431  0.000027  0.000048  0.000047   \n",
       "4  0.000134  0.000048  0.074586  0.034027  0.408849  0.336893  0.000079   \n",
       "\n",
       "      1_p_8     1_p_9    1_p_10    1_p_11  pred_1     2_p_0     2_p_1  \\\n",
       "0  0.272202  0.556261  0.000027  0.000009       9  0.071603  0.079008   \n",
       "1  0.000002  0.000014  0.012183  0.000004       2  0.085213  0.000099   \n",
       "2  0.000006  0.000046  0.000031  0.000013       1  0.098038  0.411012   \n",
       "3  0.000011  0.052247  0.047128  0.000016       2  0.102034  0.073223   \n",
       "4  0.000005  0.000055  0.000012  0.000024       5  0.143082  0.000151   \n",
       "\n",
       "      2_p_2     2_p_3     2_p_4     2_p_5     2_p_6     2_p_7     2_p_8  \\\n",
       "0  0.000070  0.031312  0.036944  0.000041  0.000055  0.000069  0.251604   \n",
       "1  0.795073  0.045320  0.028665  0.000018  0.000018  0.023906  0.000006   \n",
       "2  0.185402  0.251407  0.043956  0.000011  0.010060  0.000046  0.000006   \n",
       "3  0.650539  0.065138  0.032795  0.000009  0.000041  0.000056  0.000007   \n",
       "4  0.000074  0.062285  0.044527  0.370505  0.379225  0.000034  0.000007   \n",
       "\n",
       "      2_p_9    2_p_10    2_p_11  pred_2     3_p_0     3_p_1     3_p_2  \\\n",
       "0  0.529209  0.000039  0.000047       9  0.051013  0.065138  0.000047   \n",
       "1  0.000032  0.021636  0.000014       2  0.063042  0.000083  0.818190   \n",
       "2  0.000040  0.000016  0.000006       1  0.119018  0.314588  0.232325   \n",
       "3  0.028297  0.047849  0.000012       2  0.119811  0.055797  0.657135   \n",
       "4  0.000068  0.000024  0.000020       6  0.116442  0.000095  0.000051   \n",
       "\n",
       "      3_p_3     3_p_4     3_p_5     3_p_6     3_p_7     3_p_8     3_p_9  \\\n",
       "0  0.018638  0.030511  0.000003  0.000012  0.000043  0.448510  0.386060   \n",
       "1  0.037851  0.034649  0.000021  0.000020  0.024327  0.000012  0.000045   \n",
       "2  0.258539  0.058145  0.000012  0.017261  0.000053  0.000006  0.000033   \n",
       "3  0.045720  0.028271  0.000015  0.000038  0.000053  0.000005  0.035371   \n",
       "4  0.102519  0.048516  0.242149  0.490096  0.000049  0.000005  0.000046   \n",
       "\n",
       "     3_p_10    3_p_11  pred_3    day_time  \n",
       "0  0.000015  0.000010       8  2018-11-26  \n",
       "1  0.021735  0.000026       2  2018-11-08  \n",
       "2  0.000016  0.000004       1  2018-10-18  \n",
       "3  0.057778  0.000007       2  2018-10-25  \n",
       "4  0.000010  0.000021       6  2018-10-26  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_tr_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = np.array(second_tr_d['click_mode'])\n",
    "second_tr_d = second_tr_d.drop(['click_mode',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 70 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.709291\n",
      "[100]\tvalid_0's f1_weighted: 0.709239\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-b9b41b898968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_lgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_trsh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecond_tesh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-233-141f659944ba>\u001b[0m in \u001b[0;36mtrain_lgb\u001b[0;34m(train_x, train_y, test_x)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf1_weighted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcate_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             verbose=50, early_stopping_rounds=70)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1879\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1880\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1882\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_lgb(second_trsh, y3,second_tesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1x=tr1.drop(['min_price', ], axis=1)\n",
    "te1x=te1.drop(['min_price', ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tr['pred_0'] = second_tr['pred_0'].apply(lambda x: int(x))\n",
    "second_tr['pred_1'] = second_tr['pred_1'].apply(lambda x: int(x))\n",
    "second_tr['pred_2'] = second_tr['pred_2'].apply(lambda x: int(x))\n",
    "second_tr['pred_3'] = second_tr['pred_3'].apply(lambda x: int(x))\n",
    "second_tr['pred_5'] = second_tr['pred_5'].apply(lambda x: int(x))\n",
    "second_te['pred_0'] = second_te['pred_0'].apply(lambda x: int(x))\n",
    "second_te['pred_1'] = second_te['pred_1'].apply(lambda x: int(x))\n",
    "second_te['pred_2'] = second_te['pred_2'].apply(lambda x: int(x))\n",
    "second_te['pred_3'] = second_te['pred_3'].apply(lambda x: int(x))\n",
    "second_te['pred_5'] = second_te['pred_5'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(second_tr['pred_0'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_p_0</th>\n",
       "      <th>0_p_1</th>\n",
       "      <th>0_p_2</th>\n",
       "      <th>0_p_3</th>\n",
       "      <th>0_p_4</th>\n",
       "      <th>0_p_5</th>\n",
       "      <th>0_p_6</th>\n",
       "      <th>0_p_7</th>\n",
       "      <th>0_p_8</th>\n",
       "      <th>0_p_9</th>\n",
       "      <th>0_p_10</th>\n",
       "      <th>0_p_11</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>1_p_0</th>\n",
       "      <th>1_p_1</th>\n",
       "      <th>1_p_2</th>\n",
       "      <th>1_p_3</th>\n",
       "      <th>1_p_4</th>\n",
       "      <th>1_p_5</th>\n",
       "      <th>1_p_6</th>\n",
       "      <th>1_p_7</th>\n",
       "      <th>1_p_8</th>\n",
       "      <th>1_p_9</th>\n",
       "      <th>1_p_10</th>\n",
       "      <th>1_p_11</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>2_p_0</th>\n",
       "      <th>2_p_1</th>\n",
       "      <th>2_p_2</th>\n",
       "      <th>2_p_3</th>\n",
       "      <th>2_p_4</th>\n",
       "      <th>2_p_5</th>\n",
       "      <th>2_p_6</th>\n",
       "      <th>2_p_7</th>\n",
       "      <th>2_p_8</th>\n",
       "      <th>2_p_9</th>\n",
       "      <th>2_p_10</th>\n",
       "      <th>2_p_11</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>3_p_0</th>\n",
       "      <th>3_p_1</th>\n",
       "      <th>3_p_2</th>\n",
       "      <th>3_p_3</th>\n",
       "      <th>3_p_4</th>\n",
       "      <th>3_p_5</th>\n",
       "      <th>3_p_6</th>\n",
       "      <th>3_p_7</th>\n",
       "      <th>3_p_8</th>\n",
       "      <th>3_p_9</th>\n",
       "      <th>3_p_10</th>\n",
       "      <th>3_p_11</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>day_time</th>\n",
       "      <th>5_p_0</th>\n",
       "      <th>5_p_1</th>\n",
       "      <th>5_p_2</th>\n",
       "      <th>5_p_3</th>\n",
       "      <th>5_p_4</th>\n",
       "      <th>5_p_5</th>\n",
       "      <th>5_p_6</th>\n",
       "      <th>5_p_7</th>\n",
       "      <th>5_p_8</th>\n",
       "      <th>5_p_9</th>\n",
       "      <th>5_p_10</th>\n",
       "      <th>5_p_11</th>\n",
       "      <th>pred_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033562</td>\n",
       "      <td>0.047144</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.022349</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.873864</td>\n",
       "      <td>1.757926e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.044344</td>\n",
       "      <td>0.050507</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.035109</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.844135</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.056194</td>\n",
       "      <td>0.053114</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.035755</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.830428</td>\n",
       "      <td>2.350842e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>7</td>\n",
       "      <td>0.047073</td>\n",
       "      <td>0.043507</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.027892</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.849509</td>\n",
       "      <td>6.131563e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.031411</td>\n",
       "      <td>0.024484</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.855020</td>\n",
       "      <td>2.849463e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130341</td>\n",
       "      <td>0.520585</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.093939</td>\n",
       "      <td>0.077766</td>\n",
       "      <td>0.035163</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>1.914481e-06</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131059</td>\n",
       "      <td>0.501161</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.092742</td>\n",
       "      <td>0.067975</td>\n",
       "      <td>0.038309</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.143977</td>\n",
       "      <td>0.497717</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.093092</td>\n",
       "      <td>0.048945</td>\n",
       "      <td>0.030602</td>\n",
       "      <td>0.185598</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>9.879611e-07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.108037</td>\n",
       "      <td>0.546799</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.083617</td>\n",
       "      <td>0.064925</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.162060</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>9.971085e-07</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>0.137766</td>\n",
       "      <td>0.517716</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.085009</td>\n",
       "      <td>0.049220</td>\n",
       "      <td>0.027040</td>\n",
       "      <td>0.183161</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1.949927e-06</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049892</td>\n",
       "      <td>0.132524</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>0.025480</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>4.238716e-01</td>\n",
       "      <td>0.332889</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>8</td>\n",
       "      <td>0.056489</td>\n",
       "      <td>0.131915</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.029554</td>\n",
       "      <td>0.031692</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.397685</td>\n",
       "      <td>0.352599</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8</td>\n",
       "      <td>0.051440</td>\n",
       "      <td>0.087051</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.192888e-01</td>\n",
       "      <td>0.380202</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>8</td>\n",
       "      <td>0.059624</td>\n",
       "      <td>0.164638</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.035616</td>\n",
       "      <td>0.037137</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>2.909146e-01</td>\n",
       "      <td>0.411998</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>0.064366</td>\n",
       "      <td>0.171058</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.059308</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>2.830513e-01</td>\n",
       "      <td>0.372248</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042847</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.021508</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.820962</td>\n",
       "      <td>2.984088e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "      <td>0.043479</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.023530</td>\n",
       "      <td>0.023543</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.781998</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.127370</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.029401</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.019535</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.827252</td>\n",
       "      <td>2.177640e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.100457</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>7</td>\n",
       "      <td>0.027291</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>0.024525</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.838425</td>\n",
       "      <td>4.833374e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.087331</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>0.039298</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.025962</td>\n",
       "      <td>0.032515</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>2.892590e-06</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.123653</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.026030</td>\n",
       "      <td>0.019273</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.831006</td>\n",
       "      <td>5.659521e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7</td>\n",
       "      <td>0.142136</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.810355</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>0.177270</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.035786</td>\n",
       "      <td>0.021141</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.765710</td>\n",
       "      <td>1.628460e-06</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "      <td>0.156387</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.031264</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.778485</td>\n",
       "      <td>2.199086e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-28</td>\n",
       "      <td>0.093893</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.015023</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.872850</td>\n",
       "      <td>8.010360e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0_p_0     0_p_1     0_p_2     0_p_3     0_p_4     0_p_5     0_p_6  \\\n",
       "0  0.033562  0.047144  0.000008  0.022349  0.023047  0.000003  0.000004   \n",
       "1  0.130341  0.520585  0.000017  0.093939  0.077766  0.035163  0.142137   \n",
       "2  0.049892  0.132524  0.000013  0.035263  0.025480  0.000004  0.000012   \n",
       "3  0.042847  0.000017  0.000007  0.022492  0.021508  0.000003  0.000006   \n",
       "4  0.123653  0.000013  0.000005  0.026030  0.019273  0.000002  0.000003   \n",
       "\n",
       "      0_p_7         0_p_8     0_p_9    0_p_10    0_p_11  pred_0     1_p_0  \\\n",
       "0  0.873864  1.757926e-06  0.000006  0.000007  0.000005       7  0.044344   \n",
       "1  0.000023  1.914481e-06  0.000014  0.000007  0.000007       1  0.131059   \n",
       "2  0.000025  4.238716e-01  0.332889  0.000015  0.000011       8  0.056489   \n",
       "3  0.820962  2.984088e-06  0.000008  0.092140  0.000006       7  0.043479   \n",
       "4  0.831006  5.659521e-07  0.000005  0.000009  0.000002       7  0.142136   \n",
       "\n",
       "      1_p_1     1_p_2     1_p_3     1_p_4     1_p_5     1_p_6     1_p_7  \\\n",
       "0  0.050507  0.000009  0.035109  0.025871  0.000004  0.000002  0.844135   \n",
       "1  0.501161  0.000025  0.092742  0.067975  0.038309  0.168663  0.000024   \n",
       "2  0.131915  0.000015  0.029554  0.031692  0.000002  0.000006  0.000022   \n",
       "3  0.000022  0.000015  0.023530  0.023543  0.000007  0.000013  0.781998   \n",
       "4  0.000022  0.000010  0.027497  0.019931  0.000006  0.000012  0.810355   \n",
       "\n",
       "      1_p_8     1_p_9    1_p_10    1_p_11  pred_1     2_p_0     2_p_1  \\\n",
       "0  0.000001  0.000006  0.000008  0.000005       7  0.056194  0.053114   \n",
       "1  0.000003  0.000025  0.000005  0.000009       1  0.143977  0.497717   \n",
       "2  0.397685  0.352599  0.000013  0.000008       8  0.051440  0.087051   \n",
       "3  0.000007  0.000011  0.127370  0.000005       7  0.029401  0.000017   \n",
       "4  0.000003  0.000011  0.000010  0.000005       7  0.177270  0.000025   \n",
       "\n",
       "      2_p_2     2_p_3     2_p_4     2_p_5     2_p_6     2_p_7         2_p_8  \\\n",
       "0  0.000014  0.035755  0.024457  0.000005  0.000008  0.830428  2.350842e-06   \n",
       "1  0.000022  0.093092  0.048945  0.030602  0.185598  0.000019  9.879611e-07   \n",
       "2  0.000015  0.029175  0.032766  0.000003  0.000012  0.000020  4.192888e-01   \n",
       "3  0.000008  0.019535  0.023310  0.000002  0.000005  0.827252  2.177640e-06   \n",
       "4  0.000009  0.035786  0.021141  0.000012  0.000012  0.765710  1.628460e-06   \n",
       "\n",
       "      2_p_9    2_p_10    2_p_11  pred_2     3_p_0     3_p_1     3_p_2  \\\n",
       "0  0.000008  0.000006  0.000010       7  0.047073  0.043507  0.000009   \n",
       "1  0.000015  0.000004  0.000007       1  0.108037  0.546799  0.000019   \n",
       "2  0.380202  0.000014  0.000013       8  0.059624  0.164638  0.000017   \n",
       "3  0.000007  0.100457  0.000004       7  0.027291  0.000022  0.000014   \n",
       "4  0.000011  0.000017  0.000006       7  0.156387  0.000017  0.000009   \n",
       "\n",
       "      3_p_3     3_p_4     3_p_5     3_p_6     3_p_7         3_p_8     3_p_9  \\\n",
       "0  0.027892  0.031986  0.000003  0.000002  0.849509  6.131563e-07  0.000005   \n",
       "1  0.083617  0.064925  0.034502  0.162060  0.000019  9.971085e-07  0.000012   \n",
       "2  0.035616  0.037137  0.000003  0.000017  0.000025  2.909146e-01  0.411998   \n",
       "3  0.022358  0.024525  0.000004  0.000008  0.838425  4.833374e-06  0.000012   \n",
       "4  0.033800  0.031264  0.000003  0.000005  0.778485  2.199086e-06  0.000006   \n",
       "\n",
       "     3_p_10    3_p_11  pred_3    day_time     5_p_0     5_p_1     5_p_2  \\\n",
       "0  0.000008  0.000005       7  2018-10-07  0.040837  0.048200  0.000011   \n",
       "1  0.000003  0.000006       1  2018-10-13  0.137766  0.517716  0.000023   \n",
       "2  0.000006  0.000006       9  2018-11-26  0.064366  0.171058  0.000016   \n",
       "3  0.087331  0.000005       7  2018-10-21  0.039298  0.000018  0.000007   \n",
       "4  0.000016  0.000005       7  2018-10-28  0.093893  0.000010  0.000005   \n",
       "\n",
       "      5_p_3     5_p_4     5_p_5     5_p_6     5_p_7         5_p_8     5_p_9  \\\n",
       "0  0.031411  0.024484  0.000006  0.000006  0.855020  2.849463e-06  0.000009   \n",
       "1  0.085009  0.049220  0.027040  0.183161  0.000032  1.949927e-06  0.000018   \n",
       "2  0.059308  0.049895  0.000003  0.000016  0.000020  2.830513e-01  0.372248   \n",
       "3  0.025962  0.032515  0.000003  0.000005  0.824782  2.892590e-06  0.000010   \n",
       "4  0.018182  0.015023  0.000002  0.000008  0.872850  8.010360e-07  0.000005   \n",
       "\n",
       "     5_p_10    5_p_11  pred_5  \n",
       "0  0.000005  0.000010       7  \n",
       "1  0.000007  0.000008       1  \n",
       "2  0.000013  0.000007       9  \n",
       "3  0.077393  0.000004       7  \n",
       "4  0.000015  0.000006       7  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 0, 1, ..., 1, 0, 9])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'second_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-28dc4913ab35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain_lgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmit_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lgb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'second_tr' is not defined"
     ]
    }
   ],
   "source": [
    "result_lgb, score =train_lgb(second_tr, train_y, second_te)\n",
    "submit_result(submit, result_lgb, 'lgb', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_tr21 = pd.DataFrame(s_tr21)\n",
    "s_te21 = pd.DataFrame(s_te21)\n",
    "s_tr21.columns = ['p_{}'.format(i) for i in range(12)]\n",
    "s_te21.columns = ['p_{}'.format(i) for i in range(12)]\n",
    "s_tr21['pred'] = pred_tr21\n",
    "s_te21['pred'] = pred_test21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_tr21.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_te2 = s_te2.drop(['day_time'], axis = 1)\n",
    "a = ['p_x{}'.format(i) for i in range(12)]\n",
    "a.append('pred_x')\n",
    "s_te2.columns = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_te22  = pd.concat([s_te21, s_te2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2193953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2296423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2212635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2189960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2281854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sid  pred_1\n",
       "0   2193953       0\n",
       "1   2296423       0\n",
       "4   2212635       0\n",
       "9   2189960       0\n",
       "12  2281854       0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb1['pred' + '_' + str(1)] = 0\n",
    "sb1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_0</th>\n",
       "      <th>p_1</th>\n",
       "      <th>p_2</th>\n",
       "      <th>p_3</th>\n",
       "      <th>p_4</th>\n",
       "      <th>p_5</th>\n",
       "      <th>p_6</th>\n",
       "      <th>p_7</th>\n",
       "      <th>p_8</th>\n",
       "      <th>p_9</th>\n",
       "      <th>p_10</th>\n",
       "      <th>p_11</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064936</td>\n",
       "      <td>0.079495</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.035471</td>\n",
       "      <td>0.068954</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.160410</td>\n",
       "      <td>0.590320</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047631</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.805921</td>\n",
       "      <td>0.051170</td>\n",
       "      <td>0.031765</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.038021</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.025320</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059350</td>\n",
       "      <td>0.312222</td>\n",
       "      <td>0.357617</td>\n",
       "      <td>0.166813</td>\n",
       "      <td>0.089756</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.014082</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.045542</td>\n",
       "      <td>0.694834</td>\n",
       "      <td>0.060587</td>\n",
       "      <td>0.025109</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.037005</td>\n",
       "      <td>0.076146</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.097660</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.084640</td>\n",
       "      <td>0.068969</td>\n",
       "      <td>0.294119</td>\n",
       "      <td>0.453977</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020353</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.194611</td>\n",
       "      <td>0.022212</td>\n",
       "      <td>0.009782</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.752706</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.080680</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.031423</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.822646</td>\n",
       "      <td>0.064766</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.030837</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.856358</td>\n",
       "      <td>0.039914</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.047226</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.024463</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.850570</td>\n",
       "      <td>0.028507</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.069904</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.073524</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.044650</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.860880</td>\n",
       "      <td>0.020614</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        p_0       p_1       p_2       p_3       p_4       p_5       p_6  \\\n",
       "0  0.064936  0.079495  0.000096  0.035471  0.068954  0.000059  0.000053   \n",
       "1  0.047631  0.000078  0.805921  0.051170  0.031765  0.000012  0.000017   \n",
       "2  0.059350  0.312222  0.357617  0.166813  0.089756  0.000018  0.014082   \n",
       "3  0.060613  0.045542  0.694834  0.060587  0.025109  0.000016  0.000053   \n",
       "4  0.097660  0.000180  0.000107  0.084640  0.068969  0.294119  0.453977   \n",
       "5  0.020353  0.000117  0.194611  0.022212  0.009782  0.000030  0.000042   \n",
       "6  0.080680  0.000105  0.000053  0.031423  0.000219  0.822646  0.064766   \n",
       "7  0.030837  0.015010  0.856358  0.039914  0.010488  0.000026  0.000028   \n",
       "8  0.024463  0.000067  0.850570  0.028507  0.026359  0.000016  0.000020   \n",
       "9  0.073524  0.000069  0.000039  0.044650  0.000141  0.860880  0.020614   \n",
       "\n",
       "        p_7       p_8       p_9      p_10      p_11  pred  \n",
       "0  0.000113  0.160410  0.590320  0.000056  0.000036     9  \n",
       "1  0.038021  0.000008  0.000038  0.025320  0.000019     2  \n",
       "2  0.000050  0.000006  0.000043  0.000024  0.000018     2  \n",
       "3  0.000060  0.000019  0.037005  0.076146  0.000016     2  \n",
       "4  0.000113  0.000033  0.000092  0.000051  0.000059     6  \n",
       "5  0.000067  0.000016  0.752706  0.000039  0.000026     9  \n",
       "6  0.000042  0.000006  0.000030  0.000015  0.000017     5  \n",
       "7  0.000056  0.000016  0.047226  0.000025  0.000018     2  \n",
       "8  0.000049  0.000006  0.069904  0.000027  0.000012     2  \n",
       "9  0.000020  0.000003  0.000033  0.000016  0.000011     5  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_tr2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resx = np.array(result_lgb).T\n",
    "for line in resx:\n",
    "        print np.argmax(np.std(line))\n",
    "        print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.613923\n",
      "[20]\tvalid_0's f1_weighted: 0.651579\n",
      "[30]\tvalid_0's f1_weighted: 0.653046\n",
      "[40]\tvalid_0's f1_weighted: 0.653523\n",
      "[50]\tvalid_0's f1_weighted: 0.654577\n",
      "[60]\tvalid_0's f1_weighted: 0.655392\n",
      "[70]\tvalid_0's f1_weighted: 0.656345\n",
      "[80]\tvalid_0's f1_weighted: 0.657102\n",
      "[90]\tvalid_0's f1_weighted: 0.657732\n",
      "[100]\tvalid_0's f1_weighted: 0.658032\n",
      "[110]\tvalid_0's f1_weighted: 0.658493\n",
      "[120]\tvalid_0's f1_weighted: 0.658885\n",
      "[130]\tvalid_0's f1_weighted: 0.659427\n",
      "[140]\tvalid_0's f1_weighted: 0.659805\n",
      "[150]\tvalid_0's f1_weighted: 0.660076\n",
      "[160]\tvalid_0's f1_weighted: 0.660442\n",
      "[170]\tvalid_0's f1_weighted: 0.660816\n",
      "[180]\tvalid_0's f1_weighted: 0.660854\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[180]\tvalid_0's f1_weighted: 0.660854\n",
      "(0, 0.20230982151379212, 0.4562672909129602, 0.6415320167564332, 0.35402906208718626)\n",
      "(1, 0.14622506442683975, 0.673741768579492, 0.6129118528027385, 0.747976501305483)\n",
      "(2, 0.22025388947217714, 0.8748896689040786, 0.8311298311298311, 0.9235136072109551)\n",
      "(3, 0.03152620024816264, 0.03702632340179346, 0.4155844155844156, 0.01937632455343627)\n",
      "(4, 0.017705450033406508, 0.0032275416890801506, 0.75, 0.0016172506738544475)\n",
      "(5, 0.09632528395533073, 0.7963398925065517, 0.7216810240721359, 0.8882282996432818)\n",
      "(6, 0.01577741719958003, 0.17287234042553193, 0.32338308457711445, 0.11796733212341198)\n",
      "(7, 0.13641309535172283, 0.7523691011773714, 0.6915361604786204, 0.8249370277078085)\n",
      "(8, 0.0033024720817027777, 0.2408759124087591, 0.32673267326732675, 0.1907514450867052)\n",
      "(9, 0.09528490980242436, 0.7672829413490965, 0.6597692862292718, 0.9166583191425424)\n",
      "(10, 0.01916579173427508, 0.5059582919563059, 0.5044554455445545, 0.5074701195219123)\n",
      "(11, 0.015710604180586046, 0.6642030056664203, 0.5586406962287609, 0.818955042527339)\n",
      "0.6608538213531733\n",
      "180\n",
      "fit over\n"
     ]
    }
   ],
   "source": [
    "result_lgb, score, imp = train_lgb_time_serie(second_trx, train_y,second_tex)#用来测试特征重要性\n",
    "submit = submit_result(submit, result_lgb, 'lgb_time', score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x121239450>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hourDF = tr1.groupby(['first_mode', 'click_mode'])['first_mode'].count().unstack('click_mode').fillna(0)\n",
    "hourDF.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels, preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "\n",
    "def lgb_sample_select(train_x, train_y, test_x):\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "    \n",
    "    train_index = (train_x['day_time'] <= '2018-11-23')\n",
    "    valid_index = (train_x['day_time'] > '2018-11-23') & (train_x['day_time'] < '2018-12-01')\n",
    "    \n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    \n",
    "    tr_x1     = train_x[train_index]\n",
    "    tr_y1     = train_y[train_index]\n",
    "    val_x     = train_x[valid_index]\n",
    "    val_y     = train_y[valid_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    cate_cols = ['max_dist_mode', 'min_dist_mode', #'max_price_mode',\n",
    "                 'min_price_mode', 'max_eta_mode', 'min_eta_mode',\n",
    "                 'first_mode','last_mode', 'week', ]\n",
    "    scores = []\n",
    "    result = []\n",
    "    \n",
    "    select = pd.DataFrame()\n",
    "    for i in range(5):\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=41,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=2000, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.1, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "        \n",
    "        tr_x, tr_y, all_train_x, all_train_y = gen_data(tr_x1, tr_y1, val_x, val_y)\n",
    "        \n",
    "        eval_set = [(val_x, val_y)]\n",
    "        lgb_model.fit(\n",
    "            tr_x, tr_y, eval_set=eval_set,\n",
    "            eval_metric=f1_weighted,\n",
    "            categorical_feature=cate_cols,\n",
    "            verbose=50, early_stopping_rounds=50)\n",
    "        \n",
    "        \n",
    "        def get_weighted_fscore(y_pred, y_true):\n",
    "            f_score = 0\n",
    "            for i in range(12):\n",
    "                yt = y_true == i\n",
    "                yp = y_pred == i\n",
    "                f_score += dic_[i] * f1_score(y_true=yt, y_pred= yp)\n",
    "                print(i,dic_[i],f1_score(y_true=yt, y_pred= yp), precision_score(y_true=yt, y_pred= yp),recall_score(y_true=yt, y_pred= yp))\n",
    "            print(f_score)\n",
    "            return f_score\n",
    "        \n",
    "        val_pred = lgb_model.predict(val_x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = lgb_model.predict(val_x) \n",
    "        df_analysis = pd.DataFrame()\n",
    "        #df_analysis['sid']   = val_x['sid']\n",
    "        df_analysis['label'] = val_y\n",
    "        df_analysis['pred']  = pred\n",
    "        df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "        dic_ = df_analysis['label'].value_counts(normalize = True)\n",
    "        \n",
    "        val_score = get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "        \n",
    "        if(val_score > 0.66395):\n",
    "            df1 = pd.concat([tr_x, tr_y], axis=1)\n",
    "            \n",
    "            select = pd.concat([select, df1], axis=0)\n",
    "            select.drop_duplicates(keep='first',inplace=True)\n",
    "    select.to_csv('./data/selected_samples.csv', index=False)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12051ce90>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hourDF = tr1.groupby(['rank2_t', 'click_mode'])['rank2_t'].count().unstack('click_mode').fillna(0)\n",
    "hourDF.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x121154fd0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hourDF = tr1.groupby(['last_mode', 'click_mode'])['last_mode'].count().unstack('click_mode').fillna(0)\n",
    "hourDF.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353719"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.660105\n",
      "[100]\tvalid_0's f1_weighted: 0.662656\n",
      "[150]\tvalid_0's f1_weighted: 0.663351\n",
      "[200]\tvalid_0's f1_weighted: 0.663552\n",
      "[250]\tvalid_0's f1_weighted: 0.663936\n",
      "[300]\tvalid_0's f1_weighted: 0.664107\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid_0's f1_weighted: 0.664377\n",
      "(0, 0.20230982151379212, 0.4559170495467528, 0.6665758896151053, 0.3464332892998679)\n",
      "(1, 0.14622506442683975, 0.6705946337926032, 0.6242686768676867, 0.7243472584856396)\n",
      "(2, 0.22025388947217714, 0.8751364375888133, 0.8338369765707783, 0.920740162939851)\n",
      "(3, 0.03152620024816264, 0.08078660643103906, 0.33043478260869563, 0.04601877081441114)\n",
      "(4, 0.017705450033406508, 0.04523107177974434, 0.2569832402234637, 0.024797843665768194)\n",
      "(5, 0.09632528395533073, 0.7963071380319748, 0.7298769916618509, 0.8760404280618311)\n",
      "(6, 0.01577741719958003, 0.2669245647969052, 0.2857142857142857, 0.25045372050816694)\n",
      "(7, 0.13641309535172283, 0.7507414571244359, 0.696078431372549, 0.8147215225300868)\n",
      "(8, 0.0033024720817027777, 0.3124231242312423, 0.27194860813704497, 0.3670520231213873)\n",
      "(9, 0.09528490980242436, 0.7673317551572539, 0.6637899509983178, 0.909145547430632)\n",
      "(10, 0.01916579173427508, 0.5289993626513704, 0.46128195628010377, 0.6200199203187251)\n",
      "(11, 0.015710604180586046, 0.6601142857142857, 0.5291315500183217, 0.8772782503037667)\n",
      "0.6643774248348499\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.659572\n",
      "[100]\tvalid_0's f1_weighted: 0.662153\n",
      "[150]\tvalid_0's f1_weighted: 0.662858\n",
      "[200]\tvalid_0's f1_weighted: 0.663303\n",
      "[250]\tvalid_0's f1_weighted: 0.664002\n",
      "[300]\tvalid_0's f1_weighted: 0.664228\n",
      "Early stopping, best iteration is:\n",
      "[296]\tvalid_0's f1_weighted: 0.664417\n",
      "(0, 0.20230982151379212, 0.4552643048378059, 0.6692293611568735, 0.3449707491979619)\n",
      "(1, 0.14622506442683975, 0.6712146824438541, 0.6243261455525606, 0.7257180156657963)\n",
      "(2, 0.22025388947217714, 0.875193048205424, 0.8337976223172597, 0.920913503206795)\n",
      "(3, 0.03152620024816264, 0.07999999999999999, 0.3199152542372881, 0.0457160157432637)\n",
      "(4, 0.017705450033406508, 0.044987775061124696, 0.24210526315789474, 0.024797843665768194)\n",
      "(5, 0.09632528395533073, 0.793781353098025, 0.72970502700457, 0.8701942132382084)\n",
      "(6, 0.01577741719958003, 0.2788794460182562, 0.29068241469816275, 0.26799758015728975)\n",
      "(7, 0.13641309535172283, 0.7518292879476517, 0.6970294662602355, 0.8159809683739154)\n",
      "(8, 0.0033024720817027777, 0.2935064935064935, 0.2665094339622642, 0.3265895953757225)\n",
      "(9, 0.09528490980242436, 0.7675310984608896, 0.662758520244684, 0.9116498046679354)\n",
      "(10, 0.01916579173427508, 0.5320906587587376, 0.462956137117582, 0.6254980079681275)\n",
      "(11, 0.015710604180586046, 0.6593507087334248, 0.5285923753665689, 0.8760631834750912)\n",
      "0.6644168853549784\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.660188\n",
      "[100]\tvalid_0's f1_weighted: 0.662798\n",
      "[150]\tvalid_0's f1_weighted: 0.663214\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's f1_weighted: 0.663542\n",
      "(0, 0.20230982151379212, 0.45240410249976476, 0.6748385889398334, 0.34025287790149084)\n",
      "(1, 0.14622506442683975, 0.6713021491782554, 0.6228912970617808, 0.7278720626631854)\n",
      "(2, 0.22025388947217714, 0.8753681695536654, 0.8341511285574092, 0.920870168140059)\n",
      "(3, 0.03152620024816264, 0.07229565686538979, 0.3316831683168317, 0.04056917953375719)\n",
      "(4, 0.017705450033406508, 0.0380952380952381, 0.2714285714285714, 0.020485175202156335)\n",
      "(5, 0.09632528395533073, 0.7961068805479206, 0.7300223122056029, 0.8753468093539437)\n",
      "(6, 0.01577741719958003, 0.2718995290423862, 0.28263707571801566, 0.2619479733817302)\n",
      "(7, 0.13641309535172283, 0.7506693764314977, 0.6964146764829113, 0.8140917996081724)\n",
      "(8, 0.0033024720817027777, 0.32911392405063294, 0.2734225621414914, 0.41329479768786126)\n",
      "(9, 0.09528490980242436, 0.7675534601784813, 0.6619472881725115, 0.9132525292998097)\n",
      "(10, 0.01916579173427508, 0.5293870696893368, 0.45754716981132076, 0.6279880478087649)\n",
      "(11, 0.015710604180586046, 0.6589673913043478, 0.5252707581227437, 0.8839611178614823)\n",
      "0.6635422147997652\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.65972\n",
      "[100]\tvalid_0's f1_weighted: 0.662118\n",
      "[150]\tvalid_0's f1_weighted: 0.663002\n",
      "[200]\tvalid_0's f1_weighted: 0.662943\n",
      "Early stopping, best iteration is:\n",
      "[175]\tvalid_0's f1_weighted: 0.663331\n",
      "(0, 0.20230982151379212, 0.45297385009049496, 0.6689400921658987, 0.3424230986978675)\n",
      "(1, 0.14622506442683975, 0.6715315260983626, 0.6240542509667657, 0.7268276762402088)\n",
      "(2, 0.22025388947217714, 0.8754863012288755, 0.8337972946481083, 0.921563529207835)\n",
      "(3, 0.03152620024816264, 0.07221182134260497, 0.30963302752293576, 0.04087193460490463)\n",
      "(4, 0.017705450033406508, 0.04375932371954251, 0.28205128205128205, 0.02371967654986523)\n",
      "(5, 0.09632528395533073, 0.7920747188973521, 0.7301069876295553, 0.8655370590566785)\n",
      "(6, 0.01577741719958003, 0.27627818355033346, 0.2907754010695187, 0.2631578947368421)\n",
      "(7, 0.13641309535172283, 0.7510461597888367, 0.6954811017050196, 0.8162608452280996)\n",
      "(8, 0.0033024720817027777, 0.32886723507917176, 0.28421052631578947, 0.3901734104046243)\n",
      "(9, 0.09528490980242436, 0.7658464773922188, 0.660092807424594, 0.9119503155364119)\n",
      "(10, 0.01916579173427508, 0.5290131300296486, 0.46020633750921147, 0.6220119521912351)\n",
      "(11, 0.015710604180586046, 0.6560870550895488, 0.5233273056057867, 0.8791008505467801)\n",
      "0.6633308819895068\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's f1_weighted: 0.660897\n",
      "[100]\tvalid_0's f1_weighted: 0.66299\n",
      "[150]\tvalid_0's f1_weighted: 0.663453\n",
      "[200]\tvalid_0's f1_weighted: 0.66359\n",
      "[250]\tvalid_0's f1_weighted: 0.663995\n",
      "[300]\tvalid_0's f1_weighted: 0.663544\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's f1_weighted: 0.664075\n",
      "(0, 0.20230982151379212, 0.45389013174248083, 0.6647251547142337, 0.3445933194942442)\n",
      "(1, 0.14622506442683975, 0.6717330749666949, 0.6264400271063926, 0.7240861618798956)\n",
      "(2, 0.22025388947217714, 0.8740557785324687, 0.8323728880003136, 0.9201334720055468)\n",
      "(3, 0.03152620024816264, 0.0805687203791469, 0.3090909090909091, 0.04632152588555858)\n",
      "(4, 0.017705450033406508, 0.0466038671294001, 0.29012345679012347, 0.025336927223719677)\n",
      "(5, 0.09632528395533073, 0.7965250441156508, 0.7329502872845366, 0.8721759809750297)\n",
      "(6, 0.01577741719958003, 0.28046850269072493, 0.29415670650730413, 0.26799758015728975)\n",
      "(7, 0.13641309535172283, 0.7510711639444605, 0.6959818496626664, 0.8156311223061853)\n",
      "(8, 0.0033024720817027777, 0.3128342245989305, 0.291044776119403, 0.33815028901734107)\n",
      "(9, 0.09528490980242436, 0.766676499094012, 0.6616962467267966, 0.9112491235099669)\n",
      "(10, 0.01916579173427508, 0.5278481012658228, 0.45790629575402636, 0.62300796812749)\n",
      "(11, 0.015710604180586046, 0.6578947368421053, 0.5249818971759594, 0.8809234507897934)\n",
      "0.6640748864642911\n"
     ]
    }
   ],
   "source": [
    "lgb_sample_select(tr1, y1,te1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels, preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "\n",
    "def select(train_x, train_y):\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=41,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=2000, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.05, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "    \n",
    "    train_index = (train_x['day_time'] <= '2018-11-23')\n",
    "    valid_index = (train_x['day_time'] > '2018-11-23') & (train_x['day_time'] < '2018-12-01')\n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    \n",
    "    tr_x     = train_x[train_index]\n",
    "    tr_y     = train_y[train_index]\n",
    "    val_x     = train_x[valid_index]\n",
    "    val_y     = train_y[valid_index]\n",
    "\n",
    "    cate_cols = ['max_dist_mode', 'min_dist_mode', 'max_price_mode',\n",
    "                 'min_price_mode', 'max_eta_mode', 'min_eta_mode',\n",
    "                 'first_mode', 'last_mode','week']\n",
    "        \n",
    "        \n",
    "    print('Fiting...')\n",
    "    eval_set = [(val_x, val_y)]\n",
    "    lgb_model.fit(\n",
    "        tr_x, tr_y, eval_set=eval_set,\n",
    "        eval_metric=f1_weighted,\n",
    "        categorical_feature=cate_cols,\n",
    "        verbose=10, early_stopping_rounds=50)\n",
    "    \n",
    "    pred = lgb_model.predict(val_x) \n",
    "    df_analysis = pd.DataFrame()\n",
    "    #df_analysis['sid']   = val_x['sid']\n",
    "    df_analysis['label'] = val_y\n",
    "    df_analysis['pred']  = pred\n",
    "    df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "    dic_ = df_analysis['label'].value_counts(normalize = True)\n",
    "    \n",
    "    def get_weighted_fscore(y_pred, y_true):\n",
    "        f_score = 0\n",
    "        for i in range(12):\n",
    "            yt = y_true == i\n",
    "            yp = y_pred == i\n",
    "            f_score += dic_[i] * f1_score(y_true=yt, y_pred= yp)\n",
    "            print(i,dic_[i],f1_score(y_true=yt, y_pred= yp), precision_score(y_true=yt, y_pred= yp),recall_score(y_true=yt, y_pred= yp))\n",
    "        print(f_score)\n",
    "        return f_score\n",
    "    baseloss = get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "    \n",
    "    imp = pd.DataFrame()\n",
    "    imp['fea'] = list(val_x.columns)\n",
    "    imp['imp'] = lgb_model.feature_importances_\n",
    "    #imp = imp[imp['imp']>0]\n",
    "    imp = imp.sort_values('imp',ascending = False)\n",
    "    print('feature number',imp.shape[0])\n",
    "    n = lgb_model.best_iteration_\n",
    "    print('base f1 score:',baseloss)\n",
    "    col =list(imp['fea'])\n",
    "    col = [i for i in col if i not in cate_cols] \n",
    "    \n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=61,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=500, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.05, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "    \n",
    "\n",
    "    def evalsLoss(cols):\n",
    "        print('Runing...')\n",
    "        s = time.time()\n",
    "        colsx = list(set(cols).union(set(cate_cols)))\n",
    "        val_x1 = val_x[colsx].copy()\n",
    "        tr_x1 = tr_x[colsx].copy()\n",
    "        eval_set = [(val_x1, val_y)]\n",
    "        lgb_model.fit(tr_x1, tr_y, eval_set=eval_set,\n",
    "                      eval_metric=f1_weighted,\n",
    "                      categorical_feature=cate_cols,\n",
    "                      verbose=10, early_stopping_rounds=50)\n",
    "        pred = lgb_model.predict(val_x1)\n",
    "        df_analysis = pd.DataFrame()\n",
    "        df_analysis['label'] = val_y\n",
    "        df_analysis['pred']  = pred\n",
    "        df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "        print(time.time()-s,\"s\")\n",
    "        return get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "    \n",
    "    print('start to choose...')\n",
    "    all_num = int(len(col)/10)*10\n",
    "    print('total number of',all_num,'to computer')\n",
    "    loss = []\n",
    "    best_num = all_num\n",
    "    break_num = 0\n",
    "    for i in range(110,all_num,10):\n",
    "        loss.append(evalsLoss(col[:i]))\n",
    "        if loss[-1]>baseloss:\n",
    "            best_num = i\n",
    "            baseloss = loss[-1]\n",
    "        print('the beginning',i,'feature',loss[-1],'scored as',baseloss)\n",
    "    print('筛选出来最佳特征个数为',best_num,'这下子训练速度终于可以大大提升了')\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.625142\n",
      "[20]\tvalid_0's f1_weighted: 0.67666\n",
      "[30]\tvalid_0's f1_weighted: 0.68418\n",
      "[40]\tvalid_0's f1_weighted: 0.68617\n",
      "[50]\tvalid_0's f1_weighted: 0.68747\n",
      "[60]\tvalid_0's f1_weighted: 0.688018\n",
      "[70]\tvalid_0's f1_weighted: 0.688824\n",
      "[80]\tvalid_0's f1_weighted: 0.689075\n",
      "[90]\tvalid_0's f1_weighted: 0.689431\n",
      "[100]\tvalid_0's f1_weighted: 0.689676\n",
      "[110]\tvalid_0's f1_weighted: 0.689687\n",
      "[120]\tvalid_0's f1_weighted: 0.689934\n",
      "[130]\tvalid_0's f1_weighted: 0.689886\n",
      "[140]\tvalid_0's f1_weighted: 0.690156\n",
      "[150]\tvalid_0's f1_weighted: 0.690267\n",
      "[160]\tvalid_0's f1_weighted: 0.690415\n",
      "[170]\tvalid_0's f1_weighted: 0.690655\n",
      "[180]\tvalid_0's f1_weighted: 0.69077\n",
      "[190]\tvalid_0's f1_weighted: 0.690584\n",
      "[200]\tvalid_0's f1_weighted: 0.690694\n",
      "[210]\tvalid_0's f1_weighted: 0.690655\n",
      "[220]\tvalid_0's f1_weighted: 0.690904\n",
      "[230]\tvalid_0's f1_weighted: 0.690826\n",
      "[240]\tvalid_0's f1_weighted: 0.6907\n",
      "[250]\tvalid_0's f1_weighted: 0.690723\n",
      "[260]\tvalid_0's f1_weighted: 0.690637\n",
      "[270]\tvalid_0's f1_weighted: 0.690718\n",
      "Early stopping, best iteration is:\n",
      "[226]\tvalid_0's f1_weighted: 0.691\n",
      "(0, 0.08768752286864252, 0.351790450928382, 0.8563357546408394, 0.22136448988107657)\n",
      "(1, 0.14617636297109404, 0.6872427983539094, 0.6183947157726181, 0.7733416770963705)\n",
      "(2, 0.31163556531284303, 0.9010289887143175, 0.8519196568678732, 0.9561465304684749)\n",
      "(3, 0.04399926820343945, 0.12045616535994298, 0.42144638403990026, 0.07027027027027027)\n",
      "(4, 0.02420417124039517, 0.016212232866617538, 0.3235294117647059, 0.008314436885865457)\n",
      "(5, 0.09851811196487377, 0.8414852752880921, 0.7786729857819905, 0.9153203342618385)\n",
      "(6, 0.01995975118916941, 0.16508795669824086, 0.3152454780361757, 0.11182401466544455)\n",
      "(7, 0.17588730332967434, 0.7883551940039303, 0.7031058938615798, 0.8971291866028708)\n",
      "(8, 0.004793267471642883, 0.23671497584541065, 0.3223684210526316, 0.18702290076335878)\n",
      "(9, 0.05124405415294548, 0.5411996066863324, 0.6024518388791593, 0.491253123884327)\n",
      "(10, 0.028594950603732162, 0.5639629831246598, 0.49076267171956417, 0.6628278950735764)\n",
      "(11, 0.00729967069154775, 0.4596774193548387, 0.4956521739130435, 0.42857142857142855)\n",
      "0.6909995628240877\n",
      "('feature number', 229)\n",
      "('base f1 score:', 0.6909995628240877)\n",
      "start to choose...\n",
      "('total number of', 220, 'to computer')\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.620347\n",
      "[20]\tvalid_0's f1_weighted: 0.670001\n",
      "[30]\tvalid_0's f1_weighted: 0.676973\n",
      "[40]\tvalid_0's f1_weighted: 0.679877\n",
      "[50]\tvalid_0's f1_weighted: 0.681574\n",
      "[60]\tvalid_0's f1_weighted: 0.682573\n",
      "[70]\tvalid_0's f1_weighted: 0.683463\n",
      "[80]\tvalid_0's f1_weighted: 0.683911\n",
      "[90]\tvalid_0's f1_weighted: 0.684206\n",
      "[100]\tvalid_0's f1_weighted: 0.684622\n",
      "[110]\tvalid_0's f1_weighted: 0.684854\n",
      "[120]\tvalid_0's f1_weighted: 0.685255\n",
      "[130]\tvalid_0's f1_weighted: 0.685605\n",
      "[140]\tvalid_0's f1_weighted: 0.68584\n",
      "[150]\tvalid_0's f1_weighted: 0.685792\n",
      "[160]\tvalid_0's f1_weighted: 0.685769\n",
      "[170]\tvalid_0's f1_weighted: 0.686053\n",
      "[180]\tvalid_0's f1_weighted: 0.686306\n",
      "[190]\tvalid_0's f1_weighted: 0.686227\n",
      "[200]\tvalid_0's f1_weighted: 0.686399\n",
      "[210]\tvalid_0's f1_weighted: 0.686402\n",
      "[220]\tvalid_0's f1_weighted: 0.686539\n",
      "[230]\tvalid_0's f1_weighted: 0.686305\n",
      "[240]\tvalid_0's f1_weighted: 0.686354\n",
      "[250]\tvalid_0's f1_weighted: 0.686075\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid_0's f1_weighted: 0.686581\n",
      "(228.17549395561218, 's')\n",
      "(0, 0.08768752286864252, 0.35572343570372805, 0.8356481481481481, 0.22595451700396413)\n",
      "(1, 0.14617636297109404, 0.6858571825507084, 0.616791604197901, 0.7723404255319148)\n",
      "(2, 0.31163556531284303, 0.8990790271879997, 0.8477017470881864, 0.9570858283433133)\n",
      "(3, 0.04399926820343945, 0.11934011934011934, 0.38288288288288286, 0.07068607068607069)\n",
      "(4, 0.02420417124039517, 0.027656477438136828, 0.37254901960784315, 0.01436130007558579)\n",
      "(5, 0.09851811196487377, 0.841555327166838, 0.7809569225878239, 0.9123491179201486)\n",
      "(6, 0.01995975118916941, 0.16042077580539119, 0.2837209302325581, 0.11182401466544455)\n",
      "(7, 0.17588730332967434, 0.7841697266421869, 0.6949779027721976, 0.8996255460786353)\n",
      "(8, 0.004793267471642883, 0.1909547738693467, 0.27941176470588236, 0.1450381679389313)\n",
      "(9, 0.05124405415294548, 0.5009345794392523, 0.5988083416087389, 0.4305605141021064)\n",
      "(10, 0.028594950603732162, 0.5578861345682374, 0.4857685009487666, 0.655150351887396)\n",
      "(11, 0.00729967069154775, 0.33607907742998355, 0.49038461538461536, 0.2556390977443609)\n",
      "0.6865810046618199\n",
      "('the beginning', 110, 'feature', 0.6865810046618199, 'scored as', 0.6909995628240877)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.624082\n",
      "[20]\tvalid_0's f1_weighted: 0.675154\n",
      "[30]\tvalid_0's f1_weighted: 0.68129\n",
      "[40]\tvalid_0's f1_weighted: 0.683707\n",
      "[50]\tvalid_0's f1_weighted: 0.684905\n",
      "[60]\tvalid_0's f1_weighted: 0.685947\n",
      "[70]\tvalid_0's f1_weighted: 0.686524\n",
      "[80]\tvalid_0's f1_weighted: 0.687327\n",
      "[90]\tvalid_0's f1_weighted: 0.687745\n",
      "[100]\tvalid_0's f1_weighted: 0.68769\n",
      "[110]\tvalid_0's f1_weighted: 0.68789\n",
      "[120]\tvalid_0's f1_weighted: 0.688439\n",
      "[130]\tvalid_0's f1_weighted: 0.688529\n",
      "[140]\tvalid_0's f1_weighted: 0.688829\n",
      "[150]\tvalid_0's f1_weighted: 0.688722\n",
      "[160]\tvalid_0's f1_weighted: 0.688932\n",
      "[170]\tvalid_0's f1_weighted: 0.688913\n",
      "[180]\tvalid_0's f1_weighted: 0.689004\n",
      "[190]\tvalid_0's f1_weighted: 0.689127\n",
      "[200]\tvalid_0's f1_weighted: 0.688961\n",
      "[210]\tvalid_0's f1_weighted: 0.689206\n",
      "[220]\tvalid_0's f1_weighted: 0.689242\n",
      "[230]\tvalid_0's f1_weighted: 0.689266\n",
      "[240]\tvalid_0's f1_weighted: 0.689291\n",
      "[250]\tvalid_0's f1_weighted: 0.689105\n",
      "[260]\tvalid_0's f1_weighted: 0.689251\n",
      "[270]\tvalid_0's f1_weighted: 0.689052\n",
      "Early stopping, best iteration is:\n",
      "[227]\tvalid_0's f1_weighted: 0.689364\n",
      "(263.64778685569763, 's')\n",
      "(0, 0.08768752286864252, 0.3551957706922187, 0.8531746031746031, 0.224285416232005)\n",
      "(1, 0.14617636297109404, 0.6866904868244753, 0.6198347107438017, 0.769712140175219)\n",
      "(2, 0.31163556531284303, 0.9004642422902619, 0.8506317218335596, 0.9564987671715393)\n",
      "(3, 0.04399926820343945, 0.11743772241992882, 0.4074074074074074, 0.06860706860706861)\n",
      "(4, 0.02420417124039517, 0.020573108008817047, 0.3684210526315789, 0.010582010582010581)\n",
      "(5, 0.09851811196487377, 0.8415562206070971, 0.7798732171156894, 0.9138347260909935)\n",
      "(6, 0.01995975118916941, 0.17157825802226587, 0.30045871559633025, 0.12007332722273144)\n",
      "(7, 0.17588730332967434, 0.7864658925979681, 0.6972816470966704, 0.9018098606199293)\n",
      "(8, 0.004793267471642883, 0.19680851063829788, 0.32456140350877194, 0.14122137404580154)\n",
      "(9, 0.05124405415294548, 0.5372093023255814, 0.5875370919881305, 0.49482327740092824)\n",
      "(10, 0.028594950603732162, 0.5559586938319845, 0.49306930693069306, 0.6372360844529751)\n",
      "(11, 0.00729967069154775, 0.34596375617792424, 0.5048076923076923, 0.2631578947368421)\n",
      "0.689363692788643\n",
      "('the beginning', 120, 'feature', 0.689363692788643, 'scored as', 0.6909995628240877)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.625525\n",
      "[20]\tvalid_0's f1_weighted: 0.675396\n",
      "[30]\tvalid_0's f1_weighted: 0.682045\n",
      "[40]\tvalid_0's f1_weighted: 0.683882\n",
      "[50]\tvalid_0's f1_weighted: 0.684912\n",
      "[60]\tvalid_0's f1_weighted: 0.686107\n",
      "[70]\tvalid_0's f1_weighted: 0.686967\n",
      "[80]\tvalid_0's f1_weighted: 0.687431\n",
      "[90]\tvalid_0's f1_weighted: 0.688053\n",
      "[100]\tvalid_0's f1_weighted: 0.68881\n",
      "[110]\tvalid_0's f1_weighted: 0.689001\n",
      "[120]\tvalid_0's f1_weighted: 0.689153\n",
      "[130]\tvalid_0's f1_weighted: 0.689045\n",
      "[140]\tvalid_0's f1_weighted: 0.689289\n",
      "[150]\tvalid_0's f1_weighted: 0.689706\n",
      "[160]\tvalid_0's f1_weighted: 0.68969\n",
      "[170]\tvalid_0's f1_weighted: 0.689884\n",
      "[180]\tvalid_0's f1_weighted: 0.689907\n",
      "[190]\tvalid_0's f1_weighted: 0.689768\n",
      "[200]\tvalid_0's f1_weighted: 0.689995\n",
      "[210]\tvalid_0's f1_weighted: 0.689955\n",
      "[220]\tvalid_0's f1_weighted: 0.689725\n",
      "[230]\tvalid_0's f1_weighted: 0.689919\n",
      "[240]\tvalid_0's f1_weighted: 0.690368\n",
      "[250]\tvalid_0's f1_weighted: 0.690262\n",
      "[260]\tvalid_0's f1_weighted: 0.690293\n",
      "[270]\tvalid_0's f1_weighted: 0.690406\n",
      "[280]\tvalid_0's f1_weighted: 0.690556\n",
      "[290]\tvalid_0's f1_weighted: 0.690277\n",
      "[300]\tvalid_0's f1_weighted: 0.690378\n",
      "[310]\tvalid_0's f1_weighted: 0.690234\n",
      "[320]\tvalid_0's f1_weighted: 0.690167\n",
      "Early stopping, best iteration is:\n",
      "[272]\tvalid_0's f1_weighted: 0.690612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290.12705302238464, 's')\n",
      "(0, 0.08768752286864252, 0.3570843570843571, 0.8307926829268293, 0.22741498017942832)\n",
      "(1, 0.14617636297109404, 0.687279843444227, 0.6211217786760991, 0.7692115143929913)\n",
      "(2, 0.31163556531284303, 0.9013446723914184, 0.8508575301047803, 0.9582012445696841)\n",
      "(3, 0.04399926820343945, 0.12375533428165007, 0.4275184275184275, 0.07234927234927235)\n",
      "(4, 0.02420417124039517, 0.029368575624082235, 0.5128205128205128, 0.015117157974300832)\n",
      "(5, 0.09851811196487377, 0.842114280818984, 0.781647582697201, 0.9127205199628597)\n",
      "(6, 0.01995975118916941, 0.17282321899736144, 0.30823529411764705, 0.12007332722273144)\n",
      "(7, 0.17588730332967434, 0.7870340945203614, 0.6983001691774753, 0.9016018306636155)\n",
      "(8, 0.004793267471642883, 0.19791666666666666, 0.3114754098360656, 0.1450381679389313)\n",
      "(9, 0.05124405415294548, 0.5344192359899166, 0.5848896434634975, 0.4919671545876473)\n",
      "(10, 0.028594950603732162, 0.5594170403587443, 0.4977556109725686, 0.6385156749840051)\n",
      "(11, 0.00729967069154775, 0.3583061889250814, 0.5116279069767442, 0.2756892230576441)\n",
      "0.6906117708677186\n",
      "('the beginning', 130, 'feature', 0.6906117708677186, 'scored as', 0.6909995628240877)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.626339\n",
      "[20]\tvalid_0's f1_weighted: 0.676212\n",
      "[30]\tvalid_0's f1_weighted: 0.683393\n",
      "[40]\tvalid_0's f1_weighted: 0.685377\n",
      "[50]\tvalid_0's f1_weighted: 0.68647\n",
      "[60]\tvalid_0's f1_weighted: 0.687593\n",
      "[70]\tvalid_0's f1_weighted: 0.687665\n",
      "[80]\tvalid_0's f1_weighted: 0.688243\n",
      "[90]\tvalid_0's f1_weighted: 0.688484\n",
      "[100]\tvalid_0's f1_weighted: 0.688786\n",
      "[110]\tvalid_0's f1_weighted: 0.689218\n",
      "[120]\tvalid_0's f1_weighted: 0.689551\n",
      "[130]\tvalid_0's f1_weighted: 0.689598\n",
      "[140]\tvalid_0's f1_weighted: 0.689629\n",
      "[150]\tvalid_0's f1_weighted: 0.689934\n",
      "[160]\tvalid_0's f1_weighted: 0.689728\n",
      "[170]\tvalid_0's f1_weighted: 0.689839\n",
      "[180]\tvalid_0's f1_weighted: 0.689894\n",
      "[190]\tvalid_0's f1_weighted: 0.690018\n",
      "[200]\tvalid_0's f1_weighted: 0.689903\n",
      "[210]\tvalid_0's f1_weighted: 0.690276\n",
      "[220]\tvalid_0's f1_weighted: 0.69041\n",
      "[230]\tvalid_0's f1_weighted: 0.690586\n",
      "[240]\tvalid_0's f1_weighted: 0.690558\n",
      "[250]\tvalid_0's f1_weighted: 0.690562\n",
      "[260]\tvalid_0's f1_weighted: 0.690702\n",
      "[270]\tvalid_0's f1_weighted: 0.690537\n",
      "[280]\tvalid_0's f1_weighted: 0.690585\n",
      "[290]\tvalid_0's f1_weighted: 0.690386\n",
      "[300]\tvalid_0's f1_weighted: 0.690326\n",
      "Early stopping, best iteration is:\n",
      "[251]\tvalid_0's f1_weighted: 0.690775\n",
      "(266.0811960697174, 's')\n",
      "(0, 0.08768752286864252, 0.35536508983022913, 0.8461538461538461, 0.22491132902148966)\n",
      "(1, 0.14617636297109404, 0.6869768998995648, 0.6198147402335884, 0.7704630788485607)\n",
      "(2, 0.31163556531284303, 0.9007709524993783, 0.8509005481597494, 0.9568510038746038)\n",
      "(3, 0.04399926820343945, 0.12513255567338283, 0.41745283018867924, 0.0735966735966736)\n",
      "(4, 0.02420417124039517, 0.024799416484318017, 0.3541666666666667, 0.012849584278155708)\n",
      "(5, 0.09851811196487377, 0.8404782237403928, 0.7780237154150198, 0.9138347260909935)\n",
      "(6, 0.01995975118916941, 0.1582537517053206, 0.30933333333333335, 0.10632447296058661)\n",
      "(7, 0.17588730332967434, 0.7878124429848568, 0.7015434606011373, 0.8982733513625962)\n",
      "(8, 0.004793267471642883, 0.2367758186397985, 0.34814814814814815, 0.17938931297709923)\n",
      "(9, 0.05124405415294548, 0.5359782185919876, 0.5886373344724477, 0.4919671545876473)\n",
      "(10, 0.028594950603732162, 0.5624303232998884, 0.4982716049382716, 0.6455534229046705)\n",
      "(11, 0.00729967069154775, 0.43356643356643354, 0.49050632911392406, 0.38847117794486213)\n",
      "0.6907745498886008\n",
      "('the beginning', 140, 'feature', 0.6907745498886008, 'scored as', 0.6909995628240877)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.626749\n",
      "[20]\tvalid_0's f1_weighted: 0.676949\n",
      "[30]\tvalid_0's f1_weighted: 0.683539\n",
      "[40]\tvalid_0's f1_weighted: 0.686418\n",
      "[50]\tvalid_0's f1_weighted: 0.687367\n",
      "[60]\tvalid_0's f1_weighted: 0.688291\n",
      "[70]\tvalid_0's f1_weighted: 0.689219\n",
      "[80]\tvalid_0's f1_weighted: 0.689478\n",
      "[90]\tvalid_0's f1_weighted: 0.689945\n",
      "[100]\tvalid_0's f1_weighted: 0.690143\n",
      "[110]\tvalid_0's f1_weighted: 0.690472\n",
      "[120]\tvalid_0's f1_weighted: 0.690523\n",
      "[130]\tvalid_0's f1_weighted: 0.690731\n",
      "[140]\tvalid_0's f1_weighted: 0.690703\n",
      "[150]\tvalid_0's f1_weighted: 0.691132\n",
      "[160]\tvalid_0's f1_weighted: 0.691044\n",
      "[170]\tvalid_0's f1_weighted: 0.690865\n",
      "[180]\tvalid_0's f1_weighted: 0.690889\n",
      "[190]\tvalid_0's f1_weighted: 0.691303\n",
      "[200]\tvalid_0's f1_weighted: 0.690979\n",
      "[210]\tvalid_0's f1_weighted: 0.690985\n",
      "[220]\tvalid_0's f1_weighted: 0.690881\n",
      "[230]\tvalid_0's f1_weighted: 0.691094\n",
      "[240]\tvalid_0's f1_weighted: 0.691115\n",
      "Early stopping, best iteration is:\n",
      "[193]\tvalid_0's f1_weighted: 0.69139\n",
      "(253.21697998046875, 's')\n",
      "(0, 0.08768752286864252, 0.3539325842696629, 0.8506751389992058, 0.22345086584602544)\n",
      "(1, 0.14617636297109404, 0.6865256124721604, 0.6183550651955868, 0.7715894868585732)\n",
      "(2, 0.31163556531284303, 0.9015922158337019, 0.8519485947131961, 0.9573793589292005)\n",
      "(3, 0.04399926820343945, 0.11621233859397417, 0.42297650130548303, 0.06735966735966736)\n",
      "(4, 0.02420417124039517, 0.01896425966447848, 0.2708333333333333, 0.009826152683295541)\n",
      "(5, 0.09851811196487377, 0.8423388817755015, 0.7794628751974724, 0.9162488393686166)\n",
      "(6, 0.01995975118916941, 0.18303273213092852, 0.3374384236453202, 0.12557286892758937)\n",
      "(7, 0.17588730332967434, 0.7889254385964912, 0.7034381619683885, 0.8980653214062825)\n",
      "(8, 0.004793267471642883, 0.24213075060532693, 0.33112582781456956, 0.19083969465648856)\n",
      "(9, 0.05124405415294548, 0.5382957884427032, 0.5963541666666666, 0.4905390931810068)\n",
      "(10, 0.028594950603732162, 0.564031947122005, 0.4951644100580271, 0.655150351887396)\n",
      "(11, 0.00729967069154775, 0.43646408839779005, 0.48615384615384616, 0.39598997493734334)\n",
      "0.6913902183482357\n",
      "('the beginning', 150, 'feature', 0.6913902183482357, 'scored as', 0.6913902183482357)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.626181\n",
      "[20]\tvalid_0's f1_weighted: 0.676588\n",
      "[30]\tvalid_0's f1_weighted: 0.684147\n",
      "[40]\tvalid_0's f1_weighted: 0.686193\n",
      "[50]\tvalid_0's f1_weighted: 0.687343\n",
      "[60]\tvalid_0's f1_weighted: 0.68812\n",
      "[70]\tvalid_0's f1_weighted: 0.688721\n",
      "[80]\tvalid_0's f1_weighted: 0.689149\n",
      "[90]\tvalid_0's f1_weighted: 0.689413\n",
      "[100]\tvalid_0's f1_weighted: 0.68992\n",
      "[110]\tvalid_0's f1_weighted: 0.689989\n",
      "[120]\tvalid_0's f1_weighted: 0.690203\n",
      "[130]\tvalid_0's f1_weighted: 0.690433\n",
      "[140]\tvalid_0's f1_weighted: 0.690491\n",
      "[150]\tvalid_0's f1_weighted: 0.69079\n",
      "[160]\tvalid_0's f1_weighted: 0.69062\n",
      "[170]\tvalid_0's f1_weighted: 0.69064\n",
      "[180]\tvalid_0's f1_weighted: 0.690762\n",
      "[190]\tvalid_0's f1_weighted: 0.690649\n",
      "Early stopping, best iteration is:\n",
      "[149]\tvalid_0's f1_weighted: 0.690882\n",
      "(205.15254497528076, 's')\n",
      "(0, 0.08768752286864252, 0.3515392254220457, 0.8502802241793435, 0.22157312747757146)\n",
      "(1, 0.14617636297109404, 0.687506940588562, 0.6178642714570858, 0.7748435544430539)\n",
      "(2, 0.31163556531284303, 0.9016937894387246, 0.8529688972667295, 0.9563226488200071)\n",
      "(3, 0.04399926820343945, 0.11967036904335364, 0.4326424870466321, 0.06943866943866944)\n",
      "(4, 0.02420417124039517, 0.01624815361890694, 0.3548387096774194, 0.008314436885865457)\n",
      "(5, 0.09851811196487377, 0.8422942131806137, 0.7803294266708901, 0.9149489322191272)\n",
      "(6, 0.01995975118916941, 0.16960651289009498, 0.3263707571801567, 0.1145737855178735)\n",
      "(7, 0.17588730332967434, 0.7871592310151148, 0.7015873015873015, 0.8965050967339296)\n",
      "(8, 0.004793267471642883, 0.2222222222222222, 0.3026315789473684, 0.17557251908396945)\n",
      "(9, 0.05124405415294548, 0.5396949550254203, 0.5966277561608301, 0.4926811852909675)\n",
      "(10, 0.028594950603732162, 0.5639344262295082, 0.49213161659513593, 0.6602687140115163)\n",
      "(11, 0.00729967069154775, 0.4432284541723666, 0.4879518072289157, 0.40601503759398494)\n",
      "0.6908816842300901\n",
      "('the beginning', 160, 'feature', 0.6908816842300901, 'scored as', 0.6913902183482357)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.625747\n",
      "[20]\tvalid_0's f1_weighted: 0.678052\n",
      "[30]\tvalid_0's f1_weighted: 0.684844\n",
      "[40]\tvalid_0's f1_weighted: 0.68632\n",
      "[50]\tvalid_0's f1_weighted: 0.687372\n",
      "[60]\tvalid_0's f1_weighted: 0.688191\n",
      "[70]\tvalid_0's f1_weighted: 0.688735\n",
      "[80]\tvalid_0's f1_weighted: 0.689129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90]\tvalid_0's f1_weighted: 0.68925\n",
      "[100]\tvalid_0's f1_weighted: 0.689344\n",
      "[110]\tvalid_0's f1_weighted: 0.689771\n",
      "[120]\tvalid_0's f1_weighted: 0.689916\n",
      "[130]\tvalid_0's f1_weighted: 0.689839\n",
      "[140]\tvalid_0's f1_weighted: 0.690226\n",
      "[150]\tvalid_0's f1_weighted: 0.690108\n",
      "[160]\tvalid_0's f1_weighted: 0.690512\n",
      "[170]\tvalid_0's f1_weighted: 0.690594\n",
      "[180]\tvalid_0's f1_weighted: 0.69063\n",
      "[190]\tvalid_0's f1_weighted: 0.690432\n",
      "[200]\tvalid_0's f1_weighted: 0.690938\n",
      "[210]\tvalid_0's f1_weighted: 0.691167\n",
      "[220]\tvalid_0's f1_weighted: 0.691117\n",
      "[230]\tvalid_0's f1_weighted: 0.690944\n",
      "[240]\tvalid_0's f1_weighted: 0.69088\n",
      "[250]\tvalid_0's f1_weighted: 0.69074\n",
      "Early stopping, best iteration is:\n",
      "[209]\tvalid_0's f1_weighted: 0.691177\n",
      "(260.8494760990143, 's')\n",
      "(0, 0.08768752286864252, 0.3539852095316351, 0.8335913312693498, 0.22470269142499477)\n",
      "(1, 0.14617636297109404, 0.6882993577213069, 0.621482602118003, 0.7712140175219023)\n",
      "(2, 0.31163556531284303, 0.9012465105994859, 0.8515172089622395, 0.9571445344604907)\n",
      "(3, 0.04399926820343945, 0.11115069469184183, 0.3880597014925373, 0.06486486486486487)\n",
      "(4, 0.02420417124039517, 0.020633750921149593, 0.4117647058823529, 0.010582010582010581)\n",
      "(5, 0.09851811196487377, 0.8410256410256411, 0.7790973871733967, 0.9136490250696379)\n",
      "(6, 0.01995975118916941, 0.15951742627345844, 0.2967581047381546, 0.10907424381301559)\n",
      "(7, 0.17588730332967434, 0.7885036496350365, 0.7022590606208353, 0.8988974412315374)\n",
      "(8, 0.004793267471642883, 0.2745098039215686, 0.3835616438356164, 0.21374045801526717)\n",
      "(9, 0.05124405415294548, 0.542180465844588, 0.6000866551126517, 0.49446626204926813)\n",
      "(10, 0.028594950603732162, 0.565468020861927, 0.4951923076923077, 0.6589891234804862)\n",
      "(11, 0.00729967069154775, 0.4488078541374474, 0.5095541401273885, 0.40100250626566414)\n",
      "0.6911765915235532\n",
      "('the beginning', 170, 'feature', 0.6911765915235532, 'scored as', 0.6913902183482357)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.626848\n",
      "[20]\tvalid_0's f1_weighted: 0.67684\n",
      "[30]\tvalid_0's f1_weighted: 0.684442\n",
      "[40]\tvalid_0's f1_weighted: 0.686736\n",
      "[50]\tvalid_0's f1_weighted: 0.687733\n",
      "[60]\tvalid_0's f1_weighted: 0.688681\n",
      "[70]\tvalid_0's f1_weighted: 0.689011\n",
      "[80]\tvalid_0's f1_weighted: 0.689371\n",
      "[90]\tvalid_0's f1_weighted: 0.689601\n",
      "[100]\tvalid_0's f1_weighted: 0.689672\n",
      "[110]\tvalid_0's f1_weighted: 0.689884\n",
      "[120]\tvalid_0's f1_weighted: 0.690002\n",
      "[130]\tvalid_0's f1_weighted: 0.689868\n",
      "[140]\tvalid_0's f1_weighted: 0.689995\n",
      "[150]\tvalid_0's f1_weighted: 0.689993\n",
      "[160]\tvalid_0's f1_weighted: 0.690107\n",
      "[170]\tvalid_0's f1_weighted: 0.690015\n",
      "[180]\tvalid_0's f1_weighted: 0.690036\n",
      "[190]\tvalid_0's f1_weighted: 0.689912\n",
      "[200]\tvalid_0's f1_weighted: 0.690172\n",
      "[210]\tvalid_0's f1_weighted: 0.690226\n",
      "[220]\tvalid_0's f1_weighted: 0.69048\n",
      "[230]\tvalid_0's f1_weighted: 0.690423\n",
      "[240]\tvalid_0's f1_weighted: 0.690311\n",
      "[250]\tvalid_0's f1_weighted: 0.690361\n",
      "[260]\tvalid_0's f1_weighted: 0.69039\n",
      "[270]\tvalid_0's f1_weighted: 0.690483\n",
      "[280]\tvalid_0's f1_weighted: 0.690372\n",
      "[290]\tvalid_0's f1_weighted: 0.690442\n",
      "[300]\tvalid_0's f1_weighted: 0.690426\n",
      "[310]\tvalid_0's f1_weighted: 0.690218\n",
      "[320]\tvalid_0's f1_weighted: 0.690239\n",
      "Early stopping, best iteration is:\n",
      "[272]\tvalid_0's f1_weighted: 0.690681\n",
      "(339.9601969718933, 's')\n",
      "(0, 0.08768752286864252, 0.3527093596059113, 0.8280647648419429, 0.22407677863551012)\n",
      "(1, 0.14617636297109404, 0.6850600390952248, 0.6185577407967726, 0.7675844806007509)\n",
      "(2, 0.31163556531284303, 0.9012294515817101, 0.8512081832889724, 0.9574967711635552)\n",
      "(3, 0.04399926820343945, 0.1160555357778569, 0.4034653465346535, 0.06777546777546778)\n",
      "(4, 0.02420417124039517, 0.017699115044247787, 0.36363636363636365, 0.009070294784580499)\n",
      "(5, 0.09851811196487377, 0.8414748909230901, 0.7801395939086294, 0.9132776230269266)\n",
      "(6, 0.01995975118916941, 0.16933333333333334, 0.3105134474327628, 0.11640696608615948)\n",
      "(7, 0.17588730332967434, 0.788514129443938, 0.7017686191789713, 0.8997295610567921)\n",
      "(8, 0.004793267471642883, 0.2591687041564792, 0.36054421768707484, 0.20229007633587787)\n",
      "(9, 0.05124405415294548, 0.5410985586287496, 0.5953707672524646, 0.4958943234559086)\n",
      "(10, 0.028594950603732162, 0.5602460162147052, 0.49751737835153925, 0.6410748560460653)\n",
      "(11, 0.00729967069154775, 0.44692737430167595, 0.5047318611987381, 0.40100250626566414)\n",
      "0.6906806668059589\n",
      "('the beginning', 180, 'feature', 0.6906806668059589, 'scored as', 0.6913902183482357)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.625885\n",
      "[20]\tvalid_0's f1_weighted: 0.676953\n",
      "[30]\tvalid_0's f1_weighted: 0.683944\n",
      "[40]\tvalid_0's f1_weighted: 0.686317\n",
      "[50]\tvalid_0's f1_weighted: 0.686894\n",
      "[60]\tvalid_0's f1_weighted: 0.687433\n",
      "[70]\tvalid_0's f1_weighted: 0.688075\n",
      "[80]\tvalid_0's f1_weighted: 0.688987\n",
      "[90]\tvalid_0's f1_weighted: 0.689553\n",
      "[100]\tvalid_0's f1_weighted: 0.689906\n",
      "[110]\tvalid_0's f1_weighted: 0.690121\n",
      "[120]\tvalid_0's f1_weighted: 0.690265\n",
      "[130]\tvalid_0's f1_weighted: 0.690568\n",
      "[140]\tvalid_0's f1_weighted: 0.690845\n",
      "[150]\tvalid_0's f1_weighted: 0.691073\n",
      "[160]\tvalid_0's f1_weighted: 0.690688\n",
      "[170]\tvalid_0's f1_weighted: 0.690955\n",
      "[180]\tvalid_0's f1_weighted: 0.690482\n",
      "[190]\tvalid_0's f1_weighted: 0.690203\n",
      "[200]\tvalid_0's f1_weighted: 0.690464\n",
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's f1_weighted: 0.691073\n",
      "(223.06142401695251, 's')\n",
      "(0, 0.08768752286864252, 0.3517389154442064, 0.8375196232339089, 0.2226163154600459)\n",
      "(1, 0.14617636297109404, 0.6884589726484323, 0.6193238647729545, 0.7749687108886107)\n",
      "(2, 0.31163556531284303, 0.9014778325123153, 0.8527225130890053, 0.9561465304684749)\n",
      "(3, 0.04399926820343945, 0.11244105912223432, 0.4403409090909091, 0.06444906444906445)\n",
      "(4, 0.02420417124039517, 0.022107590272660283, 0.4411764705882353, 0.011337868480725623)\n",
      "(5, 0.09851811196487377, 0.8406044565866986, 0.7779709228824273, 0.9142061281337047)\n",
      "(6, 0.01995975118916941, 0.16205266711681296, 0.3076923076923077, 0.10999083409715857)\n",
      "(7, 0.17588730332967434, 0.7880864282125076, 0.7026146452716462, 0.8972332015810277)\n",
      "(8, 0.004793267471642883, 0.2764976958525346, 0.3488372093023256, 0.22900763358778625)\n",
      "(9, 0.05124405415294548, 0.5423197492163009, 0.6009552757273122, 0.494109246697608)\n",
      "(10, 0.028594950603732162, 0.5648729855230811, 0.49285033365109626, 0.6615483045425464)\n",
      "(11, 0.00729967069154775, 0.4447476125511597, 0.4880239520958084, 0.40852130325814534)\n",
      "0.6910732248142538\n",
      "('the beginning', 190, 'feature', 0.6910732248142538, 'scored as', 0.6913902183482357)\n",
      "Runing...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.626099\n",
      "[20]\tvalid_0's f1_weighted: 0.676574\n",
      "[30]\tvalid_0's f1_weighted: 0.684725\n",
      "[40]\tvalid_0's f1_weighted: 0.686543\n",
      "[50]\tvalid_0's f1_weighted: 0.687583\n",
      "[60]\tvalid_0's f1_weighted: 0.688465\n",
      "[70]\tvalid_0's f1_weighted: 0.689088\n",
      "[80]\tvalid_0's f1_weighted: 0.689324\n",
      "[90]\tvalid_0's f1_weighted: 0.689782\n",
      "[100]\tvalid_0's f1_weighted: 0.689904\n",
      "[110]\tvalid_0's f1_weighted: 0.689981\n",
      "[120]\tvalid_0's f1_weighted: 0.690279\n",
      "[130]\tvalid_0's f1_weighted: 0.69035\n",
      "[140]\tvalid_0's f1_weighted: 0.690288\n",
      "[150]\tvalid_0's f1_weighted: 0.690619\n",
      "[160]\tvalid_0's f1_weighted: 0.690572\n",
      "[170]\tvalid_0's f1_weighted: 0.690667\n",
      "[180]\tvalid_0's f1_weighted: 0.690787\n",
      "[190]\tvalid_0's f1_weighted: 0.690759\n",
      "[200]\tvalid_0's f1_weighted: 0.690855\n",
      "[210]\tvalid_0's f1_weighted: 0.690718\n",
      "[220]\tvalid_0's f1_weighted: 0.690765\n",
      "[230]\tvalid_0's f1_weighted: 0.690586\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid_0's f1_weighted: 0.690976\n",
      "(262.4339189529419, 's')\n",
      "(0, 0.08768752286864252, 0.3504301786896096, 0.8465227817745803, 0.22094721468808678)\n",
      "(1, 0.14617636297109404, 0.6883819510290591, 0.6208874132206459, 0.7723404255319148)\n",
      "(2, 0.31163556531284303, 0.9013079667063021, 0.8519525328035966, 0.956733591640249)\n",
      "(3, 0.04399926820343945, 0.11222301644031452, 0.3994910941475827, 0.06528066528066528)\n",
      "(4, 0.02420417124039517, 0.017543859649122806, 0.26666666666666666, 0.009070294784580499)\n",
      "(5, 0.09851811196487377, 0.8420603057999487, 0.7796583359696299, 0.9153203342618385)\n",
      "(6, 0.01995975118916941, 0.17192746809939558, 0.32160804020100503, 0.11732355637030248)\n",
      "(7, 0.17588730332967434, 0.787895385458031, 0.7019926799511996, 0.8977532764718119)\n",
      "(8, 0.004793267471642883, 0.24519230769230768, 0.33116883116883117, 0.1946564885496183)\n",
      "(9, 0.05124405415294548, 0.5418622848200312, 0.5993076590220684, 0.49446626204926813)\n",
      "(10, 0.028594950603732162, 0.5651697699890472, 0.4940162757300144, 0.6602687140115163)\n",
      "(11, 0.00729967069154775, 0.4529331514324693, 0.49700598802395207, 0.41604010025062654)\n",
      "0.690975879865437\n",
      "('the beginning', 200, 'feature', 0.690975879865437, 'scored as', 0.6913902183482357)\n",
      "Runing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.624482\n",
      "[20]\tvalid_0's f1_weighted: 0.676638\n",
      "[30]\tvalid_0's f1_weighted: 0.683907\n",
      "[40]\tvalid_0's f1_weighted: 0.686588\n",
      "[50]\tvalid_0's f1_weighted: 0.687513\n",
      "[60]\tvalid_0's f1_weighted: 0.687856\n",
      "[70]\tvalid_0's f1_weighted: 0.688956\n",
      "[80]\tvalid_0's f1_weighted: 0.689414\n",
      "[90]\tvalid_0's f1_weighted: 0.689611\n",
      "[100]\tvalid_0's f1_weighted: 0.690046\n",
      "[110]\tvalid_0's f1_weighted: 0.6906\n",
      "[120]\tvalid_0's f1_weighted: 0.690709\n",
      "[130]\tvalid_0's f1_weighted: 0.690851\n",
      "[140]\tvalid_0's f1_weighted: 0.691136\n",
      "[150]\tvalid_0's f1_weighted: 0.690946\n",
      "[160]\tvalid_0's f1_weighted: 0.690889\n",
      "[170]\tvalid_0's f1_weighted: 0.690948\n",
      "[180]\tvalid_0's f1_weighted: 0.691194\n",
      "[190]\tvalid_0's f1_weighted: 0.691346\n",
      "[200]\tvalid_0's f1_weighted: 0.691311\n",
      "[210]\tvalid_0's f1_weighted: 0.691255\n",
      "[220]\tvalid_0's f1_weighted: 0.691292\n",
      "[230]\tvalid_0's f1_weighted: 0.691343\n",
      "[240]\tvalid_0's f1_weighted: 0.691522\n",
      "Early stopping, best iteration is:\n",
      "[194]\tvalid_0's f1_weighted: 0.691556\n",
      "(279.7736530303955, 's')\n",
      "(0, 0.08768752286864252, 0.3534668211153401, 0.8544, 0.2228249530565408)\n",
      "(1, 0.14617636297109404, 0.688825789209955, 0.6203991575569151, 0.7742177722152691)\n",
      "(2, 0.31163556531284303, 0.901244124965441, 0.8517454013377926, 0.9568510038746038)\n",
      "(3, 0.04399926820343945, 0.12084376117268501, 0.43112244897959184, 0.07027027027027027)\n",
      "(4, 0.02420417124039517, 0.02498163115356356, 0.4473684210526316, 0.012849584278155708)\n",
      "(5, 0.09851811196487377, 0.8413334470116804, 0.7777427490542245, 0.9162488393686166)\n",
      "(6, 0.01995975118916941, 0.16928327645051194, 0.3315508021390374, 0.11365719523373052)\n",
      "(7, 0.17588730332967434, 0.7877653503766264, 0.701977056382719, 0.8974412315373413)\n",
      "(8, 0.004793267471642883, 0.2255639097744361, 0.3284671532846715, 0.1717557251908397)\n",
      "(9, 0.05124405415294548, 0.5412270418132082, 0.59775571860164, 0.49446626204926813)\n",
      "(10, 0.028594950603732162, 0.5637362637362637, 0.4939817043813192, 0.6564299424184261)\n",
      "(11, 0.00729967069154775, 0.4562841530054645, 0.5015015015015015, 0.41854636591478694)\n",
      "0.6915560462628269\n",
      "('the beginning', 210, 'feature', 0.6915560462628269, 'scored as', 0.6915560462628269)\n",
      "('\\xe7\\xad\\x9b\\xe9\\x80\\x89\\xe5\\x87\\xba\\xe6\\x9d\\xa5\\xe6\\x9c\\x80\\xe4\\xbd\\xb3\\xe7\\x89\\xb9\\xe5\\xbe\\x81\\xe4\\xb8\\xaa\\xe6\\x95\\xb0\\xe4\\xb8\\xba', 210, '\\xe8\\xbf\\x99\\xe4\\xb8\\x8b\\xe5\\xad\\x90\\xe8\\xae\\xad\\xe7\\xbb\\x83\\xe9\\x80\\x9f\\xe5\\xba\\xa6\\xe7\\xbb\\x88\\xe4\\xba\\x8e\\xe5\\x8f\\xaf\\xe4\\xbb\\xa5\\xe5\\xa4\\xa7\\xe5\\xa4\\xa7\\xe6\\x8f\\x90\\xe5\\x8d\\x87\\xe4\\xba\\x86')\n"
     ]
    }
   ],
   "source": [
    "col = select(train_x, train_y)#挑选特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     34055\n",
      "7     22518\n",
      "1     17860\n",
      "5     10880\n",
      "10     3419\n",
      "0      2158\n",
      "9      1832\n",
      "3       558\n",
      "6       389\n",
      "11      351\n",
      "8       255\n",
      "4        83\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "res = pd.value_counts(result_lgb)\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels, preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "   \n",
    "def train_lgb_time_serie(train_x, train_y, test_x): \n",
    "    \n",
    "    all_train_x = train_x\n",
    "    all_train_y = train_y\n",
    "\n",
    "    train_index = (train_x['day_time'] <= '2018-11-23')\n",
    "    valid_index = (train_x['day_time'] > '2018-11-23') & (train_x['day_time'] < '2018-12-01')\n",
    "    train_x = train_x.drop(['day_time'], axis = 1)\n",
    "    test_x = test_x.drop(['day_time'], axis = 1)\n",
    "    all_train_x = all_train_x.drop(['day_time'], axis = 1)\n",
    "    \n",
    "    \n",
    "    tr_x     = train_x[train_index]\n",
    "    tr_y     = train_y[train_index]\n",
    "    val_x     = train_x[valid_index]\n",
    "    val_y     = train_y[valid_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    cate_cols = ['pred_0','pred_1','cpred_0']\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        num_leaves=41,#41 \n",
    "        reg_alpha=0, \n",
    "        reg_lambda=0.01,\n",
    "        max_depth=-1, \n",
    "        n_estimators=100, \n",
    "        objective='multiclass',\n",
    "        subsample=0.8, #6\n",
    "        colsample_bytree=0.8, \n",
    "        subsample_freq=1,\n",
    "        min_child_samples = 50,  \n",
    "        learning_rate=0.2, \n",
    "        random_state=2019, \n",
    "        metric=\"None\",\n",
    "        n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    eval_set = [(val_x, val_y)]\n",
    "    lgb_model.fit(\n",
    "        tr_x, tr_y, eval_set=eval_set,\n",
    "        eval_metric=f1_weighted,\n",
    "        categorical_feature=cate_cols,\n",
    "        verbose=10, early_stopping_rounds=50)\n",
    "    \n",
    "    \n",
    "    pred = lgb_model.predict(val_x) \n",
    "    df_analysis = pd.DataFrame()\n",
    "    #df_analysis['sid']   = val_x['sid']\n",
    "    df_analysis['label'] = val_y\n",
    "    df_analysis['pred']  = pred\n",
    "    df_analysis['label'] = df_analysis['label'].astype(int)\n",
    "    dic_ = df_analysis['label'].value_counts(normalize = True)\n",
    "    \n",
    "    def get_weighted_fscore(y_pred, y_true):\n",
    "        f_score = 0\n",
    "        for i in range(12):\n",
    "            yt = y_true == i\n",
    "            yp = y_pred == i\n",
    "            f_score += dic_[i] * f1_score(y_true=yt, y_pred= yp)\n",
    "            print(i,dic_[i],f1_score(y_true=yt, y_pred= yp), precision_score(y_true=yt, y_pred= yp),recall_score(y_true=yt, y_pred= yp))\n",
    "        print(f_score)\n",
    "        return f_score\n",
    "    score = get_weighted_fscore(y_true =df_analysis['label'] , y_pred = df_analysis['pred'])\n",
    "    \n",
    "    imp = pd.DataFrame()\n",
    "    imp['fea'] = list(val_x.columns)\n",
    "    imp['imp'] = lgb_model.feature_importances_ \n",
    "    imp = imp.sort_values('imp',ascending = False)\n",
    "    \n",
    "    print(lgb_model.best_iteration_)\n",
    "    lgb_model.n_estimators = lgb_model.best_iteration_\n",
    "    #lgb_model.fit(all_train_x, all_train_y, categorical_feature=cate_cols)\n",
    "    print('fit over')\n",
    "    pred_test = lgb_model.predict(test_x)\n",
    "    return pred_test, score, imp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ",'max_OHP','max_OHP_m','second_OHP','max_DHP','max_DHP_m','second_DHP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                fea       val\n",
      "1        click_mode  1.000000\n",
      "172            fm_O  0.640902\n",
      "95       first_mode  0.640898\n",
      "52       mode_7_dis  0.407287\n",
      "53       mode_7_eta  0.406561\n",
      "54           rank_7  0.406467\n",
      "51        mode_7_pr  0.406464\n",
      "89    max_dist_mode  0.312223\n",
      "62       mode_9_dis  0.278280\n",
      "63       mode_9_eta  0.277675\n",
      "61        mode_9_pr  0.277606\n",
      "64           rank_9  0.277594\n",
      "26        mode_2_pr  0.243237\n",
      "29           rank_2  0.243235\n",
      "28       mode_2_eta  0.243084\n",
      "27       mode_2_dis  0.242443\n",
      "65            t*d_9  0.228915\n",
      "55            t*d_7  0.228136\n",
      "67      mode_10_dis  0.219395\n",
      "68      mode_10_eta  0.219091\n",
      "66       mode_10_pr  0.219054\n",
      "69          rank_10  0.219049\n",
      "86         mean_eta  0.212682\n",
      "85          min_eta  0.199715\n",
      "84          max_eta  0.197451\n",
      "70           t*d_10  0.188711\n",
      "76         max_dist  0.185831\n",
      "78        mean_dist  0.181788\n",
      "77         min_dist  0.175519\n",
      "80        max_price  0.173600\n",
      "4         time_diff  0.172313\n",
      "88      mode_length  0.161333\n",
      "45            t*d_5  0.153418\n",
      "82       mean_price  0.151261\n",
      "83        std_price  0.147644\n",
      "174      min_etam_O  0.138065\n",
      "94     min_eta_mode  0.138015\n",
      "176       min_dis_D  0.136378\n",
      "173      min_dism_O  0.136349\n",
      "90    min_dist_mode  0.136298\n",
      "177         dis/eta  0.135375\n",
      "40            t*d_4  0.130897\n",
      "47       mode_6_dis  0.129880\n",
      "48       mode_6_eta  0.129839\n",
      "49           rank_6  0.129821\n",
      "46        mode_6_pr  0.129820\n",
      "35            t*d_3  0.126768\n",
      "72      mode_11_dis  0.123520\n",
      "73      mode_11_eta  0.122862\n",
      "42       mode_5_dis  0.122805\n",
      "71       mode_11_pr  0.122784\n",
      "43       mode_5_eta  0.122784\n",
      "74          rank_11  0.122776\n",
      "44           rank_5  0.122599\n",
      "41        mode_5_pr  0.122595\n",
      "32       mode_3_dis  0.120937\n",
      "91   max_price_mode  0.117320\n",
      "87          std_eta  0.115315\n",
      "33       mode_3_eta  0.111186\n",
      "31        mode_3_pr  0.110102\n",
      "34           rank_3  0.110101\n",
      "79         std_dist  0.106629\n",
      "75           t*d_11  0.102851\n",
      "50            t*d_6  0.101235\n",
      "30            t*d_2  0.098586\n",
      "92   min_price_mode  0.093862\n",
      "93     max_eta_mode  0.086022\n",
      "98           max_OP  0.084568\n",
      "101          max_DP  0.082348\n",
      "100       second_OP  0.077288\n",
      "15         line_dis  0.072854\n",
      "24           rank_1  0.068709\n",
      "21        mode_1_pr  0.068666\n",
      "23       mode_1_eta  0.068111\n",
      "22       mode_1_dis  0.066277\n",
      "103       second_DP  0.057371\n",
      "104              p0  0.045467\n",
      "25            t*d_1  0.043799\n",
      "57       mode_8_dis  0.041015\n",
      "58       mode_8_eta  0.040927\n",
      "56        mode_8_pr  0.040927\n",
      "59           rank_8  0.040911\n",
      "37       mode_4_dis  0.040578\n",
      "96        last_mode  0.035709\n",
      "36        mode_4_pr  0.035572\n",
      "137             p33  0.034918\n",
      "38       mode_4_eta  0.034048\n",
      "102        max_DP_m  0.033762\n",
      "114             p10  0.033666\n",
      "60            t*d_8  0.033446\n",
      "39           rank_4  0.033327\n",
      "135             p31  0.032095\n",
      "113              p9  0.031881\n",
      "134             p30  0.031724\n",
      "106              p2  0.028059\n",
      "175      max_OP_m*O  0.027348\n",
      "99         max_OP_m  0.027310\n",
      "136             p32  0.026288\n",
      "126             p22  0.024475\n",
      "167             p63  0.020718\n",
      "115             p11  0.020654\n",
      "123             p19  0.018993\n",
      "151             p47  0.018371\n",
      "107              p3  0.018219\n",
      "147             p43  0.015765\n",
      "81        min_price  0.015181\n",
      "121             p17  0.014809\n",
      "155             p51  0.014339\n",
      "158             p54  0.014087\n",
      "143             p39  0.013674\n",
      "152             p48  0.013352\n",
      "132             p28  0.013287\n",
      "168             p64  0.011922\n",
      "110              p6  0.011453\n",
      "165             p61  0.011147\n",
      "108              p4  0.011104\n",
      "122             p18  0.010736\n",
      "160             p56  0.010661\n",
      "118             p14  0.010566\n",
      "7                o2  0.010156\n",
      "138             p34  0.009955\n",
      "163             p59  0.009947\n",
      "156             p52  0.009364\n",
      "119             p15  0.009327\n",
      "117             p13  0.009232\n",
      "140             p36  0.009117\n",
      "130             p26  0.008889\n",
      "139             p35  0.008733\n",
      "8          o2_trans  0.008398\n",
      "133             p29  0.008096\n",
      "18            o1*o2  0.008084\n",
      "124             p20  0.008080\n",
      "129             p25  0.007741\n",
      "109              p5  0.007567\n",
      "150             p46  0.007022\n",
      "166             p62  0.006870\n",
      "9                d1  0.006417\n",
      "17            d1&d2  0.006416\n",
      "10         d1_trans  0.006391\n",
      "12         d2_trans  0.006240\n",
      "148             p44  0.006081\n",
      "120             p16  0.006070\n",
      "164             p60  0.006021\n",
      "111              p7  0.005688\n",
      "20      d2*o2*d1*o1  0.005597\n",
      "105              p1  0.005547\n",
      "112              p8  0.005421\n",
      "154             p50  0.004837\n",
      "170             OTH  0.004796\n",
      "171             OTM  0.004747\n",
      "2              hour  0.004726\n",
      "3              mins  0.004679\n",
      "14            d2-o2  0.004673\n",
      "11               d2  0.004456\n",
      "142             p38  0.004425\n",
      "157             p53  0.003941\n",
      "131             p27  0.003582\n",
      "5                o1  0.003451\n",
      "16            o1&o2  0.003450\n",
      "125             p21  0.003398\n",
      "145             p41  0.003129\n",
      "97      second_mode  0.003081\n",
      "146             p42  0.003081\n",
      "0               pid  0.003059\n",
      "169             p65  0.002986\n",
      "13            d1-o1  0.002955\n",
      "128             p24  0.002727\n",
      "153             p49  0.002713\n",
      "162             p58  0.002628\n",
      "149             p45  0.002224\n",
      "161             p57  0.002001\n",
      "19            d1*d2  0.001724\n",
      "141             p37  0.001678\n",
      "144             p40  0.001398\n",
      "6          o1_trans  0.001051\n",
      "127             p23  0.000959\n",
      "159             p55  0.000829\n",
      "116             p12  0.000743\n"
     ]
    }
   ],
   "source": [
    "train_x = train_data.drop(['day_time','week'],axis =1)\n",
    "\n",
    "train_x.head()\n",
    "train_x = train_x.loc[0:30000]\n",
    "a = list(train_x.corrwith(train_x.click_mode).index)\n",
    "cor_f = pd.DataFrame()\n",
    "cor_f['fea'] =  a\n",
    "cor_f['val'] =  abs(train_x.corrwith(train_x.click_mode).values)\n",
    "print cor_f.sort_values('val',ascending = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>mode_1_pr</th>\n",
       "      <th>mode_1_dis</th>\n",
       "      <th>mode_1_eta</th>\n",
       "      <th>mode_2_pr</th>\n",
       "      <th>mode_2_dis</th>\n",
       "      <th>mode_2_eta</th>\n",
       "      <th>mode_3_pr</th>\n",
       "      <th>mode_3_dis</th>\n",
       "      <th>mode_3_eta</th>\n",
       "      <th>mode_4_pr</th>\n",
       "      <th>mode_4_dis</th>\n",
       "      <th>mode_4_eta</th>\n",
       "      <th>mode_5_pr</th>\n",
       "      <th>mode_5_dis</th>\n",
       "      <th>mode_5_eta</th>\n",
       "      <th>mode_6_pr</th>\n",
       "      <th>mode_6_dis</th>\n",
       "      <th>mode_6_eta</th>\n",
       "      <th>mode_7_pr</th>\n",
       "      <th>mode_7_dis</th>\n",
       "      <th>mode_7_eta</th>\n",
       "      <th>mode_8_pr</th>\n",
       "      <th>mode_8_dis</th>\n",
       "      <th>mode_8_eta</th>\n",
       "      <th>mode_9_pr</th>\n",
       "      <th>mode_9_dis</th>\n",
       "      <th>mode_9_eta</th>\n",
       "      <th>mode_10_pr</th>\n",
       "      <th>mode_10_dis</th>\n",
       "      <th>mode_10_eta</th>\n",
       "      <th>mode_11_pr</th>\n",
       "      <th>mode_11_dis</th>\n",
       "      <th>mode_11_eta</th>\n",
       "      <th>max_dist</th>\n",
       "      <th>min_dist</th>\n",
       "      <th>std_dist</th>\n",
       "      <th>std_price</th>\n",
       "      <th>max_eta</th>\n",
       "      <th>min_eta</th>\n",
       "      <th>std_eta</th>\n",
       "      <th>max_dist_mode</th>\n",
       "      <th>min_dist_mode</th>\n",
       "      <th>max_price_mode</th>\n",
       "      <th>min_price_mode</th>\n",
       "      <th>max_eta_mode</th>\n",
       "      <th>min_eta_mode</th>\n",
       "      <th>first_mode</th>\n",
       "      <th>last_mode</th>\n",
       "      <th>second_mode</th>\n",
       "      <th>day-time</th>\n",
       "      <th>month</th>\n",
       "      <th>vac</th>\n",
       "      <th>week</th>\n",
       "      <th>hour</th>\n",
       "      <th>subway_zone</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>o1</th>\n",
       "      <th>o2</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d1-o1</th>\n",
       "      <th>d2-o2</th>\n",
       "      <th>svd_fea_0</th>\n",
       "      <th>svd_fea_1</th>\n",
       "      <th>svd_fea_2</th>\n",
       "      <th>svd_fea_3</th>\n",
       "      <th>svd_fea_4</th>\n",
       "      <th>svd_fea_5</th>\n",
       "      <th>svd_fea_6</th>\n",
       "      <th>svd_fea_7</th>\n",
       "      <th>svd_fea_8</th>\n",
       "      <th>svd_fea_9</th>\n",
       "      <th>svd_fea_10</th>\n",
       "      <th>svd_fea_11</th>\n",
       "      <th>svd_fea_12</th>\n",
       "      <th>svd_fea_13</th>\n",
       "      <th>svd_fea_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>30</td>\n",
       "      <td>521</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>453</td>\n",
       "      <td>103</td>\n",
       "      <td>210</td>\n",
       "      <td>453</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>404</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>124</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>30</td>\n",
       "      <td>521</td>\n",
       "      <td>136</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>5219.0</td>\n",
       "      <td>4046.0</td>\n",
       "      <td>467.713825</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>3672.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>914.260433</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-11-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>116.29</td>\n",
       "      <td>39.97</td>\n",
       "      <td>116.32</td>\n",
       "      <td>39.96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210736.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1124</td>\n",
       "      <td>357</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1110</td>\n",
       "      <td>112</td>\n",
       "      <td>290</td>\n",
       "      <td>1110</td>\n",
       "      <td>136</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>60</td>\n",
       "      <td>1386</td>\n",
       "      <td>322</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>40</td>\n",
       "      <td>1348</td>\n",
       "      <td>231</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>13864.0</td>\n",
       "      <td>11106.0</td>\n",
       "      <td>1243.375342</td>\n",
       "      <td>1048.045801</td>\n",
       "      <td>3578.0</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>974.408251</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>116.39</td>\n",
       "      <td>39.84</td>\n",
       "      <td>116.33</td>\n",
       "      <td>39.79</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1.659321</td>\n",
       "      <td>0.658922</td>\n",
       "      <td>-0.722464</td>\n",
       "      <td>0.950416</td>\n",
       "      <td>-0.391069</td>\n",
       "      <td>0.522419</td>\n",
       "      <td>0.059124</td>\n",
       "      <td>-0.103123</td>\n",
       "      <td>0.576893</td>\n",
       "      <td>0.834474</td>\n",
       "      <td>0.564825</td>\n",
       "      <td>-0.659383</td>\n",
       "      <td>-0.606791</td>\n",
       "      <td>0.635751</td>\n",
       "      <td>0.465786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>40</td>\n",
       "      <td>1229</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>1130</td>\n",
       "      <td>140</td>\n",
       "      <td>370</td>\n",
       "      <td>1130</td>\n",
       "      <td>164</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>902</td>\n",
       "      <td>272</td>\n",
       "      <td>60</td>\n",
       "      <td>1301</td>\n",
       "      <td>365</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>40</td>\n",
       "      <td>1229</td>\n",
       "      <td>247</td>\n",
       "      <td>170</td>\n",
       "      <td>1299</td>\n",
       "      <td>234</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>13018.0</td>\n",
       "      <td>9023.0</td>\n",
       "      <td>1286.001936</td>\n",
       "      <td>1233.710183</td>\n",
       "      <td>3657.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>693.587715</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>116.31</td>\n",
       "      <td>39.93</td>\n",
       "      <td>116.27</td>\n",
       "      <td>40.00</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202427.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1364</td>\n",
       "      <td>398</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1473</td>\n",
       "      <td>194</td>\n",
       "      <td>310</td>\n",
       "      <td>1473</td>\n",
       "      <td>206</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>60</td>\n",
       "      <td>1512</td>\n",
       "      <td>374</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>170</td>\n",
       "      <td>1485</td>\n",
       "      <td>291</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>15124.0</td>\n",
       "      <td>13640.0</td>\n",
       "      <td>508.944241</td>\n",
       "      <td>1135.957746</td>\n",
       "      <td>3982.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>836.608152</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>116.27</td>\n",
       "      <td>39.88</td>\n",
       "      <td>116.39</td>\n",
       "      <td>39.90</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.299006</td>\n",
       "      <td>-0.720142</td>\n",
       "      <td>-1.369530</td>\n",
       "      <td>-0.654634</td>\n",
       "      <td>-0.417466</td>\n",
       "      <td>-0.535664</td>\n",
       "      <td>0.228437</td>\n",
       "      <td>0.477593</td>\n",
       "      <td>-0.157794</td>\n",
       "      <td>0.066474</td>\n",
       "      <td>0.079345</td>\n",
       "      <td>0.053064</td>\n",
       "      <td>-0.111287</td>\n",
       "      <td>-0.053937</td>\n",
       "      <td>0.283331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172251.0</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>60</td>\n",
       "      <td>1320</td>\n",
       "      <td>336</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>13203.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6521.737551</td>\n",
       "      <td>1577.973384</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1586.413782</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>116.34</td>\n",
       "      <td>39.96</td>\n",
       "      <td>116.37</td>\n",
       "      <td>39.86</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.662153</td>\n",
       "      <td>0.925719</td>\n",
       "      <td>1.405886</td>\n",
       "      <td>0.261671</td>\n",
       "      <td>-0.694085</td>\n",
       "      <td>-0.038230</td>\n",
       "      <td>-0.167124</td>\n",
       "      <td>0.418496</td>\n",
       "      <td>-0.072835</td>\n",
       "      <td>1.218224</td>\n",
       "      <td>0.029496</td>\n",
       "      <td>0.405468</td>\n",
       "      <td>0.124493</td>\n",
       "      <td>-0.252527</td>\n",
       "      <td>0.092441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pid  mode_1_pr  mode_1_dis  mode_1_eta  mode_2_pr  mode_2_dis  \\\n",
       "0      -1.0   -1000000    -1000000    -1000000         30         521   \n",
       "1  210736.0         30        1124         357   -1000000    -1000000   \n",
       "2      -1.0   -1000000    -1000000    -1000000         40        1229   \n",
       "3  202427.0         30        1364         398   -1000000    -1000000   \n",
       "4  172251.0   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "\n",
       "   mode_2_eta  mode_3_pr  mode_3_dis  mode_3_eta  mode_4_pr  mode_4_dis  \\\n",
       "0         167          0         453         103        210         453   \n",
       "1    -1000000          0        1110         112        290        1110   \n",
       "2         271          0        1130         140        370        1130   \n",
       "3    -1000000          0        1473         194        310        1473   \n",
       "4    -1000000          0           0           0        400           0   \n",
       "\n",
       "   mode_4_eta  mode_5_pr  mode_5_dis  mode_5_eta  mode_6_pr  mode_6_dis  \\\n",
       "0         109          0         404         367          0         411   \n",
       "1         136   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "2         164   -1000000    -1000000    -1000000          0         902   \n",
       "3         206   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "4          24   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "\n",
       "   mode_6_eta  mode_7_pr  mode_7_dis  mode_7_eta  mode_8_pr  mode_8_dis  \\\n",
       "0         124   -1000000    -1000000    -1000000   -1000000    -1000000   \n",
       "1    -1000000         60        1386         322   -1000000    -1000000   \n",
       "2         272         60        1301         365   -1000000    -1000000   \n",
       "3    -1000000         60        1512         374   -1000000    -1000000   \n",
       "4    -1000000         60        1320         336   -1000000    -1000000   \n",
       "\n",
       "   mode_8_eta  mode_9_pr  mode_9_dis  mode_9_eta  mode_10_pr  mode_10_dis  \\\n",
       "0    -1000000         30         521         136    -1000000     -1000000   \n",
       "1    -1000000         40        1348         231    -1000000     -1000000   \n",
       "2    -1000000         40        1229         247         170         1299   \n",
       "3    -1000000   -1000000    -1000000    -1000000         170         1485   \n",
       "4    -1000000   -1000000    -1000000    -1000000    -1000000     -1000000   \n",
       "\n",
       "   mode_10_eta  mode_11_pr  mode_11_dis  mode_11_eta  max_dist  min_dist  \\\n",
       "0     -1000000    -1000000     -1000000     -1000000    5219.0    4046.0   \n",
       "1     -1000000    -1000000     -1000000     -1000000   13864.0   11106.0   \n",
       "2          234    -1000000     -1000000     -1000000   13018.0    9023.0   \n",
       "3          291    -1000000     -1000000     -1000000   15124.0   13640.0   \n",
       "4     -1000000    -1000000     -1000000     -1000000   13203.0       1.0   \n",
       "\n",
       "      std_dist    std_price  max_eta  min_eta      std_eta  max_dist_mode  \\\n",
       "0   467.713825   750.000000   3672.0   1035.0   914.260433              2   \n",
       "1  1243.375342  1048.045801   3578.0   1122.0   974.408251              7   \n",
       "2  1286.001936  1233.710183   3657.0   1400.0   693.587715              7   \n",
       "3   508.944241  1135.957746   3982.0   1941.0   836.608152              7   \n",
       "4  6521.737551  1577.973384   3363.0      1.0  1586.413782              7   \n",
       "\n",
       "   min_dist_mode  max_price_mode  min_price_mode  max_eta_mode  min_eta_mode  \\\n",
       "0              5               4               3           5.0           3.0   \n",
       "1              3               4               3           1.0           3.0   \n",
       "2              6               4               3           7.0           3.0   \n",
       "3              1               4               3           1.0           3.0   \n",
       "4              3               4               3           7.0           3.0   \n",
       "\n",
       "   first_mode  last_mode  second_mode    day-time  month  vac  week  hour  \\\n",
       "0           9          5            5  2018-11-02      1    0     5    17   \n",
       "1           7          1            1  2018-11-16      0    0     5    10   \n",
       "2           9          7            7  2018-10-06      2    1     6    10   \n",
       "3          10          1            1  2018-11-23      0    0     5    14   \n",
       "4           7          7            7  2018-10-30      2    0     2    11   \n",
       "\n",
       "   subway_zone  time_zone      o1     o2      d1     d2  d1-o1  d2-o2  \\\n",
       "0            1          2  116.29  39.97  116.32  39.96    3.0   -1.0   \n",
       "1            1          1  116.39  39.84  116.33  39.79   -6.0   -5.0   \n",
       "2            1          1  116.31  39.93  116.27  40.00   -4.0    7.0   \n",
       "3            1          2  116.27  39.88  116.39  39.90   12.0    2.0   \n",
       "4            1          2  116.34  39.96  116.37  39.86    3.0  -10.0   \n",
       "\n",
       "   svd_fea_0  svd_fea_1  svd_fea_2  svd_fea_3  svd_fea_4  svd_fea_5  \\\n",
       "0   0.000000  -0.000000   0.000000  -0.000000   0.000000  -0.000000   \n",
       "1   1.659321   0.658922  -0.722464   0.950416  -0.391069   0.522419   \n",
       "2   0.000000  -0.000000   0.000000  -0.000000   0.000000  -0.000000   \n",
       "3   1.299006  -0.720142  -1.369530  -0.654634  -0.417466  -0.535664   \n",
       "4   1.662153   0.925719   1.405886   0.261671  -0.694085  -0.038230   \n",
       "\n",
       "   svd_fea_6  svd_fea_7  svd_fea_8  svd_fea_9  svd_fea_10  svd_fea_11  \\\n",
       "0  -0.000000   0.000000   0.000000   0.000000   -0.000000    0.000000   \n",
       "1   0.059124  -0.103123   0.576893   0.834474    0.564825   -0.659383   \n",
       "2  -0.000000   0.000000   0.000000   0.000000   -0.000000    0.000000   \n",
       "3   0.228437   0.477593  -0.157794   0.066474    0.079345    0.053064   \n",
       "4  -0.167124   0.418496  -0.072835   1.218224    0.029496    0.405468   \n",
       "\n",
       "   svd_fea_12  svd_fea_13  svd_fea_14  \n",
       "0    0.000000    0.000000    0.000000  \n",
       "1   -0.606791    0.635751    0.465786  \n",
       "2    0.000000    0.000000    0.000000  \n",
       "3   -0.111287   -0.053937    0.283331  \n",
       "4    0.124493   -0.252527    0.092441  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = pd.read_csv(path + 'address_info.csv', names=['lng_lat', 'address_info'], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print map(lambda s: int(s), submit['mod_list'][0].strip('[]').split(','))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_na = np.zeros(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2']"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[1,2]'.strip('[]').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
